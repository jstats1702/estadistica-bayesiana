---
title: "Regresión Binaria"
author: "J. Sosa"
date: "2024-12-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modelo

Considere una **red binaria no dirigida** representada por la matriz de adyacencia \(\mathbf{Y} = [y_{i,j}]\), donde \(\mathbf{Y}\) contiene las conexiones entre \(n\) individuos y sus elementos son \(y_{i,j} \in \{0,1\}\).  

El modelo para la probabilidad de conexión entre dos nodos se define como:  
\[
y_{i,j} \mid \theta_{i,j} \overset{\text{iid}}{\sim} \textsf{Ber}(\theta_{i,j}), \quad \text{para } i < j,
\]  
donde:  
\[
\theta_{i,j} = \Phi(\mu + \delta_i + \delta_j),
\]  
y \(\Phi(\cdot)\) denota la función de distribución acumulada de una **normal estándar**, utilizada como **función de enlace** para garantizar que las **probabilidades estimadas** \(\theta_{i,j}\) se mantengan en el intervalo \([0,1]\).  

Para los **parámetros**, se asumen las siguientes distribuciones previas:  
\[
\mu \sim \textsf{N}(0, \sigma^2), \quad \delta_i \overset{\text{iid}}{\sim} \textsf{N}(0, \tau^2), \quad \sigma^2 \sim \textsf{GI}(a_\sigma, b_\sigma), \quad \tau^2 \sim \textsf{GI}(a_\tau, b_\tau).
\]  
*
El **conjunto de parámetros** del modelo es:  
\[
\Theta = \{\mu, \delta_1, \delta_2, \dots, \delta_n, \sigma^2, \tau^2\},
\]
donde:

- **\(\mu\):** Representa el **efecto promedio global de conectividad** en la red; valores altos indican una mayor **tendencia general** a formar conexiones.  
- **\(\delta_i\) (\(i = 1, \dots, n\)):** Captura la **socialidad** del nodo \(i\); valores positivos indican una mayor **propensión individual** a conectarse, mientras que valores negativos reflejan una **menor tendencia**.  
- **\(\sigma^2\):** Varianza de \(\mu\), mide la **incertidumbre** en el **efecto global de conectividad**.  
- **\(\tau^2\):** Varianza de \(\delta_i\), refleja la **heterogeneidad** en las **tendencias de conexión** entre nodos.  

En total, el modelo tiene \(n + 3\) parámetros: \(n\) efectos de socialidad, más \(\mu\), \(\sigma^2\), y \(\tau^2\).

Los **hiperparámetros** del modelo son:  
\[
\{a_\sigma, b_\sigma, a_\tau, b_\tau\},
\]  
donde \(a_\sigma, b_\sigma\) controlan la varianza de \(\mu\), y \(a_\tau, b_\tau\) regulan la variabilidad de \(\delta_i\).

# Estimación

En el modelo inicial, **las distribuciones condicionales completas no tienen forma cerrada** debido al uso de la **función probit** como **función de enlace**. Esto se debe a que \(\Phi\) introduce una relación **no lineal** entre los parámetros \((\mu, \delta_i, \sigma^2, \tau^2)\) y las observaciones \(y_{i,j}\), lo que impide expresar las **distribuciones condicionales completas** en formas estándar como normales o gammas.

#### Uso de variables latentes auxiliares {-}

Para abordar esta limitación, se puede introducir **variables auxiliares latentes** \(z_{i,j}\), tales que:  
\[
y_{i,j} = 
\begin{cases} 
1 & \text{si } z_{i,j} > 0, \\
0 & \text{si } z_{i,j} \leq 0,
\end{cases}
\]
donde:
\[
z_{i,j} \mid \mu, \delta_i, \delta_j \sim \textsf{N}(\mu + \delta_i + \delta_j, 1).
\]

La introducción de las variables latentes \(z_{i,j}\) mantiene el modelo original porque estas variables son consistentes con la probabilidad subyacente del modelo inicial. Al integrar \(z_{i,j}\) sobre su distribución condicional, recuperamos la probabilidad original de \(y_{i,j}\):

1. Dado que \(z_{i,j} \mid \mu, \delta_i, \delta_j \sim \textsf{N}(\mu + \delta_i + \delta_j, 1)\), la probabilidad de que \(y_{i,j} = 1\) es equivalente a la probabilidad de que \(z_{i,j} > 0\):  
   \[
   \textsf{P}(y_{i,j} = 1 \mid \mu, \delta_i, \delta_j) = \textsf{P}(z_{i,j} > 0) = \int_0^\infty \textsf{N}(z_{i,j} \mid \mu + \delta_i + \delta_j, 1) \, \textsf{d}z_{i,j}.
   \]

2. Evaluando esta integral, obtenemos:
   \[
   \textsf{P}(y_{i,j} = 1 \mid \mu, \delta_i, \delta_j) = \Phi(\mu + \delta_i + \delta_j),
   \]
   donde \(\Phi(\cdot)\) es la función de distribución acumulada de una normal estándar.

3. De manera similar:
   \[
   \textsf{P}(y_{i,j} = 0 \mid \mu, \delta_i, \delta_j) = 1 - \Phi(\mu + \delta_i + \delta_j).
   \]
 
Este enfoque **linealiza el modelo** al sustituir la **función probit** \(\Phi\) por una **variable latente** \(z_{i,j}\) con distribución normal estándar. Esto simplifica los cálculos y permite que las **distribuciones condicionales completas** sean más manejables, facilitando la implementación del **muestreo de Gibbs**.

#### Distribuciones Condicionales Completas {-}

1. \( z_{i,j} \mid y_{i,j}, \mu, \delta_i, \delta_j \) es Normal truncada tal que:
\[
z_{i,j} \mid y_{i,j}, \mu, \delta_i, \delta_j \sim
\begin{cases} 
\textsf{N}(\mu + \delta_i + \delta_j, 1), & z_{i,j} > 0 \text{ si } y_{i,j} = 1, \\
\textsf{N}(\mu + \delta_i + \delta_j, 1), & z_{i,j} \leq 0 \text{ si } y_{i,j} = 0.
\end{cases}
\]

2. Para \( \mu \mid \mathbf{z}, \boldsymbol{\delta}, \sigma^2 \sim \textsf{N}(m, v^2) \), con:
\[
v^2 = \left(\frac{1}{\sigma^2} + \sum_{i<j} 1\right)^{-1}, \quad m = v^2 \sum_{i<j} (z_{i,j} - \delta_i - \delta_j).
\]

3. Para \( \delta_i \mid \mathbf{z}, \mu, \tau^2 \sim \textsf{N}(m, v^2) \), con:  
\[
v^2 = \left(\frac{1}{\tau^2} + \sum_{j \neq i} 1\right)^{-1}, \quad m = v^2 \sum_{j \neq i} (z_{i,j} - \mu - \delta_j).
\]

4. Para \( \sigma^2 \mid \mu \sim \textsf{GI}(a, b) \), con:  
\[
a = a_\sigma + \frac{1}{2}, \quad b = b_\sigma + \frac{\mu^2}{2}.
\]

5. Para \( \tau^2 \mid \boldsymbol{\delta} \sim \textsf{GI}(a, b) \), con:  
\[
a = a_\tau + \frac{n}{2}, \quad b_\tau^* = b_\tau + \frac{\sum_{i} \delta_i^2}{2}.
\]  

# Hiperparámetros {-}

Se recomienda \(\mathbb{E}[\sigma^2] \leq \mathbb{E}[\tau^2]\) para que la heterogeneidad individual \(\delta_i\) sea mayor que la incertidumbre global \(\mu\).

#### Hiperparámetros para \(\sigma^2\) {-}

\(a_\sigma = 2, b_\sigma = 1\).  

- Valor esperado: \(\textsf{E}[\sigma^2] = \frac{b_\sigma}{a_\sigma - 1} =  1\).  
- Coeficiente de variación: \(\textsf{CV}[\sigma^2] = \sqrt{\frac{1}{a_\sigma - 2}} = \infty\).  

\(a_\sigma = 1, b_\sigma = 1\).  

- Valor esperado* \(\textsf{E}[\sigma^2] = \frac{b_\sigma}{a_\sigma - 1} = \infty\).  
- Coeficiente de variación: No definido (alta dispersión y cola pesada).  

#### Hiperparámetros para \(\tau^2\) {-}

\(a_\tau = 3, b_\tau = 2\).  

- Valor esperado: \(\textsf{E}[\tau^2] = \frac{b_\tau}{a_\tau - 1} = 1\).  
- Coeficiente de variación: \(\textsf{CV}[\tau^2] = \sqrt{\frac{1}{a_\tau - 2}} = \sqrt{\frac{1}{1}} = 1\).  

\(a_\tau = 2, b_\tau = 1\).  

Valor esperado: \(\mathbb{E}[\tau^2] = \frac{b_\tau}{a_\tau - 1} = 1\).  
Coeficiente de variación: \(\textsf{CV}[\tau^2] = \sqrt{\frac{1}{a_\tau - 2}} = \infty\).  

# Ejemplo: Simulación

#### Generación de datos {-}

```{r}
# Simulación de datos según el modelo
simulate_data <- function(n, mu_true, tau2_true) {
  # Generar parámetros verdaderos
  delta_true <- rnorm(n, mean = 0, sd = sqrt(tau2_true))  # Efectos nodales
  z_true <- matrix(0, n, n)  # Variables latentes
  Y <- matrix(0, n, n)  # Matriz de adyacencia
  # Generar la matriz de adyacencia
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      z_true[i, j] <- rnorm(1, mean = mu_true + delta_true[i] + delta_true[j], sd = 1)
      z_true[j, i] <- z_true[i, j]  # Simetría
      Y[i, j] <- ifelse(z_true[i, j] > 0, 1, 0)
      Y[j, i] <- Y[i, j]  # Simetría
    }
  }
  # Retornar los datos simulados
  list(Y = Y, z_true = z_true, delta_true = delta_true, mu_true = mu_true, tau2_true = tau2_true)
}
```


```{r}
# Parámetros verdaderos
n <- 50
mu_true <- -0.5
tau2_true <- 0.5
# Simulación
set.seed(123)
sim_data <- simulate_data(n, mu_true, tau2_true)
# Matriz de adyacencia simulada
Y <- sim_data$Y
```


```{r, fig.align='center'}
# Librerías necesarias (sin mensajes)
suppressWarnings(
  suppressMessages({
    library(igraph)
    library(ggraph)
    library(tidygraph)
}))
# Convertir matriz de adyacencia a grafo
graph <- graph_from_adjacency_matrix(Y, mode = "undirected", diag = FALSE)
# Añadir nombres a los nodos (aunque no se usarán en el gráfico)
V(graph)$name <- 1:vcount(graph)
# Convertir a tbl_graph para ggraph
graph_tbl <- as_tbl_graph(graph)
# Graficar la red sin nombres de nodos
set.seed(123)
ggraph(graph_tbl, layout = "fr") +  # Layout Fruchterman-Reingold
  geom_edge_link(aes(edge_alpha = 0.8), color = "gray70", show.legend = FALSE) +  # Enlaces semitransparentes
  geom_node_point(size = 3, color = "darkblue", shape = 21, fill = "lightblue") +  # Nodos estilizados
  theme_void() +  # Tema limpio
  theme(
    plot.background = element_rect(fill = "white", color = NA),  # Fondo blanco
    panel.border = element_blank(),  # Sin bordes
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold")  # Título centrado
  ) +
  ggtitle("Red simulada")  # Título del gráfico
```

#### Ajuste del modelo {-}


```{r}
# Distribuciones condicionales completas
# DCC 1: Muestreo de z
sample_z <- function(y, mu, delta, z) {
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      mean_z <- mu + delta[i] + delta[j]
      if (y[i, j] == 1) {
        z[i, j] <- truncnorm::rtruncnorm(n = 1, a = 0, b = Inf, mean = mean_z, sd = 1)
      } else {
        z[i, j] <- truncnorm::rtruncnorm(n = 1, a = -Inf, b = 0, mean = mean_z, sd = 1)
      }
      z[j, i] <- z[i, j]  # Simetría
    }
  }
  return(z)
}
# DCC 2: Muestreo de mu
sample_mu <- function(z, delta, sigma2) {
  v2_mu <- 1 / (1 / sigma2 + sum(upper.tri(z)))
  m_mu <- v2_mu * sum(z[upper.tri(z)] - delta[row(z)[upper.tri(z)]] - delta[col(z)[upper.tri(z)]])
  return(rnorm(1, mean = m_mu, sd = sqrt(v2_mu)))
}
# DCC 3: Muestreo de delta
sample_delta <- function(z, mu, tau2, delta) {
  for (i in 1:n) {
    neighbors <- setdiff(1:n, i)
    v2_delta <- 1 / (1 / tau2 + length(neighbors))
    m_delta <- v2_delta * sum(z[i, neighbors] - mu - delta[neighbors])
    delta[i] <- rnorm(1, mean = m_delta, sd = sqrt(v2_delta))
  }
  return(delta)
}
# DCC 4: Muestreo de sigma^2
sample_sigma2 <- function(mu) {
  a_sigma_post <- a_sigma + 0.5
  b_sigma_post <- b_sigma + 0.5 * mu^2
  return(1 / rgamma(1, shape = a_sigma_post, rate = b_sigma_post))
}
# DCC 5: Muestreo de tau^2
sample_tau2 <- function(delta) {
  a_tau_post <- a_tau + n / 2
  b_tau_post <- b_tau + 0.5 * sum(delta^2)
  return(1 / rgamma(1, shape = a_tau_post, rate = b_tau_post))
}
```


```{r}
# Muestreador de Gibbs
gibbs_sampler <- function(y, n_iter, n_burn, n_thin) {
  # Inicialización
  mu <- 0
  delta <- rnorm(n, 0, 1)
  sigma2 <- 1
  tau2 <- 1
  z <- matrix(0, n, n)  # Variables auxiliares
  # Almacenamiento
  n_samples <- (n_iter - n_burn) / n_thin
  samples <- list(mu = numeric(n_samples), 
                  delta = matrix(0, nrow = n_samples, ncol = n), 
                  sigma2 = numeric(n_samples), 
                  tau2 = numeric(n_samples))
  # Muestreo
  cat("Iniciando muestreador de Gibbs...\n")
  for (t in 1:n_iter) {
    # Llamar a las funciones de muestreo
    z <- sample_z(y, mu, delta, z)
    mu <- sample_mu(z, delta, sigma2)
    delta <- sample_delta(z, mu, tau2, delta)
    sigma2 <- sample_sigma2(mu)
    tau2 <- sample_tau2(delta)
    # Almacenar muestras según n_thin
    if (t > n_burn && (t - n_burn) %% n_thin == 0) {
      idx <- (t - n_burn) / n_thin
      samples$mu[idx] <- mu
      samples$delta[idx, ] <- delta
      samples$sigma2[idx] <- sigma2
      samples$tau2[idx] <- tau2
    }
    # Mostrar progreso
    if (t %% (n_iter / 10) == 0) {
      cat(sprintf("Progreso: %d%% completado\n", (t / n_iter) * 100))
    }
  }
  cat("Muestreador completado.\n")
  return(samples)
}
```


```{r}
# Hiperparámetros
a_sigma <- 2 
b_sigma <- 1
a_tau   <- 2 
b_tau   <- 1
```


```{r, echo=T, eval=F}
# Ajustar el modelo usando Gibbs
n_iter <- 51000
n_burn <- 1000
n_thin <- 5
samples <- gibbs_sampler(Y, n_iter, n_burn, n_thin)
save(samples, file = "samples_regresion_binaria_sinteticos.RData")
```


```{r, echo=F, eval=T}
load("C:/Users/User/Dropbox/UN/estadistica_bayesiana/samples_regresion_binaria_sinteticos.RData")
```


#### Convergencia {-}


Para cada muestra de \(\mu\) y \(\boldsymbol{\delta}\), la log-verosimilitud del modelo se calcula como:  
\[
\log p(\mu, \boldsymbol{\delta} \mid \mathbf{y}) = \sum_{i < j} \left[ y_{i,j} \log(\theta_{i,j}) + (1 - y_{i,j}) \log(1 - \theta_{i,j}) \right],
\]
donde $\theta_{i,j} = \Phi(\mu + \delta_i + \delta_j)$.


```{r}
# Función para calcular la log-verosimilitud
log_likelihood <- function(y, samples) {
  n <- nrow(y)  # Número de nodos
  log_lik_samples <- numeric(length(samples$mu))  # Almacenar la log-verosimilitud para cada muestra
  for (s in seq_along(samples$mu)) {
    mu <- samples$mu[s]
    delta <- samples$delta[s, ]
    log_lik <- 0
    for (i in 1:(n - 1)) {
      for (j in (i + 1):n) {
        eta_ij <- mu + delta[i] + delta[j]  # Predictor lineal
        p_ij <- pnorm(eta_ij)  # Probabilidad del modelo (probit)
        # Sumar la contribución del par (i, j) a la log-verosimilitud
        log_lik <- log_lik + y[i, j] * log(p_ij + 1e-10) + (1 - y[i, j]) * log(1 - p_ij + 1e-10)
      }
    }
    log_lik_samples[s] <- log_lik
  }
  return(log_lik_samples)
}
```


```{r}
# Calcular la log-verosimilitud para las muestras del muestreador
log_lik <- log_likelihood(Y, samples)
```


```{r, echo=F, fig.align='center'}
# Crear un data frame para el gráfico
log_lik_df <- data.frame(
  Iteration = seq_along(log_lik),  # Número de iteración
  LogLikelihood = log_lik          # Valores de la log-verosimilitud
)
# Graficar la log-verosimilitud frente a las iteraciones (solo puntos pequeños)
suppressMessages(library(ggplot2))
ggplot(log_lik_df, aes(x = Iteration, y = LogLikelihood)) +
  geom_point(size = 0.5, color = "black", alpha = 0.5) +  # Puntos pequeños y semitransparentes
  theme_minimal() +  # Tema limpio
  labs(
    title = "Log-Verosimilitud",
    x = "Iteración",
    y = "Log-Verosimilitud"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```


```{r, echo=F, fig.align='center'}
# Data frame para trazas
trace_df <- data.frame(
  Iteration = seq_along(samples$mu),
  Mu = samples$mu,
  Sigma2 = samples$sigma2,
  Tau2 = samples$tau2
)
# Trazas para Mu
ggplot(trace_df, aes(x = Iteration)) +
  geom_point(aes(y = Mu), color = "black", size = 0.5, alpha = 0.5) +
  labs(title = "Cadena para Mu", x = "Iteración", y = "Valor") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
# Trazas para Sigma^2
ggplot(trace_df, aes(x = Iteration)) +
  geom_point(aes(y = Sigma2), color = "black", size = 0.5, alpha = 0.5) +
  labs(title = "Cadena para Sigma²", x = "Iteración", y = "Valor") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
# Trazas para Tau^2
ggplot(trace_df, aes(x = Iteration)) +
  geom_point(aes(y = Tau2), color = "black", size = 0.5, alpha = 0.5) +
  labs(title = "Cadena para Tau²", x = "Iteración", y = "Valor") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 16)
  )
```


```{r}
# CV de Monte Carlo
get_cvmc <- function(x) round((sd(x)/sqrt(coda::effectiveSize(x)))/abs(mean(x)), 3)
get_cvmc(samples$mu)
get_cvmc(samples$sigma2)
get_cvmc(samples$sigma2)
summary(get_cvmc(samples$delta))
```

#### Inferencia {-}


```{r}
# inferencia sobre mu, sigma2 y tau2
# mu_true = -0.5
# tau2_true = 0.5
round(quantile(samples$mu,     probs = c(0.025,0.5,0.975)), 3)
round(quantile(samples$tau2,   probs = c(0.025,0.5,0.975)), 3)
```



```{r, fig.align='center'}
# Valores reales
delta_true <- sim_data$delta_true
# Crear un DataFrame con las estimaciones y los valores reales
delta_mean <- colMeans(samples$delta)  # Media posterior de delta
delta_ci95 <- apply(samples$delta, 2, quantile, probs = c(0.025, 0.975))  # IC al 95%
delta_ci99 <- apply(samples$delta, 2, quantile, probs = c(0.005, 0.995))  # IC al 99%
delta_df <- data.frame(
  Node = 1:n,
  Delta_Est = delta_mean,
  Delta_True = delta_true,
  CI95_Lower = delta_ci95[1, ],
  CI95_Upper = delta_ci95[2, ],
  CI99_Lower = delta_ci99[1, ],
  CI99_Upper = delta_ci99[2, ]
)
# Graficar usando ggplot2
ggplot(delta_df, aes(x = Node)) +
  # IC al 99%
  geom_linerange(aes(ymin = CI99_Lower, ymax = CI99_Upper), color = "gray70", size = 1) +
  # IC al 95%
  geom_linerange(aes(ymin = CI95_Lower, ymax = CI95_Upper), color = "lightblue", size = 1.5) +
  # Estimaciones puntuales
  geom_point(aes(y = Delta_Est), color = "blue", size = 3) +
  # Valores reales
  geom_point(aes(y = Delta_True), color = "red", shape = 4, size = 3) +
  # Personalización
  labs(
    title = "Estimaciones de Delta con Intervalos de Credibilidad",
    x = "Vértice",
    y = expression(delta)
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```


# Ejemplo: Trabajo colaborativo

Los **datos de Lazega** corresponden a una **red de relaciones de trabajo colaborativo** entre miembros de una **firma de abogados**. Estos datos se recopilaron para estudiar la **cooperación** entre **actores sociales** dentro de una **organización**, analizando el **intercambio** de diversos tipos de **recursos** entre ellos.


```{r}
# librerías
suppressMessages(suppressWarnings(library(igraph)))
suppressMessages(suppressWarnings(library(ggraph)))
suppressMessages(suppressWarnings(library(sand)))
```


```{r}
# grafo
g <- graph_from_data_frame(d = elist.lazega, directed = "F")
# clase de objeto
class(g)
# dirigida?
is_directed(g)
# ponderada?
is_weighted(g)
# orden
(n <- vcount(g))
# tamaño
(s <- ecount(g))
```


```{r}
# matriz de adyacencia
Y <- as.matrix(as_adjacency_matrix(graph = g, names = F))
# clase de objeto
class(Y)
# dimensión
dim(Y)
# simétrica?
isSymmetric(Y)
```


```{r, echo=FALSE, fig.height = 8, fig.width = 16, fig.align='center'}
# visualización
igraph_options(vertex.label = 1:vcount(g), vertex.size = 9, vertex.frame.color = 1, vertex.color = 0, vertex.label.color = "black", edge.color = "blue4")
par(mfrow = c(1,2))
# diseño circular
plot(g, layout = layout_in_circle)
# diseño de Fruchterman y Reingold: Graph Drawing by Force-directed Placement
set.seed(1234)
plot(g, layout = layout_with_fr)
```

#### Ajuste del modelo


```{r}
# Hiperparámetros
a_sigma <- 2 
b_sigma <- 1
a_tau   <- 2 
b_tau   <- 1
```


```{r, echo=T, eval=F}
# Ajustar el modelo usando Gibbs
n_iter <- 51000
n_burn <- 1000
n_thin <- 5
samples <- gibbs_sampler(Y, n_iter, n_burn, n_thin)
save(samples, file = "samples_regresion_binaria_lazega.RData")
```


```{r, echo=F, eval=T}
load("C:/Users/User/Dropbox/UN/estadistica_bayesiana/samples_regresion_binaria_lazega.RData")
```


#### Inferencia {-}


```{r, echo=F, fig.align='center'}
# Crear el DataFrame con estimaciones
delta_mean <- colMeans(samples$delta)  # Media posterior de delta
delta_ci95 <- apply(samples$delta, 2, quantile, probs = c(0.025, 0.975))  # IC al 95%

# Crear el DataFrame ordenado por la media posterior
delta_df <- data.frame(
  Node = 1:n,  # Nodo original
  Delta_Est = delta_mean,
  CI95_Lower = delta_ci95[1, ],
  CI95_Upper = delta_ci95[2, ]
)

# Ordenar por la media posterior (Delta_Est)
delta_df <- delta_df[order(delta_df$Delta_Est), ]
delta_df$Order <- 1:n  # Nueva posición ordenada

# Identificar intervalos que contienen 0
delta_df$ContainsZero <- (delta_df$CI95_Lower <= 0 & delta_df$CI95_Upper >= 0)

# Graficar usando ggplot2
ggplot(delta_df, aes(x = Order)) +
  # Intervalos de credibilidad al 95% (líneas más delgadas)
  geom_segment(aes(
    x = Order, xend = Order,
    y = CI95_Lower, yend = CI95_Upper,
    color = ContainsZero
  ), size = 0.8) + # Línea delgada
  
  # Líneas horizontales pequeñas en los extremos
  geom_segment(aes(
    x = Order - 0.2, xend = Order + 0.2, y = CI95_Lower, yend = CI95_Lower, 
    color = ContainsZero
  ), size = 0.8) + # Extremo inferior

  geom_segment(aes(
    x = Order - 0.2, xend = Order + 0.2, y = CI95_Upper, yend = CI95_Upper, 
    color = ContainsZero
  ), size = 0.8) + # Extremo superior

  # Estimaciones puntuales (color según intervalo)
  geom_point(aes(y = Delta_Est, color = ContainsZero), size = 2) +

  # Añadir números sobre cada intervalo (nodo original)
  geom_text(aes(y = CI95_Upper + 0.1, label = Node), size = 3, hjust = 0.5) +

  # Línea horizontal en delta = 0
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +

  # Personalización
  scale_color_manual(values = c("TRUE" = "gray70", "FALSE" = "blue")) +  # Colores según inclusión de 0
  labs(
    title = "Estimaciones de Delta con Intervalos de Credibilidad (95%)",
    x = NULL,  # Eliminar etiquetas del eje x
    y = expression(delta)
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    legend.position = "none",  # Ocultar leyenda
    axis.text.x = element_blank(),  # Eliminar etiquetas del eje x
    axis.ticks.x = element_blank()  # Eliminar marcas del eje x
  )
```


#### Bondad de ajuste {-}


La **validación del modelo** mediante la **distribución predictiva posterior** evalúa su capacidad para reproducir las **características estructurales** de la **red real**. Se generan **redes simuladas** en cada iteración del **muestreador Gibbs** y se calculan **estadísticas de prueba** como **densidad**, **transitividad**, **asortatividad**, **distancia geodésica promedio**, **grado promedio** y **desviación estándar del grado**. Estas se comparan con las **distribuciones posteriores simuladas**, reportando la **media posterior** y el **intervalo de credibilidad al 95%**, proporcionando evidencia de **ajuste** y **adecuación** del **modelo propuesto**.


```{r}
# Calcular las estadísticas observadas en la red real
obs_density <- edge_density(g)  # Densidad
obs_transitivity <- transitivity(g, type = "global")  # Transitividad
obs_assortativity <- assortativity_degree(g, directed = FALSE)  # Asortatividad
obs_avg_path <- mean_distance(g, directed = FALSE)  # Distancia geodésica promedio
obs_avg_degree <- mean(degree(g))  # Grado promedio
obs_sd_degree <- sd(degree(g))  # Desviación estándar del grado
# Guardar en un vector
obs_stats <- c(
  Density = obs_density,
  Transitivity = obs_transitivity,
  Assortativity = obs_assortativity,
  AvgPathLength = obs_avg_path,
  AvgDegree = obs_avg_degree,
  SD_Degree = obs_sd_degree
)
```


```{r}
# Inicializar listas para almacenar estadísticas simuladas
sim_density <- c()
sim_transitivity <- c()
sim_assortativity <- c()
sim_avg_path <- c()
sim_avg_degree <- c()
sim_sd_degree <- c()
# Iterar sobre las muestras del muestreador
for (iter in 1:length(samples$mu)) {
  # Extraer parámetros para esta iteración
  mu <- samples$mu[iter]
  delta <- samples$delta[iter, ]
  # Construir la matriz de probabilidad
  P <- pnorm(mu + outer(delta, delta, "+"))
  # Simular la red binaria
  Y_sim <- matrix(rbinom(length(P), size = 1, prob = P), n, n)
  Y_sim[lower.tri(Y_sim)] <- t(Y_sim)[lower.tri(Y_sim)]  # Simetrizar
  diag(Y_sim) <- 0  # Sin bucles
  # Crear grafo a partir de la matriz simulada
  g_sim <- graph_from_adjacency_matrix(Y_sim, mode = "undirected")
  # Calcular estadísticas para la red simulada
  sim_density <- c(sim_density, edge_density(g_sim))
  sim_transitivity <- c(sim_transitivity, transitivity(g_sim, type = "global"))
  sim_assortativity <- c(sim_assortativity, assortativity_degree(g_sim, directed = FALSE))
  sim_avg_path <- c(sim_avg_path, mean_distance(g_sim, directed = FALSE))
  sim_avg_degree <- c(sim_avg_degree, mean(degree(g_sim)))
  sim_sd_degree <- c(sim_sd_degree, sd(degree(g_sim)))
}
```



```{r, echo=F, fig.align='center'}
# Nombres de los estadísticos
stat_names <- c("Densidad", "Transitividad", "Asortatividad", 
                "Dist. Geodésica", "Grado Promedio", "Desv. del Grado")

# Estadísticos observados
obs_stats <- c(
  obs_density,         # Densidad observada
  obs_transitivity,    # Transitividad observada
  obs_assortativity,   # Asortatividad observada
  obs_avg_path,        # Distancia geodésica promedio observada
  obs_avg_degree,      # Grado promedio observado
  obs_sd_degree        # Desviación estándar del grado observada
)

# Estadísticos simulados
simulated_stats <- list(
  sim_density,         # Simulaciones para densidad
  sim_transitivity,    # Simulaciones para transitividad
  sim_assortativity,   # Simulaciones para asortatividad
  sim_avg_path,        # Simulaciones para distancia geodésica
  sim_avg_degree,      # Simulaciones para grado promedio
  sim_sd_degree        # Simulaciones para desviación estándar del grado
)

# Crear DataFrame vacío con columnas definidas
plot_data <- data.frame(
  sims = numeric(0),
  stat_name = character(0),
  obs = numeric(0),
  mean_post = numeric(0),
  ci_lower = numeric(0),
  ci_upper = numeric(0)
)

# Llenar el DataFrame en el bucle
for (i in 1:6) {
  sims <- simulated_stats[[i]]  # Simulaciones para el estadístico i
  obs <- obs_stats[i]  # Valor observado
  
  # Calcular la media posterior y los intervalos al 95%
  mean_post <- mean(sims)  # Media posterior
  ci_lower <- quantile(sims, 0.025)  # Límite inferior del IC 95%
  ci_upper <- quantile(sims, 0.975)  # Límite superior del IC 95%
  
  # Crear DataFrame temporal para esta iteración
  temp_df <- data.frame(
    sims = sims,
    stat_name = rep(stat_names[i], length(sims)),
    obs = rep(obs, length(sims)),
    mean_post = rep(mean_post, length(sims)),
    ci_lower = rep(ci_lower, length(sims)),
    ci_upper = rep(ci_upper, length(sims))
  )
  
  # Concatenar datos
  plot_data <- rbind(plot_data, temp_df)
}

# Graficar en paneles 3x2
ggplot(plot_data, aes(x = sims)) +
  geom_histogram(aes(y = ..density..), color = "gray60", fill = "gray90", bins = 30, alpha = 0.8) +
  geom_vline(aes(xintercept = obs), color = "red", size = 1) +
  geom_vline(aes(xintercept = mean_post), color = "blue", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = ci_lower), color = "blue", linetype = "dashed", size = 1) +
  geom_vline(aes(xintercept = ci_upper), color = "blue", linetype = "dashed", size = 1) +
  facet_wrap(~ stat_name, nrow = 3, ncol = 2, scales = "free") +
  labs(x = "Valor", y = "Densidad", title = "Comparación de Estadísticos Simulados") +
  theme_minimal(base_size = 12)
```



