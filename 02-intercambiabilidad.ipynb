{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c3543c-e59a-4dfc-ad61-6e4770c73e86",
   "metadata": {},
   "source": [
    "# Intercambiabilidad\n",
    "\n",
    "Juan Sosa PhD\n",
    "\n",
    "Email:   jcsosam@unal.edu.co\n",
    "\n",
    "GitHub:  https://github.com/jstats1702 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4119d0-092b-486c-a765-574b5fb33b0c",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "La **inferencia Bayesiana** sobre el conjunto de parámetros $\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_k)$ a partir de los datos $\\boldsymbol{y} = (y_1,\\ldots,y_n)$ requiere que se especifique la distribución conjunta $p(\\boldsymbol{y}, \\boldsymbol{\\theta}) = p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, donde:\n",
    "\n",
    "- $p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})$ caracteriza el mecanismo aleatorio para generar $\\boldsymbol{y}$ a partir de $\\boldsymbol{\\theta}$.\n",
    "- $p(\\boldsymbol{\\theta})$ caracteriza el estado de información acerca de $\\boldsymbol{\\theta}$ externa a $\\boldsymbol{y}$.\n",
    "\n",
    "Se modela $\\boldsymbol{y}$ de manera **jerárquica**, modelando primero el comportamiento de $\\boldsymbol{\\theta}$, y luego, modelando el comportamiento de $\\boldsymbol{y}$ dado $\\boldsymbol{\\theta}$.\n",
    "\n",
    "# Razonamiento Bayesiano\n",
    "\n",
    "Una vez se observa $\\boldsymbol{y}$, ¿cómo se deberían llevar a cabo los procesos de **inferencia**, **predicción** y **toma de decisiones** de manera **óptima**?\n",
    "\n",
    "## Inferencia\n",
    "\n",
    "La **distribución posterior** de $\\boldsymbol{\\theta}$ se obtiene por medio del **Teorema de Bayes**:\n",
    "$$\n",
    "p(\\boldsymbol{\\theta}\\mid \\boldsymbol{y}) = \n",
    " \\frac{p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})}{\\int_\\Theta p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})\\,\\text{d}\\boldsymbol{\\theta}}\\propto p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})\\,,\n",
    "$$\n",
    "la cual caracteriza el estado de **información actualizada** acerca de $\\boldsymbol{\\theta}$ bajo la evidencia empírica que proporciona $\\boldsymbol{y}$.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "La **distribución predictiva posterior** de datos futuros $\\boldsymbol{y}^*$ se obtiene por medio de la expresión:\n",
    "$$\n",
    "p(\\boldsymbol{y}^*\\mid\\boldsymbol{y}) \n",
    "= \\int_\\Theta p(\\boldsymbol{y}^*,\\boldsymbol{\\theta}\\mid\\boldsymbol{y})\\,\\text{d}\\boldsymbol{\\theta} \n",
    "= \\int_\\Theta p(\\boldsymbol{y}^*\\mid\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta}\\mid\\boldsymbol{y})\\,\\text{d}\\boldsymbol{\\theta}\\,, \n",
    "$$\n",
    "siempre que no haya información sobre $\\boldsymbol{y}^*$ contenida en $\\boldsymbol{y}$ dado $\\boldsymbol{\\theta}$.\n",
    "\n",
    "## Toma de decisiones\n",
    "\n",
    "Para tomar una decisión se debe especificar un **conjunto de acciones factibles** $\\mathcal{A}$ junto con una **función de utilidad** $U(a,\\boldsymbol{\\theta})$ que cuantifique la utilidad (monetaria o de otro tipo) que se obtiene al elegir la acción $a\\in\\mathcal{A}$ bajo un valor específico de $\\boldsymbol{\\theta}$.\n",
    "\n",
    "La **decisión óptima** consiste en elegir la acción $a^*$ que **maximice la utilidad esperada posterior**:\n",
    "$$\n",
    "a^* = \\text{arg max}_{a\\in\\mathcal{A}} \\textsf{E}(U(a,\\boldsymbol{\\theta})\\mid\\boldsymbol{y})\n",
    " = \\text{arg max}_{a\\in\\mathcal{A}} \\int_\\Theta U(a,\\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta}\\mid\\boldsymbol{y})\\,\\text{d}\\boldsymbol{\\theta}\\,.\n",
    "$$\n",
    "\n",
    "# Independencia condicional\n",
    "\n",
    "Suponga que $y_1,\\ldots,y_n$ son variables aleatorias y que $\\boldsymbol{\\theta}$ es el parámetro que caracteriza el mecanismo aleatorio bajo el cual se generan estas variables. \n",
    "\n",
    "Las variables aleatorias $y_1,\\ldots,y_n$ se denominan **condicionalmente independientes** dado $\\boldsymbol{\\theta}$, si\n",
    "$$\n",
    "p(y_1,\\ldots,y_n\\mid\\boldsymbol{\\theta}) = p(y_1\\mid\\boldsymbol{\\theta})\\times\\ldots\\times p(y_n\\mid\\boldsymbol{\\theta})\\,,\n",
    "$$\n",
    "para todo $y_i\\in\\mathcal{Y}$, con $i=1,\\ldots,n$, y todo $\\boldsymbol{\\theta}\\in\\Theta$.\n",
    "\n",
    "Si $y_1,\\ldots,y_n$ son condicionalmente independientes dado $\\boldsymbol{\\theta}$, entonces\n",
    "$$\n",
    "\\textsf{Pr}(y_1\\in A_1,\\ldots,y_n\\in A_n\\mid\\boldsymbol{\\theta}) = \\textsf{Pr}(y_1\\in A_1\\mid\\boldsymbol{\\theta})\\times\\ldots\\times\\textsf{Pr}(y_n\\in A_n\\mid\\boldsymbol{\\theta})\\,,\n",
    "$$\n",
    "para cualquier colección de conjuntos $A_1,\\ldots,A_n$. \n",
    "\n",
    "**(Ejercicio.)** Además,\n",
    "$$\n",
    "\\textsf{Pr}(y_i\\in A_i\\mid y_j\\in A_j,\\boldsymbol{\\theta}) = \\textsf{Pr}(y_i\\in A_i\\mid\\boldsymbol{\\theta})\\,.\n",
    "$$\n",
    "\n",
    "Si las variables aleatorias $y_1,\\ldots,y_n$ se generan a partir de un **mecanismo aleatorio común**, entonces\n",
    "$$\n",
    "p(\\boldsymbol{y}\\mid\\boldsymbol{\\theta}) = \\prod_{i=1}^n p(y_i\\mid\\boldsymbol{\\theta})\\,,\n",
    "$$\n",
    "en cuyo caso se dice que $y_1,\\ldots,y_n$ son **condicionalmente independientes e idénticamente distribuidas**, lo que se denota con \n",
    "$$\n",
    "y_i\\mid\\boldsymbol{\\theta} \\stackrel{\\text{iid}}{\\sim} p(y_i\\mid\\boldsymbol{\\theta})\\,,\\qquad i=1,\\ldots,n\\,.\n",
    "$$\n",
    "\n",
    "# Intercambiabilidad\n",
    "\n",
    "Las variables aleatorias $y_1,\\ldots,y_n$ se denominan **intercambiables** si su distribución marginal conjunta es **simétrica**, en el sentido de que cualquier permutación del orden en el que se etiqueten las variables deja su estructura probabilística inalterada.\n",
    "\n",
    "Sea $p(y_1,\\ldots,y_n)$ la distribución marginal de $y_1,\\ldots,y_n$. Si \n",
    "$$\n",
    "p(y_1,\\ldots,y_n) = p(y_{\\pi(1)},\\ldots,y_{\\pi(n)})\\,,\n",
    "$$ \n",
    "para toda permutación $\\pi(\\cdot)$ de $\\{1,\\ldots,n\\}$, entonces se dice que $y_1,\\ldots,y_n$ son **intercambiables**.\n",
    "\n",
    "La intercambiabilidad indica que el orden en que se observan o recopilan los datos no afecta la distribución de probabilidad utilizada para modelar los datos.\n",
    "\n",
    "**(Ejercicio.)** Si las variables aleatorias $y_1,\\ldots,y_n$ son condicionalmente independientes dado $\\boldsymbol{\\theta}$, entonces $y_1,\\ldots,y_n$ son intercambiables. \n",
    "\n",
    "# Teorema de De Finetti\n",
    "\n",
    "Sea $y_1,\\ldots,y_n$ una secuencia de variables aleatorias definida sobre el mismo espacio de resultados $\\mathcal{Y}$. Si $y_1,\\ldots,y_n$ es intercambiable para cualquier $n$, entonces la distribución marginal de $y_1,\\ldots,y_n$ se puede expresar como\n",
    "$$\n",
    "p(y_1,\\ldots,y_n) = \\int_\\Theta \\left[\\prod_{i=1}^n p(y_i\\mid\\boldsymbol{\\theta})\\right]\\,p(\\boldsymbol{\\theta})\\,\\text{d}\\boldsymbol{\\theta}\\,,\n",
    "$$\n",
    "para algún conjunto de parámetros $\\boldsymbol{\\theta}$, alguna distribución previa $p(\\boldsymbol{\\theta})$, y alguna distribución muestral común $p(y_i\\mid\\boldsymbol{\\theta})$, para $i=1,\\ldots,n$.\n",
    "\n",
    "El teorema de De Finetti justifica el uso del supuesto de independencia condicional en la formulación del modelo y permite utilizar distribuciones de probabilidad simples para modelar los datos condicionalmente.\n",
    "\n",
    "$ y_1, \\dots, y_n \\mid \\boldsymbol{\\theta} $ son i.i.d. y $ \\theta \\sim p(\\theta) $ si y sólo si $ y_1, \\dots, y_n $ son intercambiables para todo $ n $.\n",
    "\n",
    "La condición de intercambiabilidad de $ Y_1, \\dots, Y_n $ para todo $ n $ es razonable en escenarios como experimentos repetibles, muestreo con reemplazo de una población finita, muestreo sin reemplazo de una población infinita o muestreo sin reemplazo de una población finita significativamente mayor ($ N \\gg n $)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0dc07-e59e-449d-b809-c784c8e07d6c",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "- Sean $x$, $y$ y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z) \\propto p(x,z) \\, p(y,z) \\, p(z)$. Muestre que:\n",
    "\n",
    "     a. $p(x \\mid y,z) \\propto p(x,z)$, i.e., $p(x \\mid y,z)$ es función de $x$ y $z$.  \n",
    "     b. $p(y \\mid x,z) \\propto p(y,z)$, i.e., $p(y \\mid x,z)$ es función de $y$ y $z$.  \n",
    "     c. $x$ y $y$ son condicionalmente independientes dado $z$.\n",
    "\n",
    "- Sean $A$, $B$ y $C$ tres eventos. Suponga que $A$ y $B$ son condicionalmente independientes, dado $C$. Muestre que:\n",
    "\n",
    "     a. $A$ y $B^C$ son condicionalmente independientes, dado $C$.  \n",
    "     b. $A^C$ y $B^C$ son condicionalmente independientes, dado $C$.\n",
    "\n",
    "- Demuestre que para cualquier par de conjuntos $F$ y $G$, la probabilidad de su unión satisface la desigualdad:  \n",
    "$$\n",
    "\\textsf{Pr}(F \\cup G) \\geq \\max\\{\\textsf{Pr}(F), \\textsf{Pr}(G)\\}.\n",
    "$$\n",
    "\n",
    "- Demuestre que si $y_1,\\ldots,y_n$ son condicionalmente independientes dado $\\boldsymbol{\\theta}$, entonces\n",
    "$$\n",
    "\\textsf{Pr}(y_1 \\in A_1,\\ldots,y_n \\in A_n \\mid \\boldsymbol{\\theta}) = \\textsf{Pr}(y_1 \\in A_1 \\mid \\boldsymbol{\\theta}) \\times \\ldots \\times \\textsf{Pr}(y_n \\in A_n \\mid \\boldsymbol{\\theta}),\n",
    "$$\n",
    "para cualquier colección de conjuntos $A_1,\\ldots,A_n$, y además,\n",
    "$$\n",
    "\\textsf{Pr}(y_i \\in A_i \\mid y_j \\in A_j, \\boldsymbol{\\theta}) = \\textsf{Pr}(y_i \\in A_i \\mid \\boldsymbol{\\theta}).\n",
    "$$\n",
    "\n",
    "- Sea $y_1$ y $y_2$ variables condicionalmente i.i.d. con distribución Bernoulli de parámetro $\\theta$, es decir, $y_1, y_2 \\mid \\theta \\sim \\textsf{Ber}(\\theta)$, y suponga que $\\theta \\sim \\textsf{Beta}(\\eta, \\eta)$.  \n",
    "\n",
    "     a. Calcule $\\textsf{E}(y_i)$ y $\\textsf{Var}(y_i)$ (el valor esperado y la varianza de $y_i$ marginalizadas sobre $\\theta$) en función de $\\eta$.  \n",
    "     b. Calcule $\\textsf{E}(y_1 y_2)$, que corresponde a $\\textsf{Pr}(y_1 = 1 \\cap y_2 = 1)$ marginalizada sobre $\\theta$.  \n",
    "     c. Utilizando los resultados previos, grafique la correlación entre $y_1$ y $y_2$ como función de $\\eta$.  \n",
    "     d. Interpretando $\\eta$ como una medida de certeza sobre qué tan cerca está $\\theta$ de $1/2$ y la correlación $\\textsf{Cor}(y_1, y_2)$ como el grado de dependencia entre $y_1$ y $y_2$, explique de manera intuitiva por qué la correlación cambia en función de $\\eta$.\n",
    "\n",
    "- Demuestre que si las variables aleatorias $y_1,\\ldots,y_n$ son condicionalmente independientes dado $\\boldsymbol{\\theta}$, entonces $y_1,\\ldots,y_n$ son intercambiables.\n",
    "\n",
    "- Suponga que $x, y \\mid z \\sim \\textsf{N}(z,1)$ y que $z \\sim \\textsf{N}(0,1)$. Demuestre que $x$ y $y$ son independientes condicionalmente dado $z$, pero no son marginalmente independientes.\n",
    "\n",
    "- Suponga que $x \\to y \\to z$ forma una cadena de Markov, es decir, la distribución condicional de $z$ dado $x$ y $y$ depende únicamente de $y$. Demuestre que esto implica que $x$ y $z$ son condicionalmente independientes dado $y$.\n",
    "\n",
    "- Sea $\\boldsymbol{y} = (y_1, \\dots, y_n)$ un vector aleatorio tal que $\\boldsymbol{y} \\sim \\textsf{N}_n(\\boldsymbol{0}, \\Sigma)$, donde $\\textsf{Var}(y_i) = 1$ para $i = 1, \\dots, n$ y $\\textsf{E}(y_i y_j) = \\rho$ para $i \\neq j$, con $\\rho > 0$. Demuestre que en este caso, las variables $y_1, \\dots, y_n$ son intercambiables, pero no son independientes idénticamente distribuidas.\n",
    "\n",
    "- Un instituto de investigación está estudiando las preferencias electorales de una población a través de encuestas. Suponga que cada persona entrevistada vota por el candidato A con una probabilidad desconocida $\\theta$. Se asume que los votos de los encuestados, $y_1, \\dots, y_n$, son intercambiables.  \n",
    "\n",
    "     a. Explique por qué los votos pueden modelarse como una secuencia intercambiable.  \n",
    "     b. Utilizando el Teorema de De Finetti, justifique por qué la distribución conjunta de los votos puede expresarse como una mezcla de distribuciones Bernoulli con una distribución previa $p(\\theta)$.  \n",
    "     c. Suponga que la distribución previa de $\\theta$ es $\\textsf{Beta}(1,1)$. Si en una muestra de 10 personas, 7 votan por el candidato A y 3 por otro candidato, determine la distribución posterior de $\\theta$.\n",
    "\n",
    "\n",
    "# Referencias\n",
    "\n",
    "Hoff, P. D. (2009). ***A First Course in Bayesian Statistical Methods***. Springer New York.\n",
    "\n",
    "Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). ***Bayesian Data Analysis*** (3rd ed.). Chapman & Hall/CRC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
