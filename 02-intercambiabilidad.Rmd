---
title: "Intercambiabilidad"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

La inferencia Bayesiana sobre $\boldsymbol{\theta} = (\theta_1,\ldots,\theta_k)\in\Theta$ a partir del conjunto de datos $\boldsymbol{y} = (y_1,\ldots,y_n)\in\mathcal{Y}^n$ requiere que se especifique de manera inequívoca una distribución conjunta $p(\boldsymbol{y}, \boldsymbol{\theta})$.

La forma natural de especificar $p(\boldsymbol{y}, \boldsymbol{\theta})$ es a partir de la factorización 
$$
p(\boldsymbol{y}, \boldsymbol{\theta}) = p(\boldsymbol{y}\mid\boldsymbol{\theta})\,p(\boldsymbol{\theta})\,,
$$
donde:

- $p(\boldsymbol{y}\mid\boldsymbol{\theta})$ caracteriza el mecanismo aleatorio para generar $\boldsymbol{y}$ a partir de cada $\boldsymbol{\theta}$.
- $p(\boldsymbol{\theta})$ caracteriza el estado de información acerca de $\boldsymbol{\theta}$ externo a $\boldsymbol{y}$.


Una vez se ha observado $\boldsymbol{y}$, se calcula la **distribución posterior** de $\boldsymbol{\theta}$ por medio del **teorema de Bayes**,
$$
p(\boldsymbol{\theta}\mid \boldsymbol{y}) = \frac{p(\boldsymbol{\theta},\boldsymbol{y})}{p(\boldsymbol{y})} = \frac{p(\boldsymbol{y}\mid\boldsymbol{\theta})\,p(\boldsymbol{\theta})}{\int_\Theta p(\boldsymbol{y}\mid\boldsymbol{\theta})\,p(\boldsymbol{\theta})\,\text{d}\boldsymbol{\theta}}\propto p(\boldsymbol{y}\mid\boldsymbol{\theta})\,p(\boldsymbol{\theta})\,,
$$
que caracteriza el estado de información **actualizado** acerca de $\boldsymbol{\theta}$ dado que se ha observado $\boldsymbol{y}$.

# Independencia condicional

Suponga que $y_1,\ldots,y_n$ son variables aleatorias y que $\boldsymbol{\theta}$ es un parámetro que caracteriza las condiciones bajo las cuales se generan estas variables. 

**(Definición.)** Las variables aleatorias $y_1,\ldots,y_n$ se denominan **independientes condicionalmente** dado $\boldsymbol{\theta}$, si para cualquier colección de $n$ conjuntos $A_1,\ldots,A_n$ se tiene que
$$
\textsf{Pr}(y_1\in A_1,\ldots,y_n\in A_n\mid\boldsymbol{\theta}) = \textsf{Pr}(y_1\in A_1\mid\boldsymbol{\theta})\times\ldots\times\textsf{Pr}(y_n\in A_n\mid\boldsymbol{\theta})\,.
$$

La independencia condicional se puede interpretar en el sentido de que $y_j$ no proporciona información adicional sobre $y_i$ más allá de aquella contenida en $\boldsymbol{\theta}$, dado que
$$
\textsf{Pr}(y_i\in A_i\mid\boldsymbol{\theta},y_j\in A_j) = \textsf{Pr}(y_i\in A_i\mid\boldsymbol{\theta})
$$

Bajo el supuesto de **independencia condicional** se tiene que la distribución condicional conjunta es el producto de las distribuciones marginales correspondientes:
$$
p(\boldsymbol{y}\mid\boldsymbol{\theta}) = p(y_1,\ldots,y_n\mid\boldsymbol{\theta}) = p(y_1\mid\boldsymbol{\theta})\times\ldots\times p(y_n\mid\boldsymbol{\theta})
$$


Además, si las variables aleatorias $y_1,\ldots,y_n$ se generan a partir de un **mecanismo aleatorio común**, entonces
$$
p(\boldsymbol{y}\mid\boldsymbol{\theta}) = \prod_{i=1}^n p(y_i\mid\boldsymbol{\theta})\,,
$$
en cuyo caso se dice que $y_1,\ldots,y_n$ son **condicionalmente independientes e idénticamente distribuidas**, lo que se denota con 
$$
        y_i\mid\boldsymbol{\theta} \stackrel{\text{iid}}{\sim} p(y_i\mid\boldsymbol{\theta})\,,\qquad i=1,\ldots,n\,.
$$

# Intercambiabilidad

Las variables aleatorias $y_1,\ldots,y_n$ se denominan intercambiables si su distribución marginal conjunta es **simétrica**, en el sentido de que cualquier permutación del orden en el que se etiqueten las variables deja su estructura probabilística inalterada.

**(Definición.)** Sea $p(y_1,\ldots,y_n)$ la distribución marginal conjunta de $y_1,\ldots,y_n$. Si 
$$
        p(y_1,\ldots,y_n) = p(y_{\pi(1)},\ldots,y_{\pi(n)})
$$ 
para toda permutación $\pi(\cdot)$ de $\{1,\ldots,n\}$, entonces se dice que $y_1,\ldots,y_n$ son **intercambiables**.

**(Proposición.)** Si las variables aleatorias $y_1,\ldots,y_n$ son condicionalmente independientes dado $\boldsymbol{\theta}$, entonces $y_1,\ldots,y_n$ son intercambiables. 

# Teorema de representación de de Finetti

**(Teorema.)** Sea $y_1,y_2,\ldots$ una secuencia infinita de variables aleatorias definida sobre el mismo espacio de resultados $\mathcal{Y}$. Si $y_1,\ldots,y_n$ son intercambiables para todo $n$, entonces la distribución marginal conjunta de $y_1,\ldots,y_n$ se puede escribir como
$$
p(\boldsymbol{y}) = p(y_1,\ldots,y_n) = \int_\Theta \left\{\prod_{i=1}^n p(y_i\mid\boldsymbol{\theta})\right\}\,p(\boldsymbol{\theta})\,\text{d}\boldsymbol{\theta}\,,
$$
para algún parámetro $\boldsymbol{\theta}$, alguna distribución previa $p(\boldsymbol{\theta})$, y alguna distribución muestral común $p(y\mid\boldsymbol{\theta})$.


Se puede modelar el comportamiento de $\boldsymbol{y}$ directamente, a través de $p(\boldsymbol{y})$, o bien **jerárquicamente**, modelando primero el comportamiento de $\boldsymbol{\theta}$, y luego modelando el comportamiento condicional de $\boldsymbol{y}$ dado $\boldsymbol{\theta}$, es decir:
$$
\boldsymbol{y}\sim p(\boldsymbol{y}) 
\quad\Longleftrightarrow\quad
\boldsymbol{\theta}\sim p(\boldsymbol{\theta})
\quad\text{y}\quad 
y_i\mid\boldsymbol{\theta} \stackrel{\text{iid}}{\sim} p(y_i\mid\boldsymbol{\theta})
$$

**Los modelos Bayesianos son intrínsecamente jerárquicos**.

Aunque en la práctica solo se va a observar $y_1,\ldots,y_n$, el teorema de de Finetti requiere **extender la intercambiabilidad finita a una secuencia infinita numerable** $y_1,y_2,\ldots$, lo que equivale a considerar $y_1,\ldots,y_n$ como una **muestra aleatoria** de una **población** $y_1,y_2,\ldots$. 


# Ejercicios {-}

1. Sean $A$, $B$, y $C$ proposiciones de falso-verdadero. Suponga que $A$ y $B$ son condicionalmente independientes dado $C$. Muestre que:

    a. $A$ y $B^\text{c}$ son condicionalmente independientes dado $C$.	
    b. $A^\text{c}$ y $B^\text{c}$ son condicionalmente independientes dado $C$.
    
2. Sean $x$, $y$, y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z)$. Muestre que:

    a. $p(x,y\mid z) = p(y\mid x,z)\, p(x\mid z)$.
    b. $p(x\mid y,z) = p(y\mid x,z)\, p(x\mid z) / p(y\mid z)$.

3. Sean $x$, $y$, y $z$ variables aleatorias con función de densidad conjunta (discreta o continua) dada por $p(x,y,z) \propto p(x,z)p(y,z)p(z)$. Muestre que:

    a. $p(x\mid y,z)\propto p(x,z)$, i.e. $p(x\mid y,z)$ es función de $x$ y $z$.
    b. $p(y\mid x,z)\propto p(y,z)$, i.e. $p(y\mid x,z)$ es función de $y$ y $z$.
    c. $x$ y $y$ son condicionalmente independientes dado $z$.

4. Suponga que para cada $\theta$ se tiene que $f(y,\theta)$ es una función de masa de probabilidad definida sobre $\mathcal{Y} = \mathbb{N}_0$ de forma que $\sum_{y=0}^\infty f(y,\theta)  = 1$. Sea $g(\theta)$ una función de densidad de probabilidad definida sobre $\Theta = \mathbb{R}_0^+$ y sea $p(y,\theta) = f(y,\theta)\, g(\theta)$.

    a. Muestre que $p(y,\theta)$ es una función de masa-densidad de probabilidad conjunta válida defina sobre $\mathcal{Y}\times\Theta$.
    b. Encuentre las distribuciones marginales $p(y)$ y $p(\theta)$ junto con las distribuciones condicionales $p(y\mid\theta)$ y $p(\theta\mid y)$ en términos de $f$ y $g$.

5. Dé un ejemplo de una secuencia de variables aleatorias $y_1,\ldots,y_n$ intercambiables, pero que no sean marginalmente independientes e idénticamente distribuidas. 

# Referencias


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```