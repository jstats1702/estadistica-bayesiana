---
title: "Modelos de mezcla finitos"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\simiid}{\,{\stackrel{\text{iid}}{\sim}}\,}
\newcommand{\simind}{\,{\stackrel{\text{ind}}{\sim}}\,}

\newcommand{\te}{\theta}

\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\tev}{\boldsymbol{\theta}}
\newcommand{\siv}{\boldsymbol{\sigma}}
\newcommand{\omev}{\boldsymbol{\omega}}
\newcommand{\xiv}{\boldsymbol{\xi}}

\newcommand{\Nor}{\small{\textsf{N}}}
\newcommand{\Cat}{\small{\textsf{Categorica}}}
\newcommand{\Dir}{\small{\textsf{Dirichlet}}}
\newcommand{\IG} {\small{\textsf{GI}}}

# Introducción

Los datos surgen a partir de la **conformación de grupos** (*clusters*), cada uno con una probabilidad específica. 

En cada uno de los grupos la variable de estudio sigue (condicionalmente) una distribución Normal con media y varianza específicas del grupo. 

Considere el siguiente **modelo de mezcla finito** (*finite mixture model*) para **variables continuas**:
$$
\begin{align*}
	y_i\mid\omev,\tev,\siv^2 \simiid \sum_{h=1}^H\omega_h\Nor(\theta_h,\sigma^2_h)\,,\qquad i = 1,\ldots,n\,,
\end{align*}
$$
donde:

- $H$ es el **número de grupos**  en que se clasifican las observaciones (entero positivo predeterminado).
- $\tev=(\te_1,\ldots,\te_H)$ es el vector de **medias de la mezcla**. 
- $\siv^2 = (\te_1,\ldots,\te_H)$ es el vector de **varianzas de la mezcla**.
- $\omev=(\omega_1,\ldots,\omega_H)$ es un vector de **probabilidades** tales que $0<\omega_h<1$ para $h=1,\ldots,H$ y $\sum_{h=1}^H \omega_h = 1$.

La probabilidad de que cualquier observación $i$ haga parte del grupo $h$ es $\omega_h$, es decir:
$$
\textsf{Pr}(\xi_i = h\mid\omega_h) = \omega_h
$$ 
donde $\xi_i$ es una **variable categórica** que asume valores en el conjunto de números enteros $\{1,\ldots,H\}$ con probabilidades $\omega_1,\ldots,\omega_H$. 

Así, considerando el vector de variables categóricas $\xiv=(\xi_1,\ldots,\xi_n)$, el modelo se puede escribir como:
$$
y_i\mid\xi_i,\theta_{\xi_i},\sigma^2_{\xi_i} \simind \Nor(\theta_{\xi_i},\sigma^2_{\xi_i})\,,\qquad i = 1,\ldots,n\,.
$$

# Modelo

- **Distribución muestral:**
$$
y_i\mid\xi_i,\te_{\xi_i},\sigma^2_{\xi_i} \simind \Nor(\theta_{\xi_i},\sigma^2_{\xi_i})
$$

- **Distribución previa:**
$$
p(\xiv,\omev,\tev,\sigma^2) = p(\xiv\mid\omev)\,p(\omev)\,p(\tev)\,p(\sigma^2)
$$ 
donde:
$$
\begin{align*}
\xiv \mid \omev &\sim    \Cat(\omev) \\
\omev           &\sim    \Dir(\alpha^0_1,\ldots,\alpha^0_{H})\\
\theta_h        &\simiid \Nor(\mu_0,\gamma_0^2) \\
\sigma^2_h      &\simiid \IG\left(\tfrac{\nu_0}{2},\tfrac{\nu_0\sigma^2_0}{2}\right)
\end{align*}
$$
- Los **parámetros** del modelo son $\xi_1,\ldots,\xi_n,\omega_1,\ldots,\omega_H,\theta_1,\ldots,\theta_H,\sigma^2_1,\ldots,\sigma^2_H$.
- Los **hiperparámetros** del modelo son $\alpha^0_1,\ldots,\alpha^0_{H},\mu_0,\gamma_0^2,\nu_0,\sigma^2_0$.

# Ejemplo: Datos Sintéticos

Muestra aleatoria de tamaño $n = 100$ de la siguiente mezcla:
$$
y_i \simiid \tfrac{2}{3}\cdot\Nor(0,1) + \tfrac{1}{3}\cdot\Nor(4,0.75)\,, \qquad i =1,\ldots,n\,.
$$

```{r}
# parámetros de la simulación
n <- 100
H <- 2
theta <- c(0, 4)
sig2  <- c(1, 0.75) 
omega <- c(2/3, 1/3)
# simulación de datos
set.seed(123)
xi <- sample(x = 1:H, size = n, replace = TRUE, prob = omega)
y  <- rnorm(n = n, mean = theta[xi], sd = sqrt(sig2[xi]))
```

```{r, echo = F}
# valores poblaciones verdaderos
xi_true    <- xi
theta_true <- theta
sig2_true  <- sig2
omega_true <- omega
# función de densidad verdadera
f_true <- function(x) sum(omega_true*dnorm(x, mean = theta_true, sd = sqrt(sig2_true)))
```

```{r, echo = F, fig.align='center'}
# visualización
par(mar=c(2.75,2.75,0.5,0.5), mgp=c(1.7,0.7,0))
# histograma
hist(x = y, freq = F, nclass = 20, cex.axis = 0.8, col = "gray90", border = "gray90", main = "", xlab = "y", ylab = "Densidad")
# función de densidad verdadera
x0 <- seq(from = -8, to = 8, len = 1000)
y0 <- NULL
for (i in 1:length(x0)) 
  y0[i] <- f_true(x0[i]) 
lines(x = x0, y = y0, type = "l", col = 2)
# leyenda
legend("topright", legend = c("Población"), col = 2, lwd = 2, bty = "n", cex = 0.8)
```



# Ejemplo: Datos de tiempos entre erupciones

Cada fila representa una erupción observada del *Old Faithful Geyser* en el Parque Nacional de Yellowstone. 

La columna `eruptions` representa la duración de la erupción en minutos y la columna `waiting` representa la duración en minutos hasta la siguiente erupción.

***Härdle, W. (1991) Smoothing Techniques with Implementation in S. New York: Springer.***

***Hunter, D. R., Wang, S., & Hettmansperger, T. P. (2007). Inference for mixtures of symmetric distributions. The Annals of Statistics, 224-251.***

***Tatiana, B., Didier, C., David, R. H., & Derek, Y. (2009). mixtools: An R package for analyzing finite mixture models. Journal of Statistical Software, 32(6), 1-29.***

```{r, eval = TRUE, echo=FALSE, out.width="100%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("faithful.jpg")
```

```{r}

# datos
# https://cran.r-project.org/web/packages/mixtools/vignettes/mixtools.pdf
suppressMessages(suppressWarnings(library(mixtools)))
data(faithful)
y <- faithful$waiting
(n <- length(y))
summary(y)
```

```{r, echo = F, fig.align='center'}
# histograma
hist(x = y, freq = F, nclass = 15, cex.axis = 0.8, col = "gray90", border = "gray90", main = "", xlab = "y", ylab = "Densidad")
```


# Ejemplo: Datos de Galaxias

El conjunto de datos de galaxias fue publicado originalmente en astronomía por Postman et al. (1986) y consiste en mediciones univariadas que representan las **velocidades de las galaxias alejándose de nuestra galaxia**.

***Postman M, Huchra JP, Geller MJ (1986) Probes of large-scale structure in the Corona Borealis region. The Astron J 92(6):1238–1247.***

***Richardson, S., & Green, P. J. (1997). On Bayesian analysis of mixtures with an unknown number of components (with discussion). Journal of the Royal Statistical Society: series B (statistical methodology), 59(4), 731-792.***

***Grün, B., Malsiner-Walli, G., & Frühwirth-Schnatter, S. (2021). How many data clusters are in the Galaxy data set?. Advances in Data Analysis and Classification, 1-25.***

```{r, eval = TRUE, echo=FALSE, out.width="100%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("agrupamiento_galaxias.jpg")
```

```{r}
# data
# https://rdrr.io/cran/rebmix/man/galaxy.html
data(galaxy, package = "rebmix")
y <- galaxy$Velocity
(n <- length(y))
summary(y)
```

```{r, fig.align='center'}
# histograma
hist(x = y, freq = F, nclass = 25, cex.axis = 0.8, col = "gray90", border = "gray90", main = "", xlab = "y", ylab = "Densidad")
```



# Estimación

Construir un **muestreador de Gibbs** para obtener muestras de la distribución posterior $p(\boldsymbol{\theta}\mid\boldsymbol{y})$ donde: 

- $\tev = (\xi_1,\ldots,\xi_n,\omega_1,\ldots,\omega_H,\te_1,\ldots,\te_H,\sigma_1^2,\ldots,\sigma_1^2)$.
- $\boldsymbol{y}=(y_1,\ldots,y_n)$.

## Distribución posterior {-}

$$
\begin{align*}
p(\tev\mid\yv) &\propto \prod_{i=1}^n\Nor(y_i\mid\theta_{\xi_i},\sigma_{\xi_i}^2) \\
&\quad\times\prod_{i=1}^n\Cat(\xi_i\mid\omev) \times\Dir(\omev)\\
&\quad\quad\times\prod_{h=1}^H\Nor(\theta_h\mid\mu_0,\gamma^2_0)\times\prod_{h=1}^H\IG\left(\sigma_h^2\mid\tfrac{\nu_0}{2},\tfrac{\nu_0\sigma^2_0}{2}\right)\,.
\end{align*}
$$

## Distribuciones condicionales completas {-}

La distribución condicional completa de $\theta_h$, para $h = 1,\ldots,H$, es $\theta_h\mid\text{resto}\sim\Nor(m_h,v^2_h)$ con:
$$
m_h = \frac{\frac{1}{\gamma_0^2}\mu_0 + \frac{n_h}{\sigma^2}\bar{y}_h}{\frac{1}{\gamma_0^2} + \frac{n_h}{\sigma^2}}
\qquad\text{y}\qquad 
v^2_h = \frac{1}{\frac{1}{\gamma_0^2} + \frac{n_h}{\sigma^2}}\,,
$$
donde $n_h = |\{i:\xi_i=h\}|$ es el número de individuos del grupo $h$ y 
$$
\bar{y}_h = \frac{1}{n_h}\sum_{i:\xi_i=h} y_i
$$ 
es la media de las observaciones del grupo $h$. 
	
Si $n_h = 0$, entonces $m_h = \mu_0$ y $v^2_h = \gamma_0^2$.
	
```{r}
sample_theta <- function (nh, ybh, H, mu0, gam02, theta, sig2) 
{
  for (h in 1:H) {
    if (nh[h] == 0) {
      theta[h] <- rnorm(n = 1, mean = mu0, sd = sqrt(gam02))
    } else {
      v2 <- 1/(1/gam02 + nh[h]/sig2[h])
      m  <- v2*(mu0/gam02 + nh[h]*ybh[h]/sig2[h])
      theta[h] <- rnorm(n = 1, mean = m, sd = sqrt(v2))
    }
  }
  return(theta)
}
```
	
	
La distribución condicional completa de $\sigma^2_h$, para $h = 1,\ldots,H$, es $\sigma_h^2\mid\text{resto}\sim\IG\left( \frac{\nu_h}{2} , \frac{\nu_h\sigma_h^2}{2} \right)$ con:
$$
\nu_h = \nu_0 + n_h
\qquad\text{y}\qquad 
\nu_h\sigma_h^2 = \nu_0\sigma_0^2 + \sum_{i:\xi_i = h}^n (y_i - \theta_{\xi_i})^2 \,.
$$

Se observa que $\sum_{i:\xi_i=h} (y_i - \theta_{\xi_i})^2 = \sum_{i:\xi_i=h} (y_i - \bar{y}_h)^2 + n_h(\bar{y}_h - \theta_h)^2$.
  
Si $n_h = 0$, entonces $\nu_h = \nu_0$ y $\nu_h\sigma^2_h = \nu_0\sigma^2_0$.

```{r}
sample_sig2 <- function (nh, ybh, ssh, H, nu0, sig02, theta, sig2)
{
  for (h in 1:H) {
    if (nh[h] == 0) {
      sig2[h] <- 1/rgamma(n = 1, shape = 0.5*nu0, rate = 0.5*nu0*sig02)
    } else {
      a <- 0.5*(nu0 + nh[h])
      b <- 0.5*(nu0*sig02 + ssh[h] + nh[h]*(ybh[h] - theta[h])^2)
      sig2[h] <- 1/rgamma(n = 1, shape = a, rate = b)
    }
  }
  return(sig2)
}
```


La distribución condicional completa de $\xi_i$, para $i=1,\ldots,n$, es una distribución de probabilidad discreta tal que:
$$
p(\xi_i = h\mid\text{resto})\propto \omega_h\, \Nor(y_i\mid\theta_h,\sigma^2)
$$
y por lo tanto
$$
\textsf{Pr}(\xi_i = h\mid\text{resto}) = \frac{ \omega_h\, \Nor(y_i\mid\theta_h,\sigma^2) }{ \sum_{h=1}^H \omega_k\, \Nor(y_i\mid\theta_{h},\sigma^2) }
$$
para $h = 1,\ldots,H$.

```{r}
sample_xi <- function (H, omega, theta, sig2, xi, n, y) 
{
  for (i in 1:n) {
    lp <- rep(NA, H)
    for (h in 1:H) {
      lp[h] <- log(omega[h]) + dnorm(x = y[i], mean = theta[h], sd = sqrt(sig2[h]), log = TRUE)
    }
    xi[i] <- sample(x = 1:H, size = 1, replace = FALSE, prob = exp(lp - max(lp)))
  }
  return(xi)
}
```


La distribución condicional completa de $\omev$ es:
$$
\omev\mid\text{resto} \sim \Dir(\alpha^0_1+n_1,\ldots,\alpha^0_{H}+n_H)\,.
$$

```{r}
sample_omega <- function (nh, alpha0)
{
  return(c(gtools::rdirichlet(n = 1, alpha = alpha0 + nh)))
}
```

## Muestreador de Gibbs {-}

```{r}
# muestreador de Gibbs
mcmc <- function (y, H, mu0, gam02, nu0, sig02, alpha0, n_sams, n_burn, n_skip, verbose = TRUE) 
{
  # ajustes
  y  <- as.numeric(y)
  yb <- mean(y)
  sy <- sd(y) 
  y  <- scale(y)
  n  <- length(y)
  # número de iteraciones
  B <- n_burn + n_sams*n_skip
  ncat <- floor(0.1*B)
  # valores iniciales
  tmp   <- stats::kmeans(x = y, centers = H)
  xi    <- tmp$cluster
  omega <- as.numeric(table(factor(x = xi, levels = 1:H)))/H
  theta <- rnorm(n = H, mean = mu0, sd = sqrt(gam02))
  sig2  <- 1/rgamma(n = H, shape = nu0/2, rate = nu0*sig02/2)
  # almacenamiento
  THETA <- matrix(data = NA, nrow = n_sams, ncol = H) 
  SIG2  <- matrix(data = NA, nrow = n_sams, ncol = H)
  OMEGA <- matrix(data = NA, nrow = n_sams, ncol = H)
  XI    <- matrix(data = NA, nrow = n_sams, ncol = n)
  LP    <- matrix(data = NA, nrow = n_sams, ncol = 1)
  # cadena
  for (i in 1:B) {
    # actualizar estadísticos suficientes
    nh <- as.numeric(table(factor(x = xi, levels = 1:H)))
    ybh <- ssh <- rep(NA, H)
    for (h in 1:H) {
      if (nh[h] > 0) {
        indexh <- xi == h
        ybh[h] <- mean(y[indexh])
        ssh[h] <- sum((y[indexh] - ybh[h])^2)
      }
    }
    # actualizar parámetros
    sig2  <- sample_sig2  (nh, ybh, ssh, H, nu0, sig02, theta, sig2)
    theta <- sample_theta (nh, ybh, H, mu0, gam02, theta, sig2)
    omega <- sample_omega (nh, alpha0)
    xi    <- sample_xi    (H, omega, theta, sig2, xi, n, y)
    # almacenar y log-verosimilitud
    if (i > n_burn) {
      if (i%%n_skip == 0) {
        k <- (i - n_burn)/n_skip
        THETA[k,] <- sy*theta + yb
        SIG2 [k,] <- sy^2*sig2
        OMEGA[k,] <- omega
        XI   [k,] <- xi
        LP   [k,] <- sum(dnorm(x = y, mean = theta[xi], sd = sqrt(sig2[xi]), log = TRUE))
      }
    }
    # progreso
    if (verbose) if (i%%ncat == 0) cat(100*round(i/B, 1), "% completado \n", sep = "")
  }
  # salida
  return(list(THETA = THETA, SIG2 = SIG2, OMEGA = OMEGA, XI = XI, LP = LP))
}
```

# Ejemplo: Datos Sintéticos

Muestra aleatoria de tamaño $n = 1000$ de la siguiente mezcla:
$$
y_i \simiid \tfrac{2}{3}\cdot\Nor(0,1) + \tfrac{1}{3}\cdot\Nor(4,0.75)\,, \qquad i =1,\ldots,n\,.
$$

```{r}
n <- 100
H <- 2
theta <- c(0, 4)
sig2  <- c(1, 0.75) 
omega <- c(2/3, 1/3)
# simulación de datos
set.seed(123)
xi <- sample(x = 1:H, size = n, replace = TRUE, prob = omega)
y  <- rnorm(n = n, mean = theta[xi], sd = sqrt(sig2[xi]))
```


## Ajuste del modelo

Muestras de la distribución posterior $p(\xiv,\omev,\tev,\siv^2\mid\yv)$ usando un **muestreador de Gibbs** con:

- $H = 2$.
- $\alpha^0_1=\ldots=\alpha^0_H=1/H,\mu_0 = 0,\gamma_0^2 = 1,\nu_0 = 1,\sigma^2_0 = 1$. 

Los datos en el muestreador se escalan automáticamente.

```{r}
# número de grupos
H <- 2
```

```{r}
# hiperparámetros
alpha0 <- rep(1/H, H)
mu0    <- 0
gam02  <- 1
nu0    <- 1
sig02  <- 1
```

```{r}
# número de parámetros
n + 3*H
```

```{r}
# numero de iteraciones
n_sams <- 10000
n_burn <- 1000
n_skip <- 5
tictoc::tic()
set.seed(123)
muestras <- mcmc(y, H, mu0, gam02, nu0, sig02, alpha0, n_sams, n_burn, n_skip, verbose = FALSE) 
tictoc::toc()
```


## Convergencia

```{r, echo = F, fig.align='center'}
# cadena log-verosimilitud
plot(x = 1:n_sams, y = muestras$LP, type = "p", pch = ".", cex.axis = 0.8, main = "", xlab = "Iteración", ylab = "Log-verosimilitud")
```

```{r}
# errores estándar de Monte Carlo
round(apply(muestras$THETA, 2, sd)/sqrt(coda::effectiveSize(muestras$THETA)), 5)
round(apply(muestras$SIG2,  2, sd)/sqrt(coda::effectiveSize(muestras$SIG2 )), 5)
round(apply(muestras$OMEGA, 2, sd)/sqrt(coda::effectiveSize(muestras$OMEGA)), 5)
```


## Inferencia: Medias, varianzas, probabilidades

Las **etiquetas** (*labels*) de los grupos **no son identificables** (intercambiar las etiquetas no cambia la verosimilitud, esto se conoce como *Label switching*). 

Las etiquetas per se no son de interés, lo que interesa son los **miembros de los grupos**.


```{r, echo = F}
# resumen posterior
resumen_pos <- function(x) round(c(mean(x), sd(x), quantile(x = x, probs = c(0.025,0.975))), 3)
tab <- rbind(resumen_pos(muestras$THETA[,1]),
             resumen_pos(muestras$THETA[,2]),
             resumen_pos(muestras$SIG2 [,1]),
             resumen_pos(muestras$SIG2 [,2]),
             resumen_pos(muestras$OMEGA[,1]),
             resumen_pos(muestras$OMEGA[,2]))
colnames(tab) <- c("Media","SD","Q2.5%","Q97.5%")
rownames(tab) <- c(paste0("theta_",1:H),paste0("sigma_",1:H,"^2"),paste0("omega_",1:H))
knitr::kable(x = tab, digits = 3, align = "c")
```




## Inferencia: Funcón de densidad de la población

La distribución muestral $p(x\mid\omev,\tev,\siv^{2}) = \sum_{h=1}^H \omega_h\,\textsf{N}(x\mid\theta_h,\sigma_h^{2})$ se puede evaluar en una secuencia de valores de $x$ a través de las muestras $\omev^{(b)},\tev^{(b)},\siv^{2\,(b)}$, para $b=1,\ldots,B$, de la distribución posterior con el fin de **cuantificar la incertidumbre** asociada con el aprendizaje acerca de la **función de densidad de la población**.

La estimación de la función de densidad de la población $g(\cdot)$ se hace mediante
$$
\hat{g}(x) = \frac{1}{B}\sum_{b=1}^B p(x\mid\omev^{(v)},\tev^{(b)},\siv^{2\,(b)}) = \frac{1}{B}\sum_{b=1}^B\sum_{h=1}^H\omega_h^{(b)}\,\textsf{N}(x\mid\theta_h^{(b)},\sigma_h^{2\,(b)})\,.
$$

```{r}
# inferencia sobre la función de densidad de la población
M  <- 250
x0 <- seq(from = -4, to = 8, len = M)
y0 <- NULL
B <- nrow(muestras$XI)
B_grid <- seq(from = 5, to = B, len = B/10)
B <- length(B_grid)
FE <- matrix(data = NA, nrow = B, ncol = M)
for (i in 1:M) {
  y0[i] <- f_true(x0[i])
  for (b in B_grid)
    FE[which(b == B_grid),i] <- sum(muestras$OMEGA[b,]*dnorm(x = x0[i], mean = muestras$THETA[b,], sd = sqrt(muestras$SIG2[b,])))
}
f_hat <- colMeans(FE)
f_inf <- apply(X = FE, MARGIN = 2, FUN = quantile, probs = 0.025)
f_sup <- apply(X = FE, MARGIN = 2, FUN = quantile, probs = 0.975)
```


```{r, echo=F, fig.align='center', fig.width=8, fig.height=4}
par(mfrow = c(1,2), mar = c(2.75,2.75,0.5,0.5), mgp = c(1.7,0.7,0))
# visualización muestras
plot(NA, NA, xlim = c(-4, 8), ylim = c(0, 0.4), cex.axis = 0.8, xlab = "y", ylab = "", main = "")
# muestras
for (b in 1:B)
  lines(x = x0, y = FE[b,], type = "l", col = adjustcolor("red",0.1))
# función de densidad verdadera
lines(x = x0, y = y0, type = "l", col = "black", lwd = 2)
# leyenda
legend("topright", legend = c("Población", "Muestras"), fill = c("black","red"), border = c("black","red"), bty = "n", cex = 0.8)
# visualización estimación puntual
plot(NA, NA, xlim = c(-4, 8), ylim = c(0, 0.4), cex.axis = 0.8, xlab = "y", ylab = "Densidad", main = "")
# estimación modelo
lines(x = x0, y = f_hat, type = "l", col = "blue", lwd = 2)
# función de densidad verdadera
lines(x = x0, y = y0, type = "l", col = "black", lwd = 2)
# leyenda
legend("topright", legend = c("Población", "Estimación"), fill = c("black","blue"), border = c("black","blue"), bty = "n", cex = 0.8)
```


## Inferencia: Grupos

La **matriz de incidencia** $\mathbf A = [a_{i,j}]$ es una matriz cuadrada de tamaño $n\times n$ constituida por las probabilidades pareadas de que las observaciones $i$ y $j$ pertenezcan al mismo grupo, esto es, 
$$
a_{i,j} = \textsf{Pr}(\xi_i = \xi_j \mid \boldsymbol{y}) \approx \frac{1}{B}\sum_{b=1}^B \mathbb I\left\{ \xi_i^{(b)} = \xi_j^{(b)} \right\} \,,
$$
donde $\mathbb{I}(A)$ es la función indicadora del conjunto $A$.

La matriz $\mathbf A$ es simétrica dado que $\textsf{Pr}(\xi_i = \xi_j \mid \boldsymbol{y}) = \textsf{Pr}(\xi_j = \xi_i \mid \boldsymbol{y})$, y además, $a_{i,i} = \textsf{Pr}(\xi_i = \xi_i \mid \boldsymbol{y}) = 1$, para todo $i$. 

```{r, fig.align='center',fig.width=6, fig.height=6}
# matriz de incidencia
A <- matrix(data = 0, nrow = n, ncol = n)
for (b in B_grid) {
  for (i in 1:(n-1)) {
    for (j in (i+1):n) {
      if (muestras$XI[b,i] == muestras$XI[b,j]) {
        A[i,j] <- A[i,j] + 1/B
      } 
    }
  }
}
A <- A + t(A)
diag(A) <- 1
# se organizan las observaciones de acuerdo a la partición verdadera
indices <- order(xi)
A <- A[indices,indices]
# visualización de la matriz de incidencia
par(mar = c(2.75,2.75,0.5,0.5), mgp = c(1.7,0.7,0))
corrplot::corrplot(corr = A, is.corr = FALSE, addgrid.col = NA, method = "color", tl.pos = "n")
```


```{r}
# ARI: Adjusted Rand Index
# Medida normalizada de la similitud entre dos agrupaciones de datos
# 0.90 <= ARI        : excelente 
# 0.80 <= ARI < 0.90 : sobresaliente
# 0.60 <= ARI < 0.80 : moderado
# 0.50 <= ARI < 0.60 : aceptable
#         ARI < 0.50 : deficiente
ari <- NULL
for (b in B_grid) 
  ari[which(b == B_grid)] <- aricode::ARI(muestras$XI[b,], as.numeric(xi))
mean(ari)
quantile(ari, c(0.025,0.975))
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```