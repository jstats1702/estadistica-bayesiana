---
title: "Modelo Normal Multivariado"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\simiid}{\,{\stackrel{\text{iid}}{\sim}}\,}
\newcommand{\simind}{\,{\stackrel{\text{ind}}{\sim}}\,}

\newcommand{\te}{\theta}
\newcommand{\si}{\sigma}

\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\tev}{\boldsymbol{\theta}}
\newcommand{\omev}{\boldsymbol{\omega}}
\newcommand{\xiv}{\boldsymbol{\xi}}

\newcommand{\Wm}{\mathbf{W}}
\newcommand{\Ym}{\mathbf{Y}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\SIG}{\mathbf{\Sigma}}
\newcommand{\LAM}{\mathbf{\Lambda}}

\newcommand{\Nor}{\small{\textsf{N}}}
\newcommand{\Cat}{\small{\textsf{Categorica}}}
\newcommand{\Dir}{\small{\textsf{Dirichlet}}}
\newcommand{\IG} {\small{\textsf{GI}}}

# Modelo general

Si Su estado de información acerca de las secuencia de observaciones $\yv_1,\ldots,\yv_n$ es intercambiable, donde $\yv_i = (y_{i,1},\ldots,y_{i,p})\in\mathbb{R}^p$ para $i=1,\ldots,n$, entonces el modelamiento $\yv_1,\ldots,\yv_n$ admite representación jerárquica de la forma:
$$
\begin{align*}
	\yv_i\mid\tev,\SIG &\stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)\\
	(\tev,\SIG)  &\sim p(\tev,\SIG) \,.
\end{align*}
$$
El modelo tiene $k=p+p(p+1)/2$ parámetros desconocidos, a saber, el vector de medias $\tev$ y la matriz de covarianzas $\SIG$. 

El vector aleatorio $\yv=(y_1,\ldots,y_p)$ tiene distribución **Normal multivariada** si la función de densidad de probabilidad $\boldsymbol{y}$ está dada por
$$
p(\yv\mid\tev,\SIG) = (2\pi)^{-p/2}\,|\SIG|^{-1/2}\,\exp\left\{ -\frac12(\yv-\tev)^{\textsf{T}} \SIG^{-1} (\yv-\tev) \right\}\,,
$$
donde
$$
\tev = (\theta_1,\ldots,\theta_p)
\qquad\text{y}\qquad
\SIG = 
\begin{bmatrix} 
  \si_1^2     & \si_{1,2} & \cdots & \si_{1,p} \\
  \si_{2,1}^2 & \si_2^2   & \cdots & \si_{2,p} \\
  \vdots      & \vdots    & \ddots & \vdots    \\
  \si_{p,1}   & \si_{p,2} & \cdots & \si_p^2 \\
\end{bmatrix}\,,
$$
con $\SIG$ simétrica, $\SIG^{\textsf{T}}=\SIG$, y definida positiva, $\xv^{\textsf{T}}\SIG\xv > 0$ para todo $\xv\in\mathbb{R}^p$. 

El **núcleo** de esta distribución es:
$$
p(\yv\mid\tev,\SIG) \propto \exp\left\{ -\frac12\left[ \yv^{\textsf{T}}\SIG^{-1}\yv - 2\yv^{\textsf{T}}\SIG^{-1}\tev \right] \right\}\,.
$$

# Estadísticos Suficientes

Si $\yv_i\mid\tev,\SIG\stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)$, con $i=1,\ldots,n$, entonces la distribución muestral conjunta de las observaciones es:
$$
\begin{align*}
p\left(\mathbf{Y} \mid \tev,\SIG \right) 
&= \prod_{i=1}^n \left(2 \pi\right)^{-p / 2} |\SIG|^{-1/2} \exp\left\{ -\frac12(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) \right\} \\
&= \left(2 \pi\right)^{-np / 2} |\SIG|^{-n/2} \exp\left\{ -\frac12\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) \right\}\,,
\end{align*}
$$
donde $\mathbf{Y}=[y_{i,j}]$.

Como
$$
\begin{align*}
\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) 
&= \sum_{i=1}^n\yv_i^{\textsf{T}}\SIG^{-1}\yv_i - 2\sum_{i=1}^n\yv_i^{\textsf{T}}\SIG^{-1}\tev + n\tev^{\textsf{T}}\SIG^{-1}\tev \\
&= \textsf{traza}\left( \SIG^{-1} \sum_{i=1}^n\yv_i\yv_i^{\textsf{T}} \right) - 2\left(\sum_{i=1}^n\yv_i\right)^{\textsf{T}}\SIG^{-1}\tev + n\tev^{\textsf{T}}\SIG^{-1}\tev\\
\end{align*}
$$
entonces
$$
\left( \sum_{i=1}^n\yv_i, \sum_{i=1}^n\yv_i\yv_i^{\textsf{T}} \right)
$$
es un estadístico suficiente para $(\tev,\SIG)$.

La media muestral y la matriz de covarianza muestral $(\bar{\boldsymbol y},\Sm)$ también constituyen un estadístico suficiente para $(\tev,\SIG)$ dado que
$$
\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) = 
\textsf{traza}\left( \SIG^{-1}\left[ (n-1)\Sm + n(\bar{\boldsymbol y} - \tev)(\bar{\boldsymbol y} - \tev)^{\textsf{T}}  \right] \right)
$$
donde
$$
\bar{\boldsymbol y} = \frac{1}{n}\sum_{i=1}^n \boldsymbol{y}_i
\qquad\text{y}\qquad
\Sm = \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol{y}_i - \bar{\boldsymbol y})(\boldsymbol{y}_i - \bar{\boldsymbol y})^{\textsf{T}}\,.
$$

# Modelo Normal Semi-Conjugado

- **Distribución muestral**:
$$
\yv_i\mid\tev,\SIG \stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)\,,\qquad i=1,\ldots,n\,.
$$
- **Distribución previa**:
$$
p(\tev,\SIG) = p(\tev)\,p(\SIG) 
$$
donde
$$
\begin{align*}
\tev  &\sim \textsf{N} (\muv_0, \LAM_0) \\
\SIG  &\sim \textsf{WI}(\nu_0,\Sm_0^{-1})
\end{align*}
$$
- **Parámetros**: $\tev$, $\SIG$.

- **Hiperparámetros**: $\muv_0, \LAM_0, \nu_0, \Sm_0$.

## Distribución Wishart

La matriz aleatoria $\Wm > 0$ de $p \times p$ tiene **distribución Wishart** con parámetros $\nu > p+1$ (grados de libertad) y $\Sm > 0$ (matriz de escala de $p\times p$), i.e., $\Wm\sim\textsf{W}(\nu,\Sm)$, si su función de densidad de probabilidad es
$$
p(\Wm\mid\nu,\Sm) \propto |\Wm|^{(\nu-p-1)/2}\,\exp\left\{ -\frac12\textsf{traza}(\Sm^{-1}\Wm)  \right\}\,.
$$

- Si $\Wm\sim\textsf{W}(\nu,\Sm)$, entonces $\textsf{E}(\Wm) = \nu\Sm$.

## Distribución Wishart Inversa

La matriz aleatoria $\Wm > 0$ de $p \times p$ tiene **distribución Wishart Inversa** con parámetros $\nu > p+1$ (grados de libertad) y $\Sm > 0$ (matriz de escala de $p\times p$), i.e., $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$, si su función de densidad de probabilidad es
$$
p(\Wm\mid\nu,\Sm^{-1}) \propto |\Wm|^{-(\nu+p+1)/2}\,\exp\left\{ -\frac12\textsf{traza}(\Sm\Wm^{-1}) \right\}\,.
$$

- Si $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$, entonces $\textsf{E}(\Wm) = \frac{1}{\nu-p-1}\Sm$ para $\nu > p+1$.

- Si $\Wm^{-1}\sim\textsf{W}(\nu,\Sm)$, entonces $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$.


# Distribuciones condicionales completas

El muestreador de Gibbs permite **obtener muestras dependientes** de la **distribución posterior** por medio de las **distribuciones condicionales completas**.

- La distribución condicional completa de $\tev$ es $\tev\mid\text{resto}\sim\textsf{N}(\muv_n,\LAM_n)$, donde
$$
\muv_n = \left( \LAM_0^{-1} + n\SIG^{-1} \right)^{-1} \left( \LAM_0^{-1}\muv_0 + n\SIG^{-1}\bar{\boldsymbol{y}} \right)
\qquad\text{y}\qquad
\LAM_n = \left( \LAM_0^{-1} + n\SIG^{-1} \right)^{-1}\,.
$$

- La distribución condicional completa de $\SIG$ es $\SIG\mid\text{resto}\sim\textsf{WI}(\nu_n,\Sm_n^{-1})$, donde
$$
\nu_n = \nu_0 + n
\qquad\text{y}\qquad
\Sm_n = \Sm_0 + \sum_{i=1}^n (\boldsymbol{y}_i - \tev)(\boldsymbol{y}_i - \tev)^{\textsf{T}} = \Sm_0 + (n-1)\Sm + n(\bar{\boldsymbol y} - \tev)(\bar{\boldsymbol y} - \tev)^{\textsf{T}} \,.
$$

Note que el valor esperado condicional en ambos casos es un **promedio ponderado** de la información previa y la información muestral.

# Muetreador de Gibbs

Dado un **estado actual** de los parámetros del modelo $(\tev^{(b)}, \SIG^{(b)})$, se genera un nuevo estado $(\tev^{(b+1)}, \SIG^{(b+1)})$ como sigue:

1. Muestrar $\tev^{(b+1)}\sim p(\tev\mid\SIG^{(b)},  \boldsymbol{y})$.
2. Muestrar $\SIG^{(b+1)}\sim p(\SIG\mid\tev^{(b+1)},\boldsymbol{y})$.
3. Almacenar $(\tev^{(b+1)}, \SIG^{(b+1)})$ 
4. Repetir los pasos 1. a 3. hasta convergencia.

Este algoritmo genera una **secuencia dependiente** de parámetros $(\tev^{(1)}, \SIG^{(1)}),\ldots,(\tev^{(B)}, \SIG^{(B)})$ de la distribución posterior $p(\tev,\SIG \mid \boldsymbol{y})$. 

# Ejemplo: Comprensión de Lectura

A una muestra de 22 niños se les dan pruebas de comprensión de lectura antes y después de recibir un método de instrucción en particular. 

Cada estudiante $i$ tiene asociados dos variables $y_{i,1}$ y $y_{i,2}$ que denotan los **puntajes antes y después de la instrucción**, respectivamente.

El  objetivo es **evaluar la efectividad del método de enseñanza** junto con **la consistencia de la prueba de comprensión de lectura**.

***Hoff, P. D. (2009). A first course in Bayesian statistical methods (Vol. 580). New York: Springer.***

```{r}
# datos
Y <- structure(c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 
                 70, 34, 50, 41, 52, 60, 34, 28, 35, 77, 39, 46, 26, 
                 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 
                 60, 75, 47, 48, 33), 
               .Dim = c(22L, 2L), .Dimnames = list(NULL, c("pretest", "posttest")))
class(Y)
n <- nrow(Y)
n
p <- ncol(Y)
p
```

```{r}
# inspeccionar datos
head(Y)
summary(Y)
```


```{r,echo = FALSE, fig.width=4,fig.height=4, fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
plot(x = Y[,1], y = Y[,2], pch = 16, col = 4, cex = 1.1,
     xlab = "Pre-test", ylab = "Pos-test", main = paste0("Coef. Correlación: ", round(cor(Y[,1],Y[,2]), 3)))
abline(v = mean(Y[,1]), col = "gray85", lty = 2)
abline(h = mean(Y[,2]), col = "gray85", lty = 2)
lines(x = mean(Y[,1]), y = mean(Y[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
```


```{r, echo=FALSE, fig.width=8,fig.height=8, fig.align='center'}
# gráfico
par(mfrow = c(2,2), mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# histogramas
hist(Y[,1], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pre-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,1]), sd = sd(Y[,1])), col = 2, lty = 3, add = T)
hist(Y[,2], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pos-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,2]), sd = sd(Y[,2])), col = 2, lty = 3, add = T)
# qqplots
qqnorm(Y[,1], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,1], col = 2)
qqnorm(Y[,2], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,2], col = 2)
```

```{r}
# estadísticos suficientes
# información de la muestra
yb <- c(colMeans(Y))
round(yb, 1)
SS <- cov(Y)
round(SS, 1) 
```


## Previa

El examen fue diseñado para dar puntajes promedio de 50 de 100: $\muv_0 = (50, 50)$.

Usar una varianza previa tal que $\textsf{Pr}( 0 < \theta_j < 100 ) \approx 0.99$: $\sigma^2_{0,1}=\sigma^2_{0,2}=(50/3)^2 \approx 278$.

Usar correlación previa tal que $\rho_0 = 0.5$: $\sigma_{0,12} = (0.5)(50/3)^2 \approx 139$.

```{r}
# previa
mu0 <- c(50,50)
L0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
nu0 <- 4
S0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
```


## Ajuste del Modelo Normal Semi-Conjugado

```{r}
# inicializar
theta <- yb
Sigma <- SS
# numero de muestras
B <- 10000
ncat <- floor(B/10)
# almacenamiento
THETA <- SIGMA <- YS <- LP <- NULL
# cadena
iL0 <- solve(L0)
Lm0 <- solve(L0)%*%mu0
nun <- nu0 + n
SSn <- S0 + (n-1)*SS
set.seed(1)
for (b in 1:B) {
  # actualizar theta
  iSigma <- solve(Sigma)
  Ln     <- solve(iL0 + n*iSigma)
  theta  <- c(mvtnorm::rmvnorm(n = 1, mean = Ln%*%(Lm0 + n*(iSigma%*%yb)), sigma = Ln))
  # actualizar Sigma
  Sigma <- solve(rWishart::rWishart(n = 1, df = nun, Sigma = solve(SSn + n*((yb - theta)%*%t(yb - theta))))[,,1])
  # predictiva posterior
  YS <- rbind(YS, mvtnorm::rmvnorm(n = 1, mean = theta, sigma = Sigma)) 
  # log-verosimilitud
  LP[b] <- sum(apply(X = Y, MARGIN = 1, FUN = function(y) mvtnorm::dmvnorm(x = y, mean = theta, sigma = Sigma, log = T)))
  # almacenar
  THETA <- rbind(THETA, theta)
  SIGMA <- rbind(SIGMA, c(Sigma))
  # progreso
  if (b%%ncat == 0) cat(100*round(b/B, 1), "% completado ... \n", sep = "")
}
```
## Convergencia

Cadena de la log-verosimilitud:

```{r, echo = FALSE, fig.width=8,fig.height=4, fig.align='center'}
# grafico
par(mar = c(2.75,2.75,1.5,.5), mgp = c(1.7,0.7,0))
plot(x = 1:B, y = LP, type = "l", xlab = "Iteración", ylab = "Log-verosimilitud", main = "Modelo Normal Multivariado")
```

Errores estándar de Monte Carlo:

```{r}
# errores estandar de Monte Carlo
round(apply(X = THETA, MARGIN = 2, FUN = sd)/sqrt(coda::effectiveSize(THETA)), 3)
round(apply(X = SIGMA, MARGIN = 2, FUN = sd)/sqrt(coda::effectiveSize(SIGMA)), 3)
```

## Inferencia

Distribución posterior de $\tev$ y distribución predictiva posterior:

```{r,echo=FALSE,fig.width=8,fig.height=8,fig.align='center'}
# gráfico
par(mfrow = c(2,1), mar = c(2.75,2.75,1.5,.5), mgp = c(1.5,0.5,0))
# theta 2 vs theta 1
plot (x = THETA[,1], y = THETA[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(theta[1]), ylab = expression(theta[2]), main = "", cex.axis = 0.9)
abline(v = mean(THETA[,1]), col = "gray85", lty = 2)
abline(h = mean(THETA[,2]), col = "gray85", lty = 2)
lines(x = THETA[,1], y = THETA[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
# y* 2 vs y* 1
plot (x = YS[,1], y = YS[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(tilde(y)[1]), ylab = expression(tilde(y)[2]), main = "", cex.axis = 0.9)
abline(v = mean(YS[,1]), col = "gray85", lty = 2)
abline(h = mean(YS[,2]), col = "gray85", lty = 2)
lines(x = YS[,1], y = YS[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
```


Inferencia sobre $\theta_2 - \theta_1$:

¿Cuál es la probabilidad posterior de que la calificación promedio del segundo examen sea mayor que la del primero?

```{r}
round(mean(THETA[,2] - THETA[,1] > 0), 3)
```

```{r}
round(quantile(THETA[,2] - THETA[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\tilde{y}_2 - \tilde{y}_1$:

¿Cuál es la probabilidad posterior de que un niño seleccionado al azar obtenga una puntuación más alta en el segundo examen que en el primero?

```{r}
round(mean(YS[,2] - YS[,1] > 0), 3)
```

```{r}
round(quantile(YS[,2] - YS[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\rho=\frac{\sigma_{1,2}}{\sigma_{1}\,\sigma_{2}}$:

¿Las pruebas son consistentes? ¿Cuál es la probabilidad posterior de que la correlación entre las calificaciones sea superior a 0.6?

```{r}
# muestras de rho
RHO <- SIGMA[,2]/sqrt(SIGMA[,1]*SIGMA[,4])
```

```{r,echo=FALSE,fig.width=4,fig.height=4,fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
hist(RHO, freq = F, col = "gray90", border = "gray90", xlim = c(0,1), xlab = expression(rho), ylab = "Densidad", main = "")
abline(v = quantile(RHO, prob = c(0.025,0.5,0.975)), col = c(4,2,4), lty = c(4,2,4))
```


```{r}
round(mean(RHO > 0.6), 3)
```

```{r}
round(quantile(RHO, prob = c(0.025,0.5,0.975)), 3)
```

## Bondad de Ajuste

Estadísticos de prueba $\bar{y}_1 - \bar{y}_2$ y $r_{1,2}$:

```{r}
TS <- matrix(NA, B, 2)
set.seed(1)
for (b in 1:B) {
  Yrep   <- mvtnorm::rmvnorm(n = n, mean = THETA[b,], sigma = matrix(data = SIGMA[n,], nrow = p, ncol = p))
  TS[b,] <- c(mean(Yrep[,2]) - mean(Yrep[,1]), cor(Yrep[,1], Yrep[,2]))
}
```

```{r, echo =FALSE, fig.width=8, fig.height=4, fig.align='center'}
# gráfico
par(mfrow = c(1,2), mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# diferencia de medias
hist(TS[,1], freq = F, col = "gray90", border = "gray90", xlab = expression(bar(y)[2]-bar(y)[1]), ylab = "Densidad", main = "Diferencia de medias", cex.axis = 0.9)
abline(v = mean(Y[,2]) - mean(Y[,1]), col = 2, lwd = 2)
# correlación
hist(TS[,2], freq = F, col = "gray90", border = "gray90", xlab = expression(r[12]), ylab = "Densidad", main = "Correlación", xlim = c(0,1), cex.axis = 0.9)
abline(v = cor(Y[,1],Y[,2]), col = 2, lwd = 2)
```


```{r}
# ppp
round(c(mean(TS[,1] > (mean(Y[,2]) - mean(Y[,1]))), mean(TS[,2] > cor(Y[,1],Y[,2]))), 3)
```

## Comparación de modelos


Modelos:

- $M_0$: modelo Normal univariado   semiconjugado (independiente).
- $M_1$: modelo Normal multivariado semiconjugado (conjunto). 


Métricas:

- Estadísticos de prueba.
- Criterios de información (DIC).
- Validación cruzada.


### Ajuste del Modelo 1


```{r, echo = FALSE}
# hiperparametros
mu0 <- 50
t20 <- 278
nu0 <- 1
s20 <- 278
# almacenamiento
PHI <- array(data = NA, dim = c(B,3,p))
for (j in 1:p) {
  # datos
  y <- Y[,j]
  n <- length(y)
  sum_y  <- sum (y)
  mean_y <- mean(y)
  var_y  <- var (y)
  # inicializar
  theta <- mean_y
  sig2  <- var_y
  # cadena
  nun <- nu0 + n
  ssn <- nu0*s20 + (n-1)*var_y
  set.seed(1)
  for (b in 1:B) {
    # actualizar theta
    t2n   <- 1/(1/t20 + n/sig2)      
    theta <- rnorm(n = 1, mean = t2n*(mu0/t20 + sum_y/sig2), sd = sqrt(t2n))
    # actualizar sig2
    sig2 <- 1/rgamma(n = 1, shape = 0.5*nun, rate = 0.5*(ssn + n*(mean_y - theta)^2))
    # log-verosimilitud
    lp <- sum(dnorm(x = y, mean = theta, sd = sqrt(sig2), log = T))
    # almacenar
    PHI[b,,j] <- c(theta, sig2, lp)
  }
}
```


### Estadísticos de prueba


```{r, echo = F}
TS0 <- matrix(NA, B, 2)
set.seed(1)
for (b in 1:B) {
  for (j in 1:p)
    Yrep[,j] <- rnorm(n = n, mean = PHI[b,1,j], sd = sqrt(PHI[b,2,j]))
  TS0[b,] <- c(mean(Yrep[,2]) - mean(Yrep[,1]), cor(Yrep[,1], Yrep[,2]))
}
```


```{r, fig.width=8, fig.height=8, echo =F, fig.align='center'}
# grafico
par(mfrow = c(2,2), mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# M0 diferencia de medias
hist(TS0[,1], freq = F, col = "lightyellow2", border = "lightyellow2", xlab = expression(bar(y)[2]-bar(y)[1]), ylab = "Densidad", main = "M0 - Diferencia de medias", xlim = c(-10,30), cex.axis = 0.9)
abline(v = mean(Y[,2]) - mean(Y[,1]), col = 2, lwd = 2)
# M0 correlacion
hist(TS0[,2], freq = F, col = "lightyellow2", border = "lightyellow2", xlab = expression(r[12]), ylab = "Densidad", main = "M0 - Correlación", xlim = c(0,1), cex.axis = 0.9)
abline(v = cor(Y[,1],Y[,2]), col = 2, lwd = 2)
# M1 diferencia de medias
hist(TS[,1], freq = F, col = "gray90", border = "gray90", xlab = expression(bar(y)[2]-bar(y)[1]), ylab = "Densidad", main = "M1 - Diferencia de medias", xlim = c(-10,30), cex.axis = 0.9)
abline(v = mean(Y[,2]) - mean(Y[,1]), col = 2, lwd = 2)
# M1 correlacion
hist(TS[,2], freq = F, col = "gray90", border = "gray90", xlab = expression(r[12]), ylab = "Densidad", main = "M1 - Correlación", xlim = c(0,1), cex.axis = 0.9)
abline(v = cor(Y[,1],Y[,2]), col = 2, lwd = 2)
```

### DIC


```{r, echo = F}
# M0 - DIC
theta_hat <- colMeans(PHI[,1,])
sig2_hat  <- colMeans(PHI[,2,])
lpyth <- 0
for (j in 1:p) 
  lpyth <- lpyth + sum(dnorm(x = Y[,j], mean = theta_hat[j], sd = sqrt(sig2_hat[j]), log = T))
LP0 <- rowSums(PHI[,3,1:p])
pDIC_m0 <- 2*(lpyth - mean(LP0))
dic_m0  <- -2*lpyth + 2*pDIC_m0 
```


```{r, echo=F}
# M1 - DIC
theta_hat <- colMeans(THETA)
Sigma_hat <- matrix(data = colMeans(SIGMA), nrow = p, ncol = p)
lpyth     <- sum(apply(X = Y, MARGIN = 1, FUN = function(y) mvtnorm::dmvnorm(x = y, mean = theta_hat, sigma = Sigma_hat, log = T)))
pDIC_m1   <- 2*(lpyth - mean(LP))
dic_m1    <- -2*lpyth + 2*pDIC_m1 
```


```{r,echo=F}
# resultados
tab <- rbind(c(pDIC_m0, dic_m0), c(pDIC_m1, dic_m1))
colnames(tab) <- c("pDIC","DIC")
rownames(tab) <- c("Modelo 0", "Modelo 1")
knitr::kable(x = tab, digits = 3, align = "c")
```


```{r, fig.width=4, fig.height=4, echo=F, fig.align='center'}
# grafico log-verosimilitud
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# histogramas
plot(x = NA, y = NA, ylab = "Densidad", xlab = "Log-verosimilitud", cex.axis = 0.7, xlim = c(-190,-170), ylim = c(0,0.4)) 
hist(LP0, freq = F, add = T, col = "lightyellow2", border = "lightyellow2")
hist(LP , freq = F, add = T, col = "gray90", border = "gray90")
abline(v = mean(LP0), lty = 2, col = "yellow4")
abline(v = mean(LP ), lty = 2, col = "black")
legend("topleft", legend = c("Modelo 0","Modelo 1"), fill = c("lightyellow2","gray90"), border = c("lightyellow2","gray90"), bty = "n")
```

### Validación Cruzada

Establecer los **datos de entranamiento** (*training data*) y los **datos de prueba** (*testing data*).

```{r}
# fold de tamaño 4
set.seed(1)
na_index <- sort(sample(x = 1:n, size = ceiling(n/5), replace = F))
# datos con NA
Yna <- Y
Yna[na_index,] <- NA
head(Yna)
```
Valores observados (datos de prueba):

```{r}
Ytrue <- Y[na_index,]
Ytrue
```

Ajustar los modelos estimando los valores perdidos por medio de la **distribución predictiva posterior**.

Se usan como métrica de evaluación: 

- El **error cuadrático medio estándar** (*Rooted Mean Square Error*), $\textsf{RMSE} = \sqrt{\frac{1}{m}\sum_{i=1}^m(y_i - \hat{y}_i)^2}$, donde $m$ es tamaño de los datos de prueba.
- El **coeficiente de correlación** entre $y_i,\ldots,y_n$ y $\hat{y}_1,\ldots,\hat{y}_n$.


Modelo 0:

```{r}
# hiperparametros
mu0 <- 50
t20 <- 278
nu0 <- 1
s20 <- 278
# almacenamiento
Yhat_m0 <- NULL
for (j in 1:p) {
  # datos
  y <- Yna[,j]
  n <- length(y)
  # datos perdidos
  na_index <- is.na(y)
  n_na <- sum(na_index)
  # inicializar
  y0    <- 0
  theta <- mean(y, na.rm = T)
  sig2  <- var (y, na.rm = T)
  # cadena
  set.seed(1)
  for (b in 1:B) {
    # imputacion
    y[na_index] <- rnorm(n = n_na, mean = theta, sd = sqrt(sig2))
    # estadisticos 
    sum_y  <- sum (y)
    mean_y <- mean(y)
    var_y  <- var (y)
    # actualizar theta
    t2n   <- 1/(1/t20 + n/sig2)      
    theta <- rnorm(n = 1, mean = t2n*(mu0/t20 + sum_y/sig2), sd = sqrt(t2n))
    # actualizar sig2
    sig2 <- 1/rgamma(n = 1, shape = 0.5*(nu0 + n), rate = 0.5*(nu0*s20 + (n-1)*var_y + n*(mean_y - theta)^2))
    # estimacion datos faltantes
    y0 <- y0 + y[na_index]/B
  }
  Yhat_m0 <- cbind(Yhat_m0, y0)
}
```


```{r}
# M0 - metricas
RMSE_m0 <- sqrt(mean((c(Ytrue) - c(Yhat_m0))^2))
r_m0    <- cor(c(Ytrue), c(Yhat_m0))
```


Modelo 1:


```{r}
# hiperparametros
mu0 <- c(50,50)
L0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
nu0 <- 4
S0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
# datos perdidos
n <- nrow(Yna)
na_index <- which(!complete.cases(Yna))
n_na <- length(na_index)
# inicializar
Yhat_m1 <- 0
theta   <- colMeans(Yna, na.rm = T)
Sigma   <- cov(Yna, use = "complete.obs")
# cadena
iL0 <- solve(L0)
Lm0 <- solve(L0)%*%mu0
set.seed(1)
for (b in 1:B) {
  # imputacion
  for (i in 1:n_na)
    Yna[na_index[i],] <- c(mvtnorm::rmvnorm(n = 1, mean = theta, sigma = Sigma))
  # estadisticos
  yb  <- colMeans(Yna)
  SS  <- cov(Yna)
  SSn <- S0 + (n-1)*SS
  # actualizar theta
  iSigma <- solve(Sigma)
  Ln     <- solve(iL0 + n*iSigma)
  theta  <- c(mvtnorm::rmvnorm(n = 1, mean = Ln%*%(Lm0 + n*(iSigma%*%yb)), sigma = Ln))
  # actualizar Sigma
  Sigma <- solve(rWishart::rWishart(n = 1, df = nu0 + n, Sigma = solve(SSn + n*((yb - theta)%*%t(yb - theta))))[,,1])
  # estimaciondatos faltantes
  Yhat_m1 <- Yhat_m1 + Yna[na_index,]/B
}
```

```{r}
# M1 - metricas
RMSE_m1 <- sqrt(mean((c(Ytrue) - c(Yhat_m1))^2))
r_m1    <- cor(c(Ytrue), c(Yhat_m1))
```

```{r}
# resultados
tab <- rbind(c(RMSE_m0, r_m0), c(RMSE_m1, r_m1))
colnames(tab) <- c("RMSE","r")
rownames(tab) <- c("Modelo 0", "Modelo 1")
knitr::kable(x = tab, digits = 3, align = "c")
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```
