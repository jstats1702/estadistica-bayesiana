---
title: "Modelo Normal Multivariado"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\simiid}{\,{\stackrel{\text{iid}}{\sim}}\,}
\newcommand{\simind}{\,{\stackrel{\text{ind}}{\sim}}\,}

\newcommand{\te}{\theta}
\newcommand{\si}{\sigma}

\newcommand{\xv}{\boldsymbol{x}}
\newcommand{\yv}{\boldsymbol{y}}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\tev}{\boldsymbol{\theta}}
\newcommand{\omev}{\boldsymbol{\omega}}
\newcommand{\xiv}{\boldsymbol{\xi}}

\newcommand{\Wm}{\mathbf{W}}
\newcommand{\Ym}{\mathbf{Y}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\SIG}{\mathbf{\Sigma}}
\newcommand{\LAM}{\mathbf{\Lambda}}

\newcommand{\Nor}{\small{\textsf{N}}}
\newcommand{\Cat}{\small{\textsf{Categorica}}}
\newcommand{\Dir}{\small{\textsf{Dirichlet}}}
\newcommand{\IG} {\small{\textsf{GI}}}


# Modelo

El modelo Normal multivariado para secuencia de observaciones $\yv_1,\ldots,\yv_n$, donde $\yv_i = (y_{i,1},\ldots,y_{i,p})\in\mathbb{R}^p$ para $i=1,\ldots,n$, está dado por:
$$
\begin{align*}
	\yv_i\mid\tev,\SIG &\stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)\\
	(\tev,\SIG)  &\sim p(\tev,\SIG) \,.
\end{align*}
$$
El modelo tiene $k=p+p(p+1)/2$ parámetros desconocidos, a saber, el vector de medias $\tev$ y la matriz de covarianzas $\SIG$. 

El vector aleatorio $\yv=(y_1,\ldots,y_p)$ tiene distribución **Normal multivariada** si la función de densidad de probabilidad $\boldsymbol{y}$ está dada por
$$
p(\yv\mid\tev,\SIG) = (2\pi)^{-p/2}\,|\SIG|^{-1/2}\,\exp\left\{ -\frac12(\yv-\tev)^{\textsf{T}} \SIG^{-1} (\yv-\tev) \right\}\,,
$$
donde
$$
\tev = (\theta_1,\ldots,\theta_p)
\qquad\text{y}\qquad
\SIG = 
\begin{bmatrix} 
  \si_1^2     & \si_{1,2} & \cdots & \si_{1,p} \\
  \si_{2,1}^2 & \si_2^2   & \cdots & \si_{2,p} \\
  \vdots      & \vdots    & \ddots & \vdots    \\
  \si_{p,1}   & \si_{p,2} & \cdots & \si_p^2 \\
\end{bmatrix}\,,
$$
con $\SIG$ simétrica, $\SIG^{\textsf{T}}=\SIG$, y definida positiva, $\xv^{\textsf{T}}\SIG\xv > 0$ para todo $\xv\in\mathbb{R}^p$. 

El **núcleo** de esta distribución es:
$$
p(\yv\mid\tev,\SIG) \propto \exp\left\{ -\frac12\left[ \yv^{\textsf{T}}\SIG^{-1}\yv - 2\yv^{\textsf{T}}\SIG^{-1}\tev \right] \right\}\,.
$$

# Estadísticos Suficientes

Si $\yv_i\mid\tev,\SIG\stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)$, con $i=1,\ldots,n$, entonces la distribución muestral conjunta de las observaciones es:
$$
\begin{align*}
p\left(\mathbf{Y} \mid \tev,\SIG \right) 
&= \prod_{i=1}^n \left(2 \pi\right)^{-p / 2} |\SIG|^{-1/2} \exp\left\{ -\frac12(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) \right\} \\
&= \left(2 \pi\right)^{-np / 2} |\SIG|^{-n/2} \exp\left\{ -\frac12\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) \right\}\,,
\end{align*}
$$
donde $\mathbf{Y}=[y_{i,j}]$.

Como
$$
\begin{align*}
\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) 
&= \sum_{i=1}^n\yv_i^{\textsf{T}}\SIG^{-1}\yv_i - 2\sum_{i=1}^n\yv_i^{\textsf{T}}\SIG^{-1}\tev + n\tev^{\textsf{T}}\SIG^{-1}\tev \\
&= \textsf{traza}\left( \SIG^{-1} \sum_{i=1}^n\yv_i\yv_i^{\textsf{T}} \right) - 2\left(\sum_{i=1}^n\yv_i\right)^{\textsf{T}}\SIG^{-1}\tev + n\tev^{\textsf{T}}\SIG^{-1}\tev\\
\end{align*}
$$
entonces
$$
\left( \sum_{i=1}^n\yv_i, \sum_{i=1}^n\yv_i\yv_i^{\textsf{T}} \right)
$$
es un estadístico suficiente para $(\tev,\SIG)$.

La media muestral y la matriz de covarianza muestral $(\bar{\boldsymbol y},\Sm)$ también constituyen un estadístico suficiente para $(\tev,\SIG)$ dado que
$$
\sum_{i=1}^n(\yv_i-\tev)^{\textsf{T}} \SIG^{-1} (\yv_i-\tev) = 
\textsf{traza}\left( \SIG^{-1}\left[ (n-1)\Sm + n(\bar{\boldsymbol y} - \tev)(\bar{\boldsymbol y} - \tev)^{\textsf{T}}  \right] \right)
$$
donde
$$
\bar{\boldsymbol y} = \frac{1}{n}\sum_{i=1}^n \boldsymbol{y}_i
\qquad\text{y}\qquad
\Sm = \frac{1}{n-1}\sum_{i=1}^n (\boldsymbol{y}_i - \bar{\boldsymbol y})(\boldsymbol{y}_i - \bar{\boldsymbol y})^{\textsf{T}}\,.
$$

# Modelo Normal Multivariado Semi-Conjugado

- **Distribución muestral**:
$$
\yv_i\mid\tev,\SIG \stackrel{\text{iid}}{\sim} \textsf{N}(\tev,\SIG)\,,\qquad i=1,\ldots,n\,.
$$
- **Distribución previa**:
$$
p(\tev,\SIG) = p(\tev)\,p(\SIG) 
$$
donde
$$
\begin{align*}
\tev  &\sim \textsf{N} (\muv_0, \LAM_0) \\
\SIG  &\sim \textsf{WI}(\nu_0,\Sm_0^{-1})
\end{align*}
$$
- **Parámetros**: $\tev$, $\SIG$.

- **Hiperparámetros**: $\muv_0, \LAM_0, \nu_0, \Sm_0$.

## Distribución Wishart {-}

La matriz aleatoria $\Wm > 0$ de $p \times p$ tiene **distribución Wishart** con parámetros $\nu > p+1$ (grados de libertad) y $\Sm > 0$ (matriz de escala de $p\times p$), i.e., $\Wm\sim\textsf{W}(\nu,\Sm)$, si su función de densidad de probabilidad es
$$
p(\Wm\mid\nu,\Sm) \propto |\Wm|^{(\nu-p-1)/2}\,\exp\left\{ -\frac12\textsf{traza}(\Sm^{-1}\Wm)  \right\}\,.
$$

- Si $\Wm\sim\textsf{W}(\nu,\Sm)$, entonces $\textsf{E}(\Wm) = \nu\Sm$.

## Distribución Wishart Inversa {-}

La matriz aleatoria $\Wm > 0$ de $p \times p$ tiene **distribución Wishart Inversa** con parámetros $\nu > p+1$ (grados de libertad) y $\Sm > 0$ (matriz de escala de $p\times p$), i.e., $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$, si su función de densidad de probabilidad es
$$
p(\Wm\mid\nu,\Sm^{-1}) \propto |\Wm|^{-(\nu+p+1)/2}\,\exp\left\{ -\frac12\textsf{traza}(\Sm\Wm^{-1}) \right\}\,.
$$

- Si $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$, entonces $\textsf{E}(\Wm) = \frac{1}{\nu-p-1}\Sm$ para $\nu > p+1$.

- Si $\Wm^{-1}\sim\textsf{W}(\nu,\Sm)$, entonces $\Wm\sim\textsf{WI}(\nu,\Sm^{-1})$.


# Distribuciones condicionales completas

El **muestreador de Gibbs** (*Gibbs sampler*) es un algoritmo iterativo que permite **obtener muestras dependientes** de la **distribución posterior** por medio de las **distribuciones condicionales completas**.

- La distribución condicional completa de $\tev$ es $\tev\mid\text{resto}\sim\textsf{N}(\muv_n,\LAM_n)$, donde
$$
\muv_n = \left( \LAM_0^{-1} + n\SIG^{-1} \right)^{-1} \left( \LAM_0^{-1}\muv_0 + n\SIG^{-1}\bar{\boldsymbol{y}} \right)
\qquad\text{y}\qquad
\LAM_n = \left( \LAM_0^{-1} + n\SIG^{-1} \right)^{-1}\,.
$$

- La distribución condicional completa de $\SIG$ es $\SIG\mid\text{resto}\sim\textsf{WI}(\nu_n,\Sm_n^{-1})$, donde
$$
\nu_n = \nu_0 + n
\qquad\text{y}\qquad
\Sm_n = \Sm_0 + \sum_{i=1}^n (\boldsymbol{y}_i - \tev)(\boldsymbol{y}_i - \tev)^{\textsf{T}} = \Sm_0 + (n-1)\Sm + n(\bar{\boldsymbol y} - \tev)(\bar{\boldsymbol y} - \tev)^{\textsf{T}} \,.
$$

# Muetreador de Gibbs

Dado un **estado actual** de los parámetros del modelo $(\tev^{(b)}, \SIG^{(b)})$, se genera un nuevo estado $(\tev^{(b+1)}, \SIG^{(b+1)})$ como sigue:

1. Muestrar $\tev^{(b+1)}\sim p(\tev\mid\SIG^{(b)},  \boldsymbol{y})$.
2. Muestrar $\SIG^{(b+1)}\sim p(\SIG\mid\tev^{(b+1)},\boldsymbol{y})$.
3. Almacenar $(\tev^{(b+1)}, \SIG^{(b+1)})$ 
4. Repetir los pasos 1. a 3. hasta convergencia.

Este algoritmo genera una **secuencia dependiente** de parámetros $(\tev^{(1)}, \SIG^{(1)}),\ldots,(\tev^{(B)}, \SIG^{(B)})$ de la distribución posterior $p(\tev,\SIG \mid \boldsymbol{y})$. 


# Ejemplo: Comprensión de Lectura

A una muestra de 22 niños se les dan pruebas de comprensión de lectura antes y después de recibir un método de instrucción en particular. 

Cada estudiante $i$ tiene asociados dos variables $y_{i,1}$ y $y_{i,2}$ que denotan los **puntajes antes y después de la instrucción**, respectivamente.

El  objetivo es **evaluar la efectividad del método de enseñanza** junto con **la consistencia de la prueba de comprensión de lectura**.

***Hoff, P. D. (2009). A first course in Bayesian statistical methods (Vol. 580). New York: Springer.***

```{r}
# datos
Y <- structure(c(59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 
                 70, 34, 50, 41, 52, 60, 34, 28, 35, 77, 39, 46, 26, 
                 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 
                 60, 75, 47, 48, 33), 
               .Dim = c(22L, 2L), .Dimnames = list(NULL, c("pretest", "posttest")))
(n <- nrow(Y))
(p <- ncol(Y))
```


```{r}
# inspeccionar datos
summary(Y)
```


```{r,echo = FALSE, fig.width=4,fig.height=4, fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
plot(x = Y[,1], y = Y[,2], pch = 16, col = 4, cex = 1.1,
     xlab = "Pre-test", ylab = "Pos-test", main = paste0("Coef. Correlación: ", round(cor(Y[,1],Y[,2]), 3)))
abline(v = mean(Y[,1]), col = "gray85", lty = 2)
abline(h = mean(Y[,2]), col = "gray85", lty = 2)
lines(x = mean(Y[,1]), y = mean(Y[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
```


```{r, echo=FALSE, fig.width=8,fig.height=8, fig.align='center'}
# gráfico
par(mfrow = c(2,2), mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# histogramas
hist(Y[,1], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pre-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,1]), sd = sd(Y[,1])), col = 2, lty = 3, add = T)
hist(Y[,2], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pos-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,2]), sd = sd(Y[,2])), col = 2, lty = 3, add = T)
# qqplots
qqnorm(Y[,1], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,1], col = 2)
qqnorm(Y[,2], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,2], col = 2)
```


```{r}
# estadísticos suficientes
yb <- c(colMeans(Y))
round(yb, 1)
SS <- cov(Y)
round(SS, 1) 
```


## Elicitación de lo hiperparámetros

El examen fue diseñado para dar puntajes promedio de 50 de 100: $\muv_0 = (50, 50)$.

Usar una varianza previa tal que $\textsf{Pr}( 0 < \theta_j < 100 ) \approx 0.99$: $\sigma^2_{0,1}=\sigma^2_{0,2}=(50/3)^2 \approx 278$.

Usar correlación previa tal que $\rho_0 = 0.5$: $\sigma_{0,12} = (0.5)(50/3)^2 \approx 139$.


```{r}
# previa
mu0 <- c(50,50)
L0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
nu0 <- 4
S0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
```


## Ajuste del Modelo Normal MUltivariado Semi-Conjugado

```{r}
# inicializar
theta <- yb
Sigma <- SS
# número de muestras
B <- 10000
ncat <- floor(B/10)
# almacenamiento
THETA <- SIGMA <- YS <- LP <- NULL
# cadena
iL0 <- solve(L0)
Lm0 <- solve(L0)%*%mu0
nun <- nu0 + n
SSn <- S0 + (n-1)*SS
set.seed(1)
for (b in 1:B) {
  # actualizar theta
  iSigma <- solve(Sigma)
  Ln     <- solve(iL0 + n*iSigma)
  theta  <- c(mvtnorm::rmvnorm(n = 1, mean = Ln%*%(Lm0 + n*(iSigma%*%yb)), sigma = Ln))
  # actualizar Sigma
  Sigma <- solve(rWishart::rWishart(n = 1, df = nun, Sigma = solve(SSn + n*((yb - theta)%*%t(yb - theta))))[,,1])
  # predictiva posterior
  YS <- rbind(YS, mvtnorm::rmvnorm(n = 1, mean = theta, sigma = Sigma)) 
  # log-verosimilitud
  LP[b] <- sum(apply(X = Y, MARGIN = 1, FUN = function(y) mvtnorm::dmvnorm(x = y, mean = theta, sigma = Sigma, log = T)))
  # almacenar
  THETA <- rbind(THETA, theta)
  SIGMA <- rbind(SIGMA, c(Sigma))
  # progreso
  if (b%%ncat == 0) cat(100*round(b/B, 1), "% completado ... \n", sep = "")
}
colnames(THETA) <- c("theta1", "theta1")
colnames(SIGMA) <- c("sigma1^2", "sigma21", "sigma12", "sigma2^2")
```


## Convergencia

Cadena de la log-verosimilitud:

```{r, echo = FALSE, fig.width=8,fig.height=4, fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,.5), mgp = c(1.7,0.7,0))
plot(x = 1:B, y = LP, type = "p", pch = ".", xlab = "Iteración", ylab = "Log-verosimilitud", main = "Modelo Normal Multivariado")
```

Errores estándar de Monte Carlo:

```{r}
# errores estándar de Monte Carlo
round(apply(X = THETA, MARGIN = 2, FUN = sd)/sqrt(coda::effectiveSize(THETA)), 3)
round(apply(X = SIGMA, MARGIN = 2, FUN = sd)/sqrt(coda::effectiveSize(SIGMA)), 3)
```

## Inferencia

Distribución posterior de $\tev$ y distribución predictiva posterior:

```{r,echo=FALSE,fig.width=8,fig.height=8,fig.align='center'}
# gráfico
par(mfrow = c(2,1), mar = c(2.75,2.75,1.5,.5), mgp = c(1.5,0.5,0))
# theta 2 vs theta 1
plot (x = THETA[,1], y = THETA[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(theta[1]), ylab = expression(theta[2]), main = "", cex.axis = 0.9)
abline(v = mean(THETA[,1]), col = "gray85", lty = 2)
abline(h = mean(THETA[,2]), col = "gray85", lty = 2)
lines(x = THETA[,1], y = THETA[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
# y* 2 vs y* 1
plot (x = YS[,1], y = YS[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(tilde(y)[1]), ylab = expression(tilde(y)[2]), main = "", cex.axis = 0.9)
abline(v = mean(YS[,1]), col = "gray85", lty = 2)
abline(h = mean(YS[,2]), col = "gray85", lty = 2)
lines(x = YS[,1], y = YS[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
```


Inferencia sobre $\theta_2 - \theta_1$:

¿Cuál es la probabilidad posterior de que la calificación promedio del segundo examen sea mayor que la del primero?

```{r}
round(mean(THETA[,2] - THETA[,1] > 0), 3)
```


```{r}
round(quantile(THETA[,2] - THETA[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\tilde{y}_2 - \tilde{y}_1$:

¿Cuál es la probabilidad posterior de que un niño seleccionado al azar obtenga una puntuación más alta en el segundo examen que en el primero?


```{r}
round(mean(YS[,2] - YS[,1] > 0), 3)
```


```{r}
round(quantile(YS[,2] - YS[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\rho=\frac{\sigma_{1,2}}{\sigma_{1}\,\sigma_{2}}$:

¿Las pruebas son consistentes? ¿Cuál es la probabilidad posterior de que la correlación entre las calificaciones sea superior a 0.6?


```{r}
# muestras de rho
RHO <- SIGMA[,2]/sqrt(SIGMA[,1]*SIGMA[,4])
```


```{r,echo=FALSE,fig.width=4,fig.height=4,fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
hist(RHO, freq = F, col = "gray90", border = "gray90", xlim = c(0,1), xlab = expression(rho), ylab = "Densidad", main = "")
abline(v = quantile(RHO, prob = c(0.025,0.5,0.975)), col = c(4,2,4), lty = c(4,2,4))
```


```{r}
round(mean(RHO > 0.6), 3)
```


```{r}
round(quantile(RHO, prob = c(0.025,0.5,0.975)), 3)
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```
