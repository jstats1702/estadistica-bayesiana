---
title: "Modelo Normal Multivariado"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modelo

El modelo **Normal multivariado** para una secuencia de observaciones \(\boldsymbol{y}_1, \ldots, \boldsymbol{y}_n\), donde \(\boldsymbol{y}_i = (y_{i,1}, \ldots, y_{i,p}) \in \mathbb{R}^p\) para \(i = 1, \ldots, n\), se define como:

\[
\begin{aligned}
    \boldsymbol{y}_i \mid \boldsymbol{\theta}, \mathbf{\Sigma} &\stackrel{\text{iid}}{\sim} \textsf{N}(\boldsymbol{\theta}, \mathbf{\Sigma}), \\
    (\boldsymbol{\theta}, \mathbf{\Sigma}) &\sim p(\boldsymbol{\theta}, \mathbf{\Sigma}),
\end{aligned}
\]
donde \(\boldsymbol{\theta}\) es el vector de medias y \(\mathbf{\Sigma}\) es la matriz de covarianzas. 

El modelo involucra \(k = p + \frac{p(p+1)}{2}\) parámetros desconocidos por estimar. 

El vector aleatorio \(\boldsymbol{y} = (y_1, \ldots, y_p)\) sigue una distribución **Normal multivariada** si su función de densidad de probabilidad está dada por:
\[
p(\boldsymbol{y} \mid \boldsymbol{\theta}, \mathbf{\Sigma}) = (2\pi)^{-p/2} |\mathbf{\Sigma}|^{-1/2} \exp\left\{ -\frac{1}{2} (\boldsymbol{y} - \boldsymbol{\theta})^{\textsf{T}} \mathbf{\Sigma}^{-1} (\boldsymbol{y} - \boldsymbol{\theta}) \right\},
\]
donde:

\[
\boldsymbol{\theta} = (\theta_1, \ldots, \theta_p),
\qquad
\mathbf{\Sigma} = 
\begin{bmatrix} 
  \sigma_1^2     & \sigma_{1,2} & \cdots & \sigma_{1,p} \\
  \sigma_{2,1}   & \sigma_2^2   & \cdots & \sigma_{2,p} \\
  \vdots         & \vdots       & \ddots & \vdots       \\
  \sigma_{p,1}   & \sigma_{p,2} & \cdots & \sigma_p^2 \\
\end{bmatrix}.
\]
La matriz \(\mathbf{\Sigma}\) es simétrica (\(\mathbf{\Sigma}^{\textsf{T}} = \mathbf{\Sigma}\)) y definida positiva, lo que implica que \(\boldsymbol{x}^{\textsf{T}} \mathbf{\Sigma} \boldsymbol{x} > 0\) para todo \(\boldsymbol{x} \in \mathbb{R}^p\).

El **núcleo** de la distribución Normal multivariada está dado por:
\[
p(\boldsymbol{y} \mid \boldsymbol{\theta}, \mathbf{\Sigma}) \propto \exp\left\{ -\frac{1}{2} \left[ \boldsymbol{y}^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{y} - 2 \boldsymbol{y}^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\theta} \right] \right\}.
\]

# Estadísticos Suficientes

Si \(\boldsymbol{y}_i \mid \boldsymbol{\theta}, \mathbf{\Sigma} \stackrel{\text{iid}}{\sim} \textsf{N}(\boldsymbol{\theta}, \mathbf{\Sigma})\), con \(i = 1, \ldots, n\), entonces la distribución conjunta muestral de las observaciones está dada por:
\[
\begin{aligned}
p\left(\mathbf{Y} \mid \boldsymbol{\theta}, \mathbf{\Sigma} \right) 
&= \prod_{i=1}^n \left(2 \pi\right)^{-p / 2} |\mathbf{\Sigma}|^{-1/2} \exp\left\{ -\frac{1}{2} (\boldsymbol{y}_i - \boldsymbol{\theta})^{\textsf{T}} \mathbf{\Sigma}^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) \right\} \\
&= \left(2 \pi\right)^{-np / 2} |\mathbf{\Sigma}|^{-n / 2} \exp\left\{ -\frac{1}{2} \sum_{i=1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^{\textsf{T}} \mathbf{\Sigma}^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) \right\},
\end{aligned}
\]
donde \(\mathbf{Y} = [\boldsymbol{y}_1^{\textsf{T}}, \ldots, \boldsymbol{y}_n^{\textsf{T}}]^{\textsf{T}}\).

La expresión para la suma cuadrática en el exponente se desarrolla como:
\[
\begin{aligned}
\sum_{i=1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^{\textsf{T}} \mathbf{\Sigma}^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) 
&= \sum_{i=1}^n \boldsymbol{y}_i^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{y}_i - 2 \sum_{i=1}^n \boldsymbol{y}_i^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\theta} + n \boldsymbol{\theta}^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\theta} \\
&= \textsf{traza}\left( \mathbf{\Sigma}^{-1} \sum_{i=1}^n \boldsymbol{y}_i \boldsymbol{y}_i^{\textsf{T}} \right) - 2 \left(\sum_{i=1}^n \boldsymbol{y}_i\right)^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\theta} + n \boldsymbol{\theta}^{\textsf{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\theta}.
\end{aligned}
\]
Por lo tanto, el par de estadísticas \(\left( \sum_{i=1}^n \boldsymbol{y}_i, \sum_{i=1}^n \boldsymbol{y}_i \boldsymbol{y}_i^{\textsf{T}} \right)\) constituye un estadístico suficiente para \((\boldsymbol{\theta}, \mathbf{\Sigma})\).

Además, la media muestral \(\bar{\boldsymbol{y}}\) y la matriz de covarianza muestral \(\mathbf{S}\) también forman un estadístico suficiente para \((\boldsymbol{\theta}, \mathbf{\Sigma})\), dado que:
\[
\sum_{i=1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})^{\textsf{T}} \mathbf{\Sigma}^{-1} (\boldsymbol{y}_i - \boldsymbol{\theta}) = 
\textsf{traza}\left( \mathbf{\Sigma}^{-1} \left[ (n-1) \mathbf{S} + n (\bar{\boldsymbol{y}} - \boldsymbol{\theta})(\bar{\boldsymbol{y}} - \boldsymbol{\theta})^{\textsf{T}} \right] \right),
\]
donde:
\[
\bar{\boldsymbol{y}} = \frac{1}{n} \sum_{i=1}^n \boldsymbol{y}_i
\quad \text{y} \quad
\mathbf{S} = \frac{1}{n-1} \sum_{i=1}^n (\boldsymbol{y}_i - \bar{\boldsymbol{y}})(\boldsymbol{y}_i - \bar{\boldsymbol{y}})^{\textsf{T}}.
\]

# Modelo Normal Multivariado Semi-Conjugado

Para modelar una colección de observaciones Normales multivariadas bajo un enfoque Bayesiano **semiconjugado**, se consideran las siguientes especificaciones:

- **Distribución muestral**:  
\[
\boldsymbol{y}_i \mid \boldsymbol{\theta}, \mathbf{\Sigma} \stackrel{\text{iid}}{\sim} \textsf{N}(\boldsymbol{\theta}, \mathbf{\Sigma}), \qquad i = 1, \ldots, n.
\]

- **Distribución previa**:  
\[
p(\boldsymbol{\theta}, \mathbf{\Sigma}) = p(\boldsymbol{\theta}) \, p(\mathbf{\Sigma}),
\]  
donde:  
\[
\begin{aligned}
\boldsymbol{\theta} &\sim \textsf{N}(\boldsymbol{\mu}_0, \mathbf{\Lambda}_0), \\
\mathbf{\Sigma} &\sim \textsf{WI}(\nu_0, \mathbf{S}_0^{-1}).
\end{aligned}
\]

- **Parámetros**:  
\(\boldsymbol{\theta}\) y \(\mathbf{\Sigma}\).  

- **Hiperparámetros**:  
\(\boldsymbol{\mu}_0\), \(\mathbf{\Lambda}_0\), \(\nu_0\), y \(\mathbf{S}_0\).

## Distribución Wishart {-}

La matriz aleatoria \(\mathbf{W} > 0\) de dimensión \(p \times p\) sigue una **distribución Wishart** con parámetros \(\nu > p - 1\) (grados de libertad) y \(\mathbf{S} > 0\) (matriz de escala de \(p \times p\)), denotada como \(\mathbf{W} \sim \textsf{W}(\nu, \mathbf{S})\), si su función de densidad de probabilidad está dada por:
\[
p(\mathbf{W} \mid \nu, \mathbf{S}) \propto |\mathbf{W}|^{(\nu - p - 1) / 2} \exp\left\{ -\frac{1}{2} \textsf{traza}(\mathbf{S}^{-1} \mathbf{W}) \right\}.
\]

Si \(\mathbf{W} \sim \textsf{W}(\nu, \mathbf{S})\), entonces la matriz de esperanza está dada por:
\[
\textsf{E}(\mathbf{W}) = \nu \mathbf{S}.
\]

## Distribución Wishart Inversa {-}

La matriz aleatoria \(\mathbf{W} > 0\) de dimensión \(p \times p\) sigue una **distribución Wishart Inversa** con parámetros \(\nu > p+1\) (grados de libertad) y \(\mathbf{S} > 0\) (matriz de escala de \(p \times p\)), denotada como \(\mathbf{W} \sim \textsf{WI}(\nu, \mathbf{S}^{-1})\), si su función de densidad de probabilidad está dada por:

\[
p(\mathbf{W} \mid \nu, \mathbf{S}^{-1}) \propto |\mathbf{W}|^{-(\nu + p + 1)/2} \exp\left\{ -\frac{1}{2} \textsf{traza}(\mathbf{S} \mathbf{W}^{-1}) \right\}.
\]

Si \(\mathbf{W} \sim \textsf{WI}(\nu, \mathbf{S}^{-1})\), entonces su valor esperado es:
\[
\textsf{E}(\mathbf{W}) = \frac{1}{\nu - p - 1} \mathbf{S}, \qquad \text{para } \nu > p + 1.
\]

Si \(\mathbf{W}^{-1} \sim \textsf{W}(\nu, \mathbf{S})\), entonces \(\mathbf{W} \sim \textsf{WI}(\nu, \mathbf{S}^{-1})\).

# Distribuciones condicionales completas

El **muestreador de Gibbs** (*Gibbs sampler*) es un algoritmo iterativo utilizado para generar **muestras dependientes** de una **distribución posterior conjunta** mediante las **distribuciones condicionales completas** de los parámetros involucrados. A continuación se detallan las distribuciones condicionales completas para un modelo normal multivariado:

- **Distribución condicional completa de \(\boldsymbol{\theta}\)**:  
\(\boldsymbol{\theta} \mid \text{resto} \sim \textsf{N}(\boldsymbol{\mu}_n, \mathbf{\Lambda}_n)\), donde:  
\[
\boldsymbol{\mu}_n = \left( \mathbf{\Lambda}_0^{-1} + n \mathbf{\Sigma}^{-1} \right)^{-1} \left( \mathbf{\Lambda}_0^{-1} \boldsymbol{\mu}_0 + n \mathbf{\Sigma}^{-1} \bar{\boldsymbol{y}} \right),  
\qquad  
\mathbf{\Lambda}_n = \left( \mathbf{\Lambda}_0^{-1} + n \mathbf{\Sigma}^{-1} \right)^{-1}.
\]

- **Distribución condicional completa de \(\mathbf{\Sigma}\)**:  
\(\mathbf{\Sigma} \mid \text{resto} \sim \textsf{WI}(\nu_n, \mathbf{S}_n^{-1})\), donde:  
\[
\nu_n = \nu_0 + n,  
\qquad  
\mathbf{S}_n = \mathbf{S}_0 + \sum_{i=1}^n (\boldsymbol{y}_i - \boldsymbol{\theta})(\boldsymbol{y}_i - \boldsymbol{\theta})^{\textsf{T}}.
\]

El término \(\mathbf{S}_n\) puede reescribirse en términos de la matriz de covarianza muestral \(\mathbf{S}\) y la media muestral \(\bar{\boldsymbol{y}}\) como:  
\[
\mathbf{S}_n = \mathbf{S}_0 + (n-1) \mathbf{S} + n (\bar{\boldsymbol{y}} - \boldsymbol{\theta})(\bar{\boldsymbol{y}} - \boldsymbol{\theta})^{\textsf{T}}.
\]

Estas distribuciones condicionales completas son la base para actualizar secuencialmente los parámetros \(\boldsymbol{\theta}\) y \(\mathbf{\Sigma}\) en cada iteración del algoritmo, lo que permite aproximar la distribución posterior conjunta de forma eficiente.

# Muetreador de Gibbs

Dado un **estado actual** de los parámetros del modelo \((\boldsymbol{\theta}^{(b)}, \mathbf{\Sigma}^{(b)})\), se genera un nuevo estado \((\boldsymbol{\theta}^{(b+1)}, \mathbf{\Sigma}^{(b+1)})\) siguiendo estos pasos:

1. Generar \(\boldsymbol{\theta}^{(b+1)} \sim p(\boldsymbol{\theta} \mid \mathbf{\Sigma}^{(b)}, \boldsymbol{y})\), la distribución condicional completa de \(\boldsymbol{\theta}\).  
2. Generar \(\mathbf{\Sigma}^{(b+1)} \sim p(\mathbf{\Sigma} \mid \boldsymbol{\theta}^{(b+1)}, \boldsymbol{y})\), la distribución condicional completa de \(\mathbf{\Sigma}\).  
3. Almacenar el par actualizado \((\boldsymbol{\theta}^{(b+1)}, \mathbf{\Sigma}^{(b+1)})\).  
4. Repetir los pasos 1 a 3 hasta alcanzar convergencia.

Este procedimiento genera una **secuencia dependiente** de muestras \((\boldsymbol{\theta}^{(1)}, \mathbf{\Sigma}^{(1)}), \ldots, (\boldsymbol{\theta}^{(B)}, \mathbf{\Sigma}^{(B)})\), que provienen de la distribución posterior conjunta \(p(\boldsymbol{\theta}, \mathbf{\Sigma} \mid \boldsymbol{y})\).

# Ejemplo: Comprensión de lectura

Se realiza un estudio con una muestra de 22 niños a quienes se aplican pruebas de comprensión lectora antes y después de recibir un método de instrucción específico.  

Para cada estudiante \(i\), se registran dos variables: \(y_{i,1}\) y \(y_{i,2}\), que representan los **puntajes obtenidos antes y después de la instrucción**, respectivamente.  

El objetivo principal del análisis es **evaluar la efectividad del método de enseñanza** y **examinar la consistencia de la prueba de comprensión lectora**, es decir, determinar si los puntajes reflejan cambios significativos y coherentes asociados al método empleado.  

**Referencia**:  
Hoff, P. D. (2009). *A first course in Bayesian statistical methods* (Vol. 580). New York: Springer.

```{r}
# Datos: puntajes de comprensión de lectura antes y después de la instrucción
Y <- matrix(
  c(
    59, 43, 34, 32, 42, 38, 55, 67, 64, 45, 49, 72, 34, 70, 34, 50, 41, 52, 60, 34, 28, 35,
    77, 39, 46, 26, 38, 43, 68, 86, 77, 60, 50, 59, 38, 48, 55, 58, 54, 60, 75, 47, 48, 33
  ),
  nrow = 22, ncol = 2,
  dimnames = list(NULL, c("pretest", "posttest"))
)
(n <- nrow(Y))
(p <- ncol(Y))
```


```{r}
# inspeccionar datos
summary(Y)
```


```{r,echo = FALSE, fig.width=4,fig.height=4, fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
plot(x = Y[,1], y = Y[,2], pch = 16, col = 4, cex = 1.1,
     xlab = "Pre-test", ylab = "Pos-test", main = paste0("Coef. Correlación: ", round(cor(Y[,1],Y[,2]), 3)))
abline(v = mean(Y[,1]), col = "gray85", lty = 2)
abline(h = mean(Y[,2]), col = "gray85", lty = 2)
lines(x = mean(Y[,1]), y = mean(Y[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
```


```{r, echo=FALSE, fig.width=8,fig.height=8, fig.align='center'}
# gráfico
par(mfrow = c(2,2), mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
# histogramas
hist(Y[,1], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pre-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,1]), sd = sd(Y[,1])), col = 2, lty = 3, add = T)
hist(Y[,2], freq = F, col = "gray90", border = "gray90", xlim = c(0,100), ylim = c(0,0.03),
     xlab = "Puntaje", ylab = "Densidad", main = "Pos-test")
lines(density(Y[,1]), col = 4)
curve(expr = dnorm(x, mean = mean(Y[,2]), sd = sd(Y[,2])), col = 2, lty = 3, add = T)
# qqplots
qqnorm(Y[,1], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,1], col = 2)
qqnorm(Y[,2], xlim = c(-2,2), ylim = c(0,100), xlab = "Cuantil Normal", ylab = "Cuantil Muestral")
qqline(Y[,2], col = 2)
```


```{r}
# estadísticos suficientes
yb <- c(colMeans(Y))
round(yb, 1)
SS <- cov(Y)
round(SS, 1) 
```


## Elicitación de lo hiperparámetros

El examen fue diseñado para otorgar puntajes con un promedio de 50 sobre 100, lo que define el vector de medias previas como \(\boldsymbol{\mu}_0 = (50, 50)\).  

Se utiliza una varianza previa que asegura que \(\textsf{Pr}(0 < \theta_j < 100) \approx 0.99\). Esto se logra fijando \(\sigma^2_{0,1} = \sigma^2_{0,2} = \left(\frac{50}{3}\right)^2 \approx 278\).  

Adicionalmente, se establece una correlación previa de \(\rho_0 = 0.5\), lo que implica que la covarianza previa es \(\sigma_{0,12} = (0.5) \left(\frac{50}{3}\right)^2 \approx 139\).

\(\nu_0 = 4\) se elige porque es el mínimo requerido para garantizar que \(\mathbf{S}_0\) sea definida positiva (\(\nu_0 > p-1\), con \(p=2\)).

- \(\mathbf{S}_0 = \mathbf{\Lambda}_0\) asegura consistencia con las suposiciones previas sobre \(\mathbf{\Sigma}\).


```{r}
# previa
mu0 <- c(50,50)
L0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
nu0 <- 4
S0  <- matrix(data = c(278,139,139,278), nrow = 2, ncol = 2)
```


## Ajuste del Modelo Normal MUltivariado Semi-Conjugado

```{r}
# inicializar
theta <- yb
Sigma <- SS
# número de muestras
B <- 10000
ncat <- floor(B/10)
# almacenamiento
THETA <- SIGMA <- YS <- LP <- NULL
# cadena
iL0 <- solve(L0)
Lm0 <- solve(L0)%*%mu0
nun <- nu0 + n
SSn <- S0 + (n-1)*SS
set.seed(1)
for (b in 1:B) {
  # actualizar theta
  iSigma <- solve(Sigma)
  Ln     <- solve(iL0 + n*iSigma)
  theta  <- c(mvtnorm::rmvnorm(n = 1, mean = Ln%*%(Lm0 + n*(iSigma%*%yb)), sigma = Ln))
  # actualizar Sigma
  Sigma <- solve(rWishart::rWishart(n = 1, df = nun, Sigma = solve(SSn + n*((yb - theta)%*%t(yb - theta))))[,,1])
  # predictiva posterior
  YS <- rbind(YS, mvtnorm::rmvnorm(n = 1, mean = theta, sigma = Sigma)) 
  # log-verosimilitud
  LP[b] <- sum(apply(X = Y, MARGIN = 1, FUN = function(y) mvtnorm::dmvnorm(x = y, mean = theta, sigma = Sigma, log = T)))
  # almacenar
  THETA <- rbind(THETA, theta)
  SIGMA <- rbind(SIGMA, c(Sigma))
  # progreso
  if (b%%ncat == 0) 
       cat(100*round(b/B, 1), "% completado ... \n", sep = "")
}
colnames(THETA) <- c("theta1", "theta2")
colnames(SIGMA) <- c("sigma1^2", "sigma21", "sigma12", "sigma2^2")
```


## Convergencia

Cadena de la log-verosimilitud:

```{r, echo = FALSE, fig.width=8,fig.height=4, fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,.5), mgp = c(1.7,0.7,0))
plot(x = 1:B, y = LP, type = "p", pch = ".", xlab = "Iteración", ylab = "Log-verosimilitud", main = "Modelo Normal Multivariado")
```


## Inferencia

Distribución posterior de $\boldsymbol{\theta}$ y distribución predictiva posterior:

```{r,echo=FALSE,fig.width=8,fig.height=8,fig.align='center'}
# gráfico
par(mfrow = c(2,1), mar = c(2.75,2.75,1.5,.5), mgp = c(1.5,0.5,0))
# theta 2 vs theta 1
plot (x = THETA[,1], y = THETA[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(theta[1]), ylab = expression(theta[2]), main = "", cex.axis = 0.9)
abline(v = mean(THETA[,1]), col = "gray85", lty = 2)
abline(h = mean(THETA[,2]), col = "gray85", lty = 2)
lines(x = THETA[,1], y = THETA[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
# y* 2 vs y* 1
plot (x = YS[,1], y = YS[,2], pch = 16, col = adjustcolor("black", 0.1), xlab = expression(tilde(y)[1]), ylab = expression(tilde(y)[2]), main = "", cex.axis = 0.9)
abline(v = mean(YS[,1]), col = "gray85", lty = 2)
abline(h = mean(YS[,2]), col = "gray85", lty = 2)
lines(x = YS[,1], y = YS[,2], type = "p", pch = 16, col = adjustcolor("black", 0.1))
lines(x = mean(THETA[,1]), y = mean(THETA[,2]), type = "p", pch = 3, col = 2, cex = 1.2)
abline(a = 0, b = 1, col = 4)
```


Inferencia sobre $\theta_2 - \theta_1$:

¿Cuál es la probabilidad posterior de que la calificación promedio del segundo examen sea mayor que la del primero?

```{r}
round(mean(THETA[,2] - THETA[,1] > 0), 3)
```


```{r}
round(quantile(THETA[,2] - THETA[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\tilde{y}_2 - \tilde{y}_1$:

¿Cuál es la probabilidad posterior de que un niño seleccionado al azar obtenga una puntuación más alta en el segundo examen que en el primero?


```{r}
round(mean(YS[,2] - YS[,1] > 0), 3)
```


```{r}
round(quantile(YS[,2] - YS[,1], probs = c(0.025, 0.5, 0.975)), 3)
```

Inferencia sobre $\rho=\frac{\sigma_{1,2}}{\sigma_{1}\,\sigma_{2}}$:

¿Las pruebas son consistentes? ¿Cuál es la probabilidad posterior de que la correlación entre las calificaciones sea superior a 0.6?


```{r}
# muestras de rho
RHO <- SIGMA[,2]/sqrt(SIGMA[,1]*SIGMA[,4])
```


```{r,echo=FALSE,fig.width=4,fig.height=4,fig.align='center'}
# gráfico
par(mar = c(2.75,2.75,1.5,0.5), mgp = c(1.7,0.7,0))
hist(RHO, freq = F, col = "gray90", border = "gray90", xlim = c(0,1), xlab = expression(rho), ylab = "Densidad", main = "")
abline(v = quantile(RHO, prob = c(0.025,0.5,0.975)), col = c(4,2,4), lty = c(4,2,4))
```


```{r}
round(mean(RHO > 0.6), 3)
```


```{r}
round(quantile(RHO, prob = c(0.025,0.5,0.975)), 3)
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```
