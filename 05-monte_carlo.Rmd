---
title: "Métodos de Monte Carlo"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El objetivo consiste en **aproximar cualquier cantidad** asociada con la distribución posterior de $\boldsymbol{\theta}$.

Si podemos obtener muestras de $\boldsymbol{\theta}$ procedentes de la distribución posterior, **cualquier cantidad posterior** de interés se puede **aproximar con un grado de precisión arbitrario** usando **métodos de Monte Carlo** (Stanislaw Ulam, John von Neumann, Nicholas Metropolis).

Los **métodos de Monte Carlo** son algoritmos computacionales que se basan en un **muestreo aleatorio iterativo** para obtener **resultados numéricos**. El concepto subyacente es utilizar la aleatoriedad para **resolver problemas** tanto **estocásticos** (generación de muestras de una distribución de probabilidad) como **determinísticos** (optimización, integración numérica).

**Cualquier distribución de probabilidad** (y por ende cualquier característica de esa distribución) se puede **aproximar arbitrariamente bien** tomando tantas **muestras aleatorias de esa distribución** como sea necesario **dependiendo del nivel de precisión** que se requiera.

# Ejemplo: Distribución Gamma

Aproximación de una distribución Gamma con diferentes niveles de precisión.


```{r}
# parametros distribucion Gamma
a <- 68
b <- 45
# tamaños
m <- c(50, 250, 1250)
# simulacion con diferentes niveles de precision
set.seed(1234)
for (j in 1:length(m))
  assign(x = paste0("theta_sim_", j), value = rgamma(n = m[j], shape = a, rate = b))
```


```{r, fig.height=3, fig.width=9}
# grafico
par(mfrow = c(1,3), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
for (j in 1:length(m)) {
  hist(x = get(paste0("theta_sim_", j)), prob = T, xlim = c(0.75, 2.25), ylim = c(0, 2.7), xlab = expression(theta), ylab = "Densidad", main = paste0("B = ", m[j]), col = "gray90", border = "gray90")
  curve(expr = dgamma(x, shape = a, rate = b), col = "blue", add = T)
}
```

# Implementación

Sea $\theta$ un parámetro de interés y $\boldsymbol{y} = (y_1,\dots,y_n)$ un conjunto de observaciones. Suponga que **es posible obtener una muestra aleatoria** de $B$ valores de $\theta$ asociados con la **distribución posterior** $p(\theta\mid \boldsymbol{y})$, esto es,
$$
\theta^{(1)},\ldots,\theta^{(B)}\stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})\,.
$$

La **distribución empírica** de $\theta^{(1)},\ldots,\theta^{(B)}$ **aproxima** la **distribución posterior** $p(\theta\mid \boldsymbol{y})$ y tal aproximación puede ser **tan precisa como se quiera** incrementando el valor de $B$. Esta aproximación se conoce como la **aproximación de Monte Carlo** de $p(\theta\mid \boldsymbol{y})$. 

**(Ley débil de los grandes números).** Sea $X_1,\ldots,X_n$ una secuencia de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza finita $\sigma^2$. Entonces, el promedio muestral $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ **converge en probabilidad** a $\mu$ cuando $n\rightarrow\infty$, i.e., para todo $\epsilon > 0$ se tiene que
$$
\lim\limits_{n\to\infty}\textsf{Pr}(|\bar{X}_n - \mu| > \epsilon) = 0\,.
$$

La **ley débil de los grandes números** garantiza que, si $\theta^{(1)},\ldots,\theta^{(B)}\stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})$, entonces
$$
\frac{1}{B}\sum_{b=1}^{B} g\left(\theta^{(b)}\right)\longrightarrow \int_\Theta g(\theta)\,p(\theta\mid \boldsymbol{y})\,\text{d}\theta = \textsf{E}(g(\theta)\mid \boldsymbol{y})\,\,\text{cuando $B\rightarrow\infty$},
$$
donde $g(\theta)$ es una función arbitraria de $\theta$. 

Por lo tanto:

- **Media** posterior: 
$$
\bar{\theta}=\frac{1}{B}\sum_{b=1}^{B}\theta^{(b)}\longrightarrow\textsf{E}(\theta\mid \boldsymbol{y}) = \int_\Theta \theta\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$
- **Varianza** posterior:
$$
\frac{1}{B-1}\sum_{b=1}^{B}(\theta^{(b)} - \bar{\theta})^2\longrightarrow\textsf{Var}(\theta\mid \boldsymbol{y}) = \textsf{E}\left((\theta-\textsf{E}(\theta\mid\boldsymbol{y}))^2\mid\boldsymbol{y}\right) = \int_\Theta \left(\theta - \textsf{E}(\theta\mid\boldsymbol{y})\right)^2\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$
- **Probabilidad** posterior:
$$
\frac{1}{B}\sum_{b=1}^{B}I( \theta^{(b)}\in A )\longrightarrow\textsf{Pr}(\theta\in A\mid \boldsymbol{y}) = \textsf{E}\left(I ( \theta\in A ) \mid \boldsymbol{y} \right) = \int_\Theta I( \theta\in A )\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$

# Inferencia sobre una función arbitraria de $\theta$ 

Los métodos de Monte Carlo permiten hacer fácilmente inferencia posterior sobre cualquier **función arbitraria** de $\theta$, digamos $\gamma = g(\theta)$:

- Simular $\theta^{(1)},\ldots,\theta^{(B)} \stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})$.
- Calcular $\gamma^{(b)} = g\left(\theta^{(b)}\right)$ para $b=1,\ldots,B$. 

La secuencia $\gamma^{(1)},\ldots,\gamma^{(B)}$ constituye un conjunto de valores independientes de $p(\gamma\mid \boldsymbol{y})$ con los cuales se puede hacer inferencia posterior sobre cualquier aspecto de $\gamma$.

# Bondad de ajuste

Los métodos de Monte Carlo también permiten examinar detalladamente la **distribución predictiva posterior** $p(y^*\mid\boldsymbol{y})$, lo que hace posible chequear la **bondad de ajuste interna del modelo** por medio **estadísticos de prueba** calculados a partir de la distribución predictiva posterior:

- Simular $\theta^{(1)},\ldots,\theta^{(B)} \stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})$.
- Simular $(y^*_1)^{(b)},\ldots,(y^*_n)^{(b)} \stackrel{\text{iid}}{\sim} p\left(y\mid\theta^{(b)}\right)$ para $b=1,\dots,B$.
- Calcular $t^{(b)}=t((y^*_1)^{(b)},\ldots,(y^*_n)^{(b)})$ para $b=1,\dots,B$, donde $t(\cdot)$ es una estadístico de interés (denominado **estadístico de prueba**). 
- Comparar la distribución de $t^{(1)},\ldots,t^{(B)}$  con el **valor observado** $t_0=t(y_1,\ldots,y_n)$.

Si $t_0$ es un **valor típico de la distribución** de $t^{(1)},\ldots,t^{(B)}$, entonces se dice que el modelo **captura adecuadamente la característica de interés** que representa el estadístico de prueba. 

Se recomienda evaluar todos aquellos aspectos de la población que sea de interés caracterizar por medio del modelo.

# Ejemplo: Número de hijos y educación

COLOMBIA - Censo Nacional de Población y Vivienda - CNPV - 2018 disponible en https://microdatos.dane.gov.co/index.php/catalog/643/study-description

La tabla de Personas contiene la información de una muestra aleatoria simple de personas que residen en hogares particulares o personas que residen en lugares especiales de alojamiento con las características correspondientes al censo.

Modelar el número de hijos de personas identificadas como: mujer, jefe de hogar, 40 a 44 años, alfabeta, lugar de nacimiento en Colombia, lugar de residencia hace 5 años en Colombia, ningún grupo étnico, informa si tiene hijos o no.

Diccionario de datos (ddi-documentation-spanish-643.pdf) disponible en https://microdatos.dane.gov.co/index.php/catalog/643/datafile/F11

## Aproximaciones con diferentes niveles de precisión

```{r}
# modelo Gamma-Poisson
# datos mujeres con educacion superior
n <- 110
s <- 127
# hiperparametros
a <- 2
b <- 1
# tamaños
m <- c(5, 500, 50000)
# simulacion con diferentes niveles de precision
set.seed(1234)
for (j in 1:length(m))
  assign(x = paste0("theta_sim_", j), value = rgamma(n = m[j], shape = a+s, rate = b+n))
# media posterior aproximada
tab <- as.matrix(c(mean(theta_sim_1), mean(theta_sim_2), mean(theta_sim_3), (a+s)/(b+n) ))
colnames(tab) <- c("Estimación")
rownames(tab) <- c(paste0("B = ", m), "Exacto")
print(tab)
# intervalo de credibilidad al 95% aproximado
tab <- NULL
for (j in 1:length(m))
  tab <- rbind(tab, quantile(x = get(paste0("theta_sim_", j)), probs = c(.025,.975)))
tab <- rbind(tab, qgamma(p = c(.025,.975), shape = a+s, rate = b+n))
colnames(tab) <- c("Q2.5%", "Q97.5%")
rownames(tab) <- c(paste0("B = ", m), "Exacto")
print(tab)
```

## Inferencia sobre $\gamma = \theta_1 - \theta_2$

```{r}
# datos mujeres sin educacion superior
n1 <- 355
s1 <- 748
# datos muejeres con educacion superior
n2 <- 110   
s2 <- 127
# hiperparametros
a <- 2
b <- 1
# numero de muestras de MC 
B <- 50000
# simulacion
set.seed(1234)
theta1_mc <- rgamma(n = B, shape = a+s1, rate = b+n1)
theta2_mc <- rgamma(n = B, shape = a+s2, rate = b+n2)
gamma_mc  <- theta1_mc - theta2_mc
# probabilidad posterior de que gamma > 0
round(mean(gamma_mc > 0), 3)
# estimacion puntual de gamma
round(mean(gamma_mc), 3)
# intervalo de credibilidad al 95% para gamma
round(quantile(x = gamma_mc, probs = c(0.025, 0.975)), 3)
```

```{r, fig.height = 4, fig.width = 6}
# grafico
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
hist(x = gamma_mc, freq = F, col = "gray90", border = "gray90", xlim = mean(gamma_mc) + 4*c(-1,1)*sd(gamma_mc), xlab = expression(gamma), ylab = expression(paste("p(",gamma," | y)", sep="")), main = "")
lines(density(gamma_mc), col = 4, lwd = 2)
abline(v = quantile(x = gamma_mc, probs = c(0.025, 0.975)), lty = 2, lwd = 2, col = 2)
abline(v = mean(gamma_mc), lty = 2, lwd = 2, col = 3)
legend("topright", legend = c("Posterior", "IC 95%", "Media"), 
       col = c(4, 2, 3), lty = 1, lwd = 2, bty = "n")
```

## Distribución predictiva posterior

```{r}
# distribucion predictiva posterior
set.seed(1234)
y1_mc <- rpois(n = B, lambda = theta1_mc)
y2_mc <- rpois(n = B, lambda = theta2_mc)
# media predictiva posterior 
tab <- rbind(c((a+s1)/(b+n1), (a+s2)/(b+n2)), c(mean(y1_mc), mean(y2_mc)))
colnames(tab) <- c("Menos que pregrado","Pregrado o más")
rownames(tab) <- c("Exacta","Aproximada")
round(tab, 3)
# probabilidades de la distribucion predictiva muejres sin pregrado o menos
tab <- rbind(dnbinom(x = 0:7, size = a+s1, mu = (a+s1)/(b+n1)), table(y1_mc)[1:8]/B)
colnames(tab) <- 0:7
rownames(tab) <- c("Exacta", "Aproximada")
round(tab, 3)
# probabilidades de la distribucion predictiva mujeres con pregrado o mas
tab <- rbind(dnbinom(x = 0:7, size = a+s2, mu = (a+s2)/(b+n2)), table(y2_mc)[1:8]/B)
colnames(tab) <- 0:7
rownames(tab) <- c("Exacta", "Aproximada")
round(tab, 3)
# probabilidad posterior de que y*_1 > y*_2
round(mean(y1_mc > y2_mc), 3)
```

```{r, fig.height = 4, fig.width = 6}
# distribucion predictiva posterior de d = y*_1 - y*_2
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
plot(table(y1_mc - y2_mc)/B, type = "h", cex.axis = 0.9, xlab = "d", ylab = "p(d | y )", col = 4, lwd = 3)
```

## Chequeo del modelo

```{r}
# estadistico observado
t_obs <- s1/n1 - s2/n2
round(t_obs, 3)
# distribucion predictiva posterior
t_mc <- NULL
set.seed(1234)
for (i in 1:B) {
  # datos
  y1_rep  <- rpois(n = n1, lambda = theta1_mc[i])
  y2_rep  <- rpois(n = n2, lambda = theta2_mc[i])
  # estadistico
  t_mc[i] <- mean(y1_rep) - mean(y2_rep)
}
```

```{r, fig.height = 4, fig.width = 6}
# grafico
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
hist(x = t_mc, freq = F, col = "gray90", border = "gray90", xlab = "t", ylab = "p(t | y)", main = "")
lines(density(t_mc), col = 4, lwd = 2)
abline(v = t_obs, col = 1, lwd = 2, lty = 1)
abline(v = quantile(x = t_mc, probs = c(0.025, 0.975)), lty = 2, lwd = 2, col = 2)
legend("topright", legend = c("Posterior", "IC 95%", "t obs"), col = c(4, 2, 1), lty = 1, lwd = 2, bty = "n")
# ppp (valor p predictivo posterior)
mean(t_mc > t_obs)
```


# Referencias


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```
