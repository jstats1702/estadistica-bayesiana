---
title: "Métodos de Monte Carlo"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Ejemplo de motivación

Red de **relaciones de trabajo colaborativo** entre miembros de una firma de abogados (SG&R) disponible en el paquete `sand` de `R` y en este [enlace](https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm).

Un enlace indica que los miembros de la firma han trabajado juntos en al menos un caso.

¿La **probabilidad de interacción** es común para todos los pares de empleados de la firma?


# Métodos de Monte Carlo

Los **métodos de Monte Carlo** son algoritmos que se basan en **muestreo aleatorio** para obtener **resultados numéricos**. 

La idea subyacente es utilizar la aleatoriedad para **resolver problemas** tanto **estocásticos** (generación de muestras de una distribución de probabilidad) como **determinísticos** (optimización, integración numérica).

El término "Monte Carlo" fue utilizado por primera vez por **Ulam** y **Metropolis** en honor al famoso casino de Monte Carlo en Mónaco.

**Cualquier distribución de probabilidad** (y por ende cualquier característica de esa distribución) se puede **aproximar arbitrariamente bien** tomando tantas **muestras aleatorias de esa distribución** como sea necesario **dependiendo del nivel de precisión** requerido.


# Implementación

Sea $\theta$ un parámetro de interés y $\boldsymbol{y} = (y_1,\dots,y_n)$ un conjunto de observaciones. 

Suponga que **es posible obtener una muestra aleatoria** de $B$ valores provenientes de la **distribución posterior** de $\theta$:
$$
\theta^{(1)},\ldots,\theta^{(B)}\stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})\,.
$$

La **distribución empírica** de $\theta^{(1)},\ldots,\theta^{(B)}$ **aproxima** la **distribución posterior** de $\theta$. Esta aproximación puede ser **tan precisa como se quiera** incrementando el valor de $B$.

**(Ley débil de los grandes números).** Sea $X_1,\ldots,X_n$ una secuencia de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza finita $\sigma^2$. Entonces, el promedio muestral $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ **converge en probabilidad** a $\mu$ cuando $n\rightarrow\infty$, i.e., para todo $\epsilon > 0$ se tiene que
$$
\lim\limits_{n\to\infty}\textsf{Pr}(|\bar{X}_n - \mu| < \epsilon) = 1\,.
$$

La **ley débil de los grandes números** garantiza que:
$$
\frac{1}{B}\sum_{b=1}^{B} g(\theta^{(b)})\longrightarrow \textsf{E}(g(\theta)\mid \boldsymbol{y}) = \int_\Theta g(\theta)\,p(\theta\mid \boldsymbol{y})\,\text{d}\theta\,,
$$
cuando $B\rightarrow\infty$, con $g(\theta)$ una función arbitraria de $\theta$. 

- **Media** posterior: 
$$
\frac{1}{B}\sum_{b=1}^{B}\theta^{(b)}\longrightarrow\textsf{E}(\theta\mid \boldsymbol{y}) = \int_\Theta \theta\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$
- **Varianza** posterior:
$$
\frac{1}{B}\sum_{b=1}^{B}(\theta^{(b)} - \bar{\theta})^2\longrightarrow\textsf{Var}(\theta\mid \boldsymbol{y}) = \textsf{E}\left((\theta-\textsf{E}(\theta\mid\boldsymbol{y}))^2\mid\boldsymbol{y}\right) = \int_\Theta \left(\theta - \textsf{E}(\theta\mid\boldsymbol{y})\right)^2\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$
- **Probabilidad** posterior:
$$
\frac{1}{B}\sum_{b=1}^{B}I( \theta^{(b)}\in A )\longrightarrow\textsf{Pr}(\theta\in A\mid \boldsymbol{y}) = \textsf{E}\left(I ( \theta\in A ) \mid \boldsymbol{y} \right) = \int_\Theta I( \theta\in A )\,p(\theta\mid\boldsymbol{y})\,\text{d}\theta\,.
$$

**(Definición).** Sea $x_1,\ldots,x_n$ la realización de una muestra aleatoria de la variable aleatoria con función de distribución acumulada $F$. La función 
$$
F_n(x) = \frac{1}{n}\sum_{i=1}^n I(x_i \leq x)\,, \qquad -\infty < x < \infty\,,
$$
se denomina **función de distribución acumulada empírica** de $x_1,\ldots,x_n$.

$F_n$ es una función escalonada no decreciente con saltos de $\frac{1}{n}$ en cada uno de los $x_i$. Además, $F_n$ es continua por la derecha y está acotada entre 0 y 1.

**(Ley fuerte de los grandes números).** Sea $X_1,\ldots,X_n$ una secuencia de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza finita $\sigma^2$. Entonces, el promedio muestral $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ **converge casi seguramente** a $\mu$ cuando $n\rightarrow\infty$, i.e.,
$$
\textsf{Pr}\left(\lim\limits_{n\to\infty} \bar{X}_n = \mu \right) = 1\,.
$$
La **ley fuerte de los grandes números** garantiza que:
$$
\textsf{Pr}\left(\lim\limits_{n\to\infty} F_B(\theta) = F_{\theta\mid\boldsymbol{y}}(\theta) \right) = 1\,.
$$

- **Distribución empírica** posterior:
$$
F_B(\theta) \longrightarrow  F_{\theta\mid\boldsymbol{y}}(\theta)\,.
$$
- **Cuantil** $\alpha$ posterior: 
$$
Q_\alpha\longrightarrow (\theta\mid \boldsymbol{y})_{\alpha}\,.
$$


## Ejemplo

Aproximación de la distribución Gamma con parámetros $\alpha=3$ y $\beta=2$.


```{r}
# parámetros distribución Gamma
a <- 3
b <- 2
# tamaños de muestra
m <- c(10, 30, 1000)
# simulación
set.seed(1234)
for (j in 1:length(m))
  assign(x = paste0("theta_mc_", j), value = rgamma(n = m[j], shape = a, rate = b))
```


```{r, fig.height=6, fig.width=9, echo=F, fig.align='center'}
# histogramas
par(mfrow = c(2,3), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
for (j in 1:length(m)) {
  hist(x = get(paste0("theta_mc_", j)), prob = T, xlim = c(0, 6), ylim = c(0, 0.8), xlab = expression(theta), ylab = "Densidad", main = paste0("B = ", m[j]), col = "gray90", border = "gray90")
  curve(expr = dgamma(x, shape = a, rate = b), col = "blue", add = T, n = 1000)
}
# distribuciones acumuladas
for (j in 1:length(m)) {
  x <- get(paste0("theta_mc_", j))
  B <- length(x)
  plot(x = sort(x), y = (1:B)/B, type = 's', col = "gray80", lwd = 5, xlim = c(0, 6), ylim = c(0, 1), xlab = expression(theta), ylab = "Distr. Acumulada", main = "")
  curve(expr = pgamma(x, shape = a, rate = b), col = "blue", add = T, n = 1000)
}
```

A partir de la muestra $\theta^{(1)},\ldots,\theta^{(1000)}\stackrel{\text{iid}}{\sim} \textsf{Gamma}(3,2)$ se obtiene que:

```{r, echo=F}
tab <- rbind(c(a/b, mean(theta_mc_3)),
             c( a/b^2, var(theta_mc_3)),
             c(pgamma(q = 1, shape = a, rate = b), mean(theta_mc_3 < 1)),
             c(qgamma(p = 0.50, shape = a, rate = b), quantile(theta_mc_3, probs = 0.50)))
colnames(tab) <- c("Exacto", "Monte Carlo")
rownames(tab) <- c("Media", "Varianza", "Pr. < 1", "Q50%")
knitr::kable(x = t(tab), digits = 3, align = "c", caption = "Aproximación de algunas cantidades de la distribución Gamma ")
```


## Errores estándar de Monte Carlo

Sea $\theta^{(1)},\ldots,\theta^{(B)}$ una muestra aleatoria de $\theta\mid\boldsymbol{y}$, y además, 
$$
\bar{\theta} = \frac{1}{B}\sum_{b=1}^B \theta^{(b)}
\qquad\text{y}\qquad
s^2_\theta = \sqrt{\frac{1}{B-1}\sum_{b=1}^B (\theta^{(b)} - \bar{\theta})^2}
$$
la media muestral y la desviación estándar muestral de $\theta^{(1)},\ldots,\theta^{(B)}$, respectivamente. 

**(Definición).** El **error estándar de Monte Carlo** de $\bar{\theta}$ corresponde a la aproximación de la desviación estándar de $\bar{\theta}$, esto es, $s_\theta/\sqrt{B}$.

Similarmente, el **coeficiente de variación de Monte Carlo** se calcula como $(s_\theta/\sqrt{B})/|\bar\theta|$.

Por el **Teorema del Límite Central** se tiene que el **margen de error de Monte Carlo** al $95\%$ de confianza para $\textsf{E}(\theta\mid\boldsymbol{y})$ es $1.96\,s_\theta/\sqrt{B}$.

¿Qué tan "grande" debe ser **número de muestras** de Monte Carlo? Típicamente, se elige $B$ lo suficientemente grande para que el error estándar de Monte Carlo sea menor que un margen de error dado.


## Ejemplo (cont.)

A partir de la muestra $\theta^{(1)},\ldots,\theta^{(1000)}\stackrel{\text{iid}}{\sim} \textsf{Gamma}(3,2)$ se obtiene que $\bar\theta = 1.543$ y $s_\theta = 0.873$.

Por lo tanto, el error estándar y el margen de error al 95\% de Monte Carlo son $s_\theta/\sqrt{B} = 0.028$ (coeficiente de variación de $0.018$) y $1.96\,s_\theta/\sqrt{B} = 0.054$, respectivamente.

Para alcanzar un margen de error de $0.01$ se deben tomar 
$B = (1.96\, s_\theta / 0.01)^2\approx 29253$ muestras de la distribución. 

```{r, eval=F, echo=F}
# media
mean(theta_mc_3)
# desviación estándar
sd(theta_mc_3)
# error estándar
sd(theta_mc_3)/sqrt(length(theta_mc_3))
# coeficiente de variación
(sd(theta_mc_3)/sqrt(length(theta_mc_3)))/abs(mean(theta_mc_3))
# margen de error
1.96*sd(theta_mc_3)/sqrt(length(theta_mc_3))
# tamaño de muestra para alcanzar un margen de error de 0.01
1.96^2*sd(theta_mc_3)^2/0.01^2
```


# Inferencia sobre una función arbitraria de $\theta$ 

Los métodos de Monte Carlo permiten hacer fácilmente inferencia posterior sobre cualquier **función arbitraria** de $\theta$, digamos $\gamma = g(\theta)$:

- Simular $\theta^{(1)},\ldots,\theta^{(B)} \stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})$.
- Calcular $\gamma^{(b)} = g\left(\theta^{(b)}\right)$, para $b=1,\ldots,B$. 

La secuencia $\gamma^{(1)},\ldots,\gamma^{(B)}$ constituye una **muestra aleatoria** de $p(\gamma\mid \boldsymbol{y})$ y a partir de ella se puede hacer inferencia posterior sobre $\gamma$.


# Bondad de ajuste

Los métodos de Monte Carlo también permiten analizar la **distribución predictiva posterior** $p(y^*\mid\boldsymbol{y})$.

Esto hace posible chequear la **bondad de ajuste interna del modelo** por medio **estadísticos de prueba** calculados a partir de la distribución predictiva posterior:

- Simular $\theta^{(1)},\ldots,\theta^{(B)} \stackrel{\text{iid}}{\sim} p(\theta\mid \boldsymbol{y})$.
- Simular $(y^*_1)^{(b)},\ldots,(y^*_n)^{(b)} \stackrel{\text{iid}}{\sim} p\left(y\mid\theta^{(b)}\right)$, para $b=1,\dots,B$.
- Calcular $t^{(b)}=t((y^*_1)^{(b)},\ldots,(y^*_n)^{(b)})$, para $b=1,\dots,B$, donde $t(\cdot)$ es una estadístico de interés (denominado **estadístico de prueba**). 
- Comparar la distribución de $t^{(1)},\ldots,t^{(B)}$  con el **valor observado** $t_0=t(y_1,\ldots,y_n)$.

Si $t_0$ es un **valor típico de la distribución** de $t^{(1)},\ldots,t^{(B)}$, entonces se dice que el modelo **captura adecuadamente la característica de interés** que representa el estadístico de prueba. 

Se recomienda evaluar todos aquellos aspectos de la población que sea de interés caracterizar por medio del modelo.



# Ejemplo: Trabajo colaborativo

Red de **relaciones de trabajo colaborativo** entre miembros de una firma de abogados (SG&R) disponible en el paquete `sand` de `R` y en este [enlace](https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm).

Un enlace indica que los miembros de la firma han trabajado juntos en al menos un caso.

¿La **probabilidad de interacción** es común para todos los pares de empleados de la firma?


## Datos

**Red binaria no dirigida** con $n=34$ vértices (orden) y $s=115$ aristas (tamaño).

```{r}
# paquetes
suppressMessages(suppressWarnings(library(igraph)))
suppressMessages(suppressWarnings(library(sand)))
```


```{r}
# grafo
g <- graph_from_data_frame(d = elist.lazega, directed = "F")
# orden
n <- vcount(g)
# tamaño
s <- ecount(g)
# matriz de adyacencia
Y <- as.matrix(get.adjacency(graph = g, names = F))
```


```{r, fig.height = 8, fig.width = 16, fig.align='center', echo=F}
# visualización
igraph_options(vertex.label = 1:vcount(g), vertex.size = 9, vertex.frame.color = 1, vertex.color = 0, vertex.label.color = "black", edge.color = "blue4")
par(mfrow = c(1,2))
# grafo
set.seed(1234)
plot(g, layout = layout_with_fr)
# matriz de adyacencia
corrplot::corrplot(corr = Y, col.lim = c(0,1), method = "color", tl.col = "black", tl.cex = 1.4, addgrid.col = "gray90", cl.pos = "n")
```


## Modelo de grafos aleatorios

El **modelo de grafos aleatorios** se refiere a un modelo en el que las entradas de la matriz de adyacencia $\mathbf{Y}=[y_{i,j}]$ son tales que
$$
y_{i,j}\mid\theta\stackrel{\text{iid}}{\sim} \textsf{Bernoulli}(\theta)\,,\qquad i < j\,,
$$
y por lo tanto, la **distribución muestral** es
$$
p(\mathbf{Y}\mid\theta) = \prod_{i<j}\theta^{y_{i,j}}(1-\theta)^{1-y_{i,j}} = \theta^{s} (1-\theta)^{m -s}\,,
$$
donde $s=\sum_{i<j} y_{i,j} = 115$ y $m=n(n-1)/2 = 561$.


## Inferencia

Asumiendo una **distribución previa** de la forma $\theta\sim\textsf{Beta}(a,b)$, se tiene que la **distribución posterior** es
$$
\theta\mid \mathbf{Y} \sim \textsf{Beta}\left(\theta\mid a + s, b+m-s\right)\,.
$$

Si $\theta\sim\textsf{Beta}(1,1)$, entonces $\theta\mid \mathbf{Y} \sim \textsf{Beta}\left(\theta\mid 116, 447\right)$.


```{r}
# distribucion previa
a <- 1
b <- 1
# distribucion posterior
(ap <- a + s)
(bp <- b + n*(n-1)/2 - s)
# media posterior
round(ap/(ap + bp), 3)
# intervalo de credibilidad al 95%
round(qbeta(p = c(.025,.975), shape1 = ap, shape2 = bp), 3)
```


```{r, fig.height=6, fig.width=12, fig.align='center', echo=F}
par(mfrow = c(1,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
# gráfico 1
curve(dbeta(x, shape1 = ap, shape2 = bp), from = 0, to = 1, n = 1000, lwd = 2, col = 4, xlab = expression(theta), ylab = expression(paste("p","( ",theta," | ",Y," )",sep="")))
curve(dbeta(x, shape1 = a, shape2 = b), n = 1000, lwd = 2, col = 1, add = T)
# gráfico 2
curve(dbeta(x, shape1 = ap, shape2 = bp), from = 0.1, to = 0.3, n = 1000, lwd = 2, col = 4, xlab = expression(theta), ylab = expression(paste("p","( ",theta," | ",Y," )",sep="")))
curve(dbeta(x, shape1 = a, shape2 = b), n = 1000, lwd = 2, col = 1, add = T)
abline(v = ap/(ap + bp), lty = 1, col = 3)
abline(v = qbeta(p = c(.025,.975), shape1 = ap, shape2 = bp), lty = 4, col = 2)
legend("topright", legend = c("Previa", "Posterior", "IC 95%", "Media"), col = c(1, 4, 2, 3), lty = 1, lwd = 2, bty = "n")
```


## Bondad de ajuste

Estadísticos de prueba:

- Densidad.
- Transitividad.
- Asortatividad.
- Grado.

Estos estadísticos se pueden revisar en este [enlace](https://kateto.net/networks-r-igraph).

```{r, echo=F}
# estadísticos de prueba
t0 <- as.matrix(c(edge_density(g), transitivity(g), assortativity_degree(g), mean(degree(g)), sd(degree(g))))
colnames(t0) <- c("Estadístico")
rownames(t0) <- c("Densidad", "Transitividad", "Asortatividad", "Promedio Grado", "Desv. Estándar Grado")
knitr::kable(x = t(t0), digits = 3, align = "c", caption = "Estadísticos de prueba para chequear el modelo de grafos aleatorios por medio de la distribución predictiva posterior.")
```


```{r}
# distribución posterior
set.seed(1234)
theta_mc <- rbeta(n = 1000, shape1 = ap, shape2 = bp)
# distribución predictiva posterior
Yrep <- matrix(data = 0, nrow = n, ncol = n)
t_mc <- NULL
set.seed(1234)
for (i in 1:length(theta_mc)) {
  # datos
  Yrep[lower.tri(Yrep)] <- rbinom(n = n*(n-1)/2, size = 1, prob = theta_mc[i])
  grep <- graph_from_adjacency_matrix(adjmatrix = Yrep, mode = "undirected", diag = F)
  # estadísticos
  t_mc <- rbind(t_mc, c(edge_density(grep), transitivity(grep), assortativity_degree(grep), mean(degree(grep)), sd(degree(grep))))
}
```


```{r, fig.height = 9, fig.width = 9, fig.align='center', echo = F, fig.cap="Distribución predictiva posterior de estadísticos de prueba. Valor observado en azul. Intervalo de credibilidad al 95% en rojo."}
# gráfico
par(mfrow = c(2,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
for (k in 1:(nrow(t0)-2)) {
  hist(x = t_mc[,k], freq = F, col = "gray90", border = "gray90", xlim = range(t0[k], t_mc[,k]), xlab = "t", ylab = "p( t | Y )", main = rownames(t0)[k])
  abline(v = t0[k], lwd = 2, col = 4)
  abline(v = quantile(x = t_mc[,k], probs = c(0.025, 0.975)), lty = 4, col = 2)
}
# media y desviación estándar grado
plot(x = t_mc[,4], y = t_mc[,5], pch = 16, col = "gray90", xlim = range(t0[4], t_mc[,4]), ylim = range(t0[5], t_mc[,5]), xlab = "Media", ylab = "Desv. Estándar", main = "Grado")
lines(x = t0[4], y = t0[5], type = "p", pch = 15, col = 4, cex = 1.25)
abline(v = quantile(x = t_mc[,4], probs = c(0.025, 0.975)), lty = 4, col = 2)
abline(h = quantile(x = t_mc[,5], probs = c(0.025, 0.975)), lty = 4, col = 2)
```


# Referencias {-}


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```