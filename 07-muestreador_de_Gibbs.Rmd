---
title: "Muestreador de Gibbs"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Motivación: modelo Normal con una distribución previa semi-conjugada

En la mayoría de los casos **no es posible obtener directamente muestras la distribución posterior**.

Considere especificar Su estado de información previo acerca de $\theta$ de manera **independiente** de $\sigma^2$ de forma que:
$$
p(\theta,\sigma^2) = p(\theta)\,p(\sigma^2)\,.
$$
Esta formulación es mas **flexible** porque no hay una restricción de dependencia a priori entre $\theta$ y $\sigma^2$.

Asumiendo que las observaciones dadas en $\boldsymbol{y} = (y_1,\ldots,y_n)$ son intercambiables, bajo el **modelo Normal con una distribución previa semi-conjugada** se tiene que:

- **Verosimilitud**:
$$
y_i\mid\theta,\sigma^2 \stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2)\,,\qquad i=1,\ldots,n.
$$
- **Previa**:
$$
\begin{align*}
\theta   &\sim \textsf{N}(\mu_0, \tau^2_0) \\
\sigma^2 &\sim \textsf{GI}\left(\tfrac{\nu_0}{2},\tfrac{\nu_0\,\sigma^2_0}{2}\right)
\end{align*}
$$

- **Hiperparámetros**: $\mu_0$, $\tau^2_0$, $\nu_0$, y $\sigma^2_0$.

En el caso de la previa conjugada (donde $\tau_0^2$ es proporcional a $\sigma^2$) se tiene que la distribución posterior de $\sigma^2$ es Gamma Inversa. En este caso, **la distribución posterior de $\sigma^2$ no sigue una distribución estándar conocida** de la cual se pueda obtener muestras directamente.

## Distribuciones condicionales completas

El **muestreador de Gibbs** (*Gibbs sampler*) es un algoritmo iterativo que permite **obtener muestras dependientes** de la **distribución posterior** por medio de las **distribuciones condicionales completas**.

Bajo esta especificación del modelo Normal, se demuestra que:

- La distribución condicional completa de $\theta$ es $\theta\mid\sigma^2,\boldsymbol{y}\sim\textsf{N}(\mu_n,\tau^2_n)$, donde
$$
\mu_n = \frac{\frac{1}{\tau^2_0}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}} \qquad\text{y}\qquad\tau^2_n=\frac{1}{\frac{1}{\tau^2_0} + \frac{n}{\sigma^2}}\,.
$$
- La distribución condicional completa de $\sigma^2$ es $\sigma^2\mid\theta,\boldsymbol{y}\sim\textsf{GI}\left(\tfrac{\nu_n}{2},\tfrac{\nu_n\,\sigma^2_n}{2}\right)$, donde
$$
\nu_n = \nu_0+n\qquad\text{y}\qquad\sigma^2_n = \frac{1}{\nu_n}\left( \nu_0\sigma^2_0 + ns^2(\theta) \right)\,.
$$
con $s^2(\theta) = \tfrac{1}{n}\sum_{i=1}^n (y_i-\theta)^2 = (n-1)s^2 + n(\bar{y}-\theta)^2$, el cual corresponde al estimardor insesgado de $\sigma^2$ si $\theta$ fuera conocido. 


# Muetreador de Gibbs

Dado un **estado actual** de los parámetros del modelo $\boldsymbol{\theta}^{(b)} = \left(\theta^{(b)}, (\sigma^2)^{(b)}\right)$, se genera un nuevo estado $\boldsymbol{\theta}^{(b+1)}$ como sigue:

1. Muestrear $\theta^{(b+1)}\sim p(\theta\mid(\sigma^2)^{(b)}, \boldsymbol{y})$.
2. Muestrear $(\sigma^2)^{(b+1)}\sim p(\sigma^2\mid\theta^{(b+1)}, \boldsymbol{y})$.
3. Establecer $\boldsymbol{\theta}^{(b+1)} = \left(\theta^{(b+1)}, (\sigma^2)^{(b+1)}\right)$. 
4. Repetir los pasos 1. a 3. hasta convergencia.

Este algoritmo se denomina **muestreador de Gibbs** y genera una **secuencia dependiente** de parámetros $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ de la distribución posterior $p(\theta,\sigma^2\mid \boldsymbol{y})$. 

Como punto de partida, solo es necesario proporcionar un valor inicial para $\sigma^2$. Usualmente este valor se muestra de la distribución previa correspondiente, esto es, $(\sigma^2)^{(0)}\sim\textsf{IG}\left(\tfrac{\nu_0}{2},\tfrac{\nu_0\,\sigma^2_0}{2}\right)$.

## Ejemplo: Muestreador de Gibbs para el modelo Normal

En 1981, los biólogos W. L. Grogan y W. W. Wirth descubrieron en las selvas de 
Brasil dos nuevas variedades de un diminuto insecto picador llamado mosquito 
(*midge*). Llamaron a un tipo de mosquito Apf y al otro mosquito Af. 
Los biólogos descubrieron que el mosquito Apf es portador de una enfermedad 
debilitante que causa inflamación del cerebro cuando un humano está mordido 
por un mosquito infectado. Aunque la enfermedad rara vez es fatal, la 
discapacidad causada por la hinchazón puede ser permanente. La otra forma de 
mosquito, el Af, es bastante inofensiva y un valioso polinizador. 
En un esfuerzo por distinguir las dos variedades, los biólogos tomaron medidas 
en los mosquitos que capturaron. Este es un conjunto de datos valioso para 
probar métodos de clasificación.

***Grogan Jr, W. L., & Wirth, W. W. (1981). A new American genus of predaceous midges related to Palpomyia and Bezzia (Diptera: Ceratopogonidae). Proceedings of the Biological Society of Washington., 94(4), 1279-1305.***


```{r, eval = TRUE, echo=FALSE, out.width="75%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("modelo_normal_midge.png")
```

Considere los datos de la **longitud del ala en milímetros** ($y$) de $n=9$ miembros de 
la **especie Af de mosquitos**. A partir de estas nueve mediciones, se quiere 
hacer inferencia sobre la **media poblacional** $\theta$. 

Otros estudios sugieren que 
la longitud de las alas suele ser de alrededor de 1.9 mm. Claramente, se tiene 
que las longitudes deben ser positivas, lo que implica que $\theta > 0$.

Los datos de Grogan y Wirth se encuentran disponibles en la librería `Flury` de R, pero esta librería no se encuentra disponible para versiones reciente de R.

### Muestreador de Gibbs {-}

```{r}
# datos 
y <- c(1.64,1.70,1.72,1.74,1.82,1.82,1.82,1.90,2.08)
# tamaño de la muestra
n <- length(y)
# estadisticos
mean_y <- mean(y)
mean_y
var_y  <- var(y)
var_y
sum_y  <- sum(y)
sum_y
```


```{r}
# hiperparametros
mu0 <- 1.9 
t20 <- 0.5^2
s20 <- 0.01 
nu0 <- 1
# numero de muestras 
B <- 100000
# matriz para almacenar las muestras
PHI <- matrix(data = NA, nrow = B, ncol = 2)
# mostrar anuncios cada 10% de las iteraciones
ncat <- floor(B/10) 
# ALGORITMO (muestreador de Gibbs)
# 1. inicializar la cadena
#    valor inicial: simular de la previa
#    solo es necesario alguno de los valores
set.seed(1234)
theta <- rnorm(n = 1, mean = mu0, sd = sqrt(t20))
isig2 <- rgamma(n = 1, shape = nu0/2, rate = nu0*s20/2)
PHI[1,] <- c(theta, isig2)
# 2. simular iterativamente de las distribuciones condicionales completas
set.seed(1234)
for(b in 2:B) {
  # 2.1 actualizar el valor de theta
  t2n   <- 1/(1/t20 + n*isig2)      
  mun   <- (mu0/t20 + sum_y*isig2)*t2n
  theta <- rnorm(n = 1, mean = mun, sd = sqrt(t2n))
  # 2.2 actualizar el valor de sigma^2
  nun   <- nu0 + n
  s2n   <- (nu0*s20 + (n-1)*var_y + n*(mean_y - theta)^2)/nun
  isig2 <- rgamma(n = 1, shape = nun/2, rate = nun*s2n/2)
  # 2.3 almacenar
  PHI[b,] <- c(theta, isig2)
  # 2.4 progreso
  if (b%%ncat == 0) cat(100*round(b/B, 1), "% completado ... \n", sep = "")
}
# muestras
head(PHI)
```

```{r, fig.width = 8, fig.height = 4, fig.align = 'center'}
# grafico del algoritmo
# este grafico no se acostumbra a hacer en la practica
par(mfrow = c(1,2), mar = c(2.75,3,0.5,0.5), mgp = c(1.70,0.7,0))
m1<-15
plot(PHI[1:m1,], type = "l", xlim = range(PHI[1:100,1]), ylim = range(PHI[1:100,2]), lty = 1, col="gray", xlab = expression(theta), ylab = expression(tilde(sigma)^2))
text(PHI[1:m1,1], PHI[1:m1,2], c(1:m1))
m1<-100
plot(PHI[1:m1,], type = "l", xlim = range(PHI[1:100,1]), ylim = range(PHI[1:100,2]), lty = 1, col="gray", xlab = expression(theta), ylab = expression(tilde(sigma)^2))
text(PHI[1:m1,1], PHI[1:m1,2], c(1:m1))
```


### Distribuciones posterior y marginales {-}

```{r, fig.width=10, fig.height=10}
par(mfrow = c(2,2), mar = c(2.75,3,0.5,0.5), mgp = c(1.70,0.7,0))
# distribucion conjunta
plot(PHI[,1], PHI[,2], pch = ".", xlim = range(PHI[,1]), ylim = c(0,225), xlab = expression(theta), ylab = expression(tilde(sigma)^2))
# distribucion conjunta
plot(PHI[,1], 1/PHI[,2], pch = ".", xlim = range(PHI[,1]), ylim = c(0,0.15), xlab = expression(theta), ylab = expression(sigma^2))
# theta
plot(density(PHI[,1]), xlab = expression(theta), main = "", xlim = c(1.55,2.05), ylab = expression(paste(italic("p("), theta," | y)",sep="")))
# precision
plot(density(PHI[,2]), xlab = expression(tilde(sigma)^2), main = "", ylab = expression(paste(italic("p("),tilde(sigma)^2," | y)",sep=""))) 
```

### Inferencia {-}

```{r}
# intervalos de credibilidad
# media
round(quantile(PHI[,1], c(.025, 0.5, 0.975)), 3)
# precision 
round(quantile(PHI[,2], c(.025, 0.5, 0.975)), 3)
# desviancion estandar
round(quantile(1/sqrt(PHI[,2]), c(.025, 0.5, 0.975)), 3)
# coeficiente de variacion
round(quantile((1/sqrt(PHI[,2]))/PHI[,1], c(0.025, 0.5, 0.975)), 3)
# probabilidad posterior
round(mean(PHI[,1] > 1.8), 3)
```


## Algoritmo general

Dado un **estado actual** de los parámetros del modelo $\boldsymbol{\theta}^{(b)} = \left(\theta_1^{(b)},\ldots,\theta_k^{(b)}\right)$, se genera un nuevo estado $\boldsymbol{\theta}^{(b+1)}$ como sigue:

1. Muestrear $\theta_1^{(b+1)}\sim p\left(\theta_1\mid\theta_2^{(b)},\theta_3^{(b)},\ldots,\theta_k^{(b)}\right)$.
2. Muestrear $\theta_2^{(b+1)}\sim p\left(\theta_2\mid\theta_1^{(b+1)},\theta_3^{(b)},\ldots,\theta_k^{(b)}\right)$.
3. Muestrear $\theta_3^{(b+1)}\sim p\left(\theta_3\mid\theta_1^{(b+1)},\theta_2^{(b+1)},\ldots,\theta_k^{(b)}\right)$.
4. ...
5. Muestrear $\theta_k^{(b+1)}\sim p\left(\theta_k\mid\theta_1^{(b+1)},\theta_2^{(b+1)},\ldots,\theta_{k-1}^{(b+1)}\right)$.
6. Establecer $\boldsymbol{\theta}^{(b+1)} = \left(\theta_1^{(b+1)},\ldots,\theta_k^{(b+1)}\right)$.
7. Repetir los pasos 1. a 6. hasta convergencia.

Este algoritmo genera una **secuencia dependiente** de parámetros $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ de la distribución posterior $p(\theta_1,\ldots,\theta_k\mid \boldsymbol{y})$.

Observe que $\boldsymbol{\theta}^{(b+1)}$ depende únicamente de $\boldsymbol{\theta}^{(b)}$ lo cual sugiere que $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ define una **cadena de Markov** (*Markov chain*).

***Smith, A. F., & Roberts, G. O. (1993). Bayesian computation via the Gibbs sampler and related Markov chain Monte Carlo methods. Journal of the Royal Statistical Society: Series B (Methodological), 55(1), 3-23.***

## Cadenas de Markov

Un **proceso estocástico** es una colección de variables aleatorias $\{\theta_t\in S:t\in T\}$ para algún **espacio de estados** $S$ bien sea **discreto**, e.g., $\{1,\ldots,k\}$, o continuo, e.g., $(-\infty,\infty)$, y un **conjunto de índices** $T$, bien sea **discreto**, e.g., $\{0,1,\ldots\}$), o continuo, e.g., $[0,\infty)$.

Un proceso estocástico $\{\theta_t\in S:t\in T\}$, con $T=\{0,1,\ldots\}$, se denomina **cadena de Markov** si, para todo $A\subset S$, se tiene que
$$
\textsf{Pr}(\theta_{t+1}\in A\mid \theta_0,\ldots,\theta_t) = \textsf{Pr}(\theta_{t+1}\in A\mid\theta_t)\,.
$$
Una **cadena de Markov** es una clase particular de proceso estocástico cuyos **estados pasados y futuros son independientes dado el estado actual**, i.e., para caracterizar probabilísticamente hacia dónde se moverá la cadena a continuación, no necesita saber dónde ha estado, **solo debe considerar dónde está ahora**.

### Cadenas de Markov bien comportadas {-}

- Una cadena se denomina **irreducible** si es posible llegar a cualquier estado desde cualquier otro.      
- Un estado $i$ se denomina **periódico** (*periodic*) con periodo $d$ si $i$ es visitado después de un número de pasos múltiplo de un entero $d > 1$.
- Una cadena se denomina **aperiódica** (*aperiodic*) si para todos los estados se tiene que $d=1$.
- Una cadena se denomina **positiva recurrente** (*positive recurrent*) si para todos los estados $i$:
    a. Si el proceso comienza en un estado $i$ regresará a $i$ con probabilidad 1. 
    b. El tiempo de espera medio para el regreso al estado $i$ es finito.
- Una cadena se denomina **ergódica** (*ergodic*) si es **irreducible**, **aperiódica** y **positiva recurrente**. 

### Teorema Ergódico {-}

Las **cadenas de Markov ergódicas** poseen una **distribución estacionaria** única $\pi(\theta)$. Esta distribución caracteriza el **comportamiento que adopta la cadena después de evolucionar mucho tiempo**, independientemente de su estado inicial.

**(Teorema Ergódico.)** Si una cadena de Markov $\{\theta_t\in S:t\in T\}$ es **ergódica** y si $f$ es una función de valor real tal que $\textsf{E}_\pi|f(\theta)|<\infty$, entonces, con probabilidad 1, cuando $B\rightarrow\infty$ se tiene que
$$
\frac{1}{B}\sum_{b=1}^B f(\theta_b) \longrightarrow \textsf{E}_{\pi(\theta)}(f(\theta))
$$
donde el valor esperado de $f(\theta)$ se toma respecto a la **distribución estacionaria** $\pi(\theta)$.

El teorema **no dice nada** sobre tres aspectos prácticos fundamentales: 

- ¿Cuál debe ser el mejor punto de partida?
- ¿Cuánto tiempo se debe esperar para lograr la estacionariedad?
- ¿Cuánto tiempo se debe monitorear después de eso?

Siempre que la distribución estacionaria sea la distribución posterior, se puede **aprender con precisión arbitraria** sobre **cualquier aspecto de la distribución posterior**. simplemente **esperando lo suficiente a que se logre la estacionariedad** y monitoreando a partir de entonces un período lo suficientemente largo.

## Resumen

La idea es simular muestras de la distribución posterior $p(\boldsymbol{\theta}\mid\boldsymbol{y})$, por medio de una **cadena de Markov** con las siguientes características:

- Debe tener el **mismo espacio de estados** que $\boldsymbol{\theta}$.
- Debe ser **fácil de simular** (muestrear).
- Debe tener a $p(\boldsymbol{\theta}\mid\boldsymbol{y})$ como **distribución estacionaria**.

Se demuestra que una **cadena de Markov** construida a partir del **muestreador de Gibbs** es **ergódica** y tiene la **distribución posterior** como **distribución estacionaria**, sin importar el punto de partida de la cadena (algunos puntos de partida pueden ser mejores que otros). 

La secuencia $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ se puede usar tal y como se tratara de una muestra aleatoria de valores de $\boldsymbol{\theta}$ provenientes de la distribución posterior $p(\boldsymbol{\theta}\mid y)$. Esto es,
$$
\frac{1}{B} \sum_{b=1}^{B} g(\boldsymbol{\theta}) \longrightarrow \textsf{E}[g(\boldsymbol{\theta}) \mid \boldsymbol{y}]=\int_{\Theta} g(\boldsymbol{\theta})\, p(\boldsymbol{\theta} \mid \boldsymbol{y})\,\textsf{d}\boldsymbol{\theta}\qquad\text{cuando}\qquad B\rightarrow \infty\,.
$$

El punto de partida $\boldsymbol{\theta}^{(0)}$ es arbitrario y usualmente se muestrea de la distribución previa.

El **muestreador de Gibbs** hace parte de un conjunto de técnicas de aproximación denominadas **cadenas de Markov de Monte Carlo** (*Markov Chain Monte Carlo*, MCMC).

Los métodos de MCMC constituyen **técnicas de aproximación numérica**, **no son modelos**, **no generan más información** además de la contenida en $\boldsymbol{y}$.

## Diagnósticos de convergencia (estacionariedad)

**Preguntas:**

- ¿Cual debe ser el punto de partida?
- ¿La cadena alcanzo el equilibrio?
- Una vez alcanzado el equilibrio, ¿cuánto más se debe monitorear para lograr buenas aproximaciones?

Para que las **aproximaciones a las cantidades posteriores sean precisas**, se necesita que la distribución empírica de $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ esté lo suficientemente cerca de $p(\boldsymbol{\theta}\mid y)$.

El muestreador de Gibbs produce **muestras que eventualmente van a converger** a la distribución objetivo, pero en algunos casos la **convergencia puede ser lenta** debido a la **autocorrelación** de los parámetros.

Es usual pensar en la secuencia $\boldsymbol{\theta}^{(1)},\ldots,\boldsymbol{\theta}^{(B)}$ como la **trayectoria de una partícula** $\boldsymbol{\theta}$ moviéndose a lo largo del espacio de parámetros $\Theta$.

No es posible estar absolutamente seguro que la cadena ha convergido. **Solo se puede saber si no lo ha hecho.**

### Serie {-}

Permiten chequear **estacionariedad** (muestras de una parte de la cadena tienen una distribución similar a muestras de otra parte de la cadena). 

También permiten ver si la cadena está **mezclando** bien o no (en simulación de Monte Carlo la partícula se mueve libremente a cualquier región del espacio de parámetros, lo cual significa cero autocorrelación).

Para resolver problemas de estacionariedad se recomienda **correr la cadena con más iteraciones**.

### Autocorrelación {-}

La función de autocorrelación está dada por
$$
\operatorname{acf}_{t}(\theta)=\frac{\frac{1}{B-t} \sum_{b=1}^{B-t}\left(\theta^{(b)}-\bar{\theta}\right)\left(\theta^{(b+t)}-\bar{\theta}\right)}{\frac{1}{B-1} \sum_{b=1}^{B}\left(\theta^{(b)}-\bar{\theta}\right)^{2}}\qquad\text{donde}\qquad\bar{\theta} = \frac{1}{B}\sum_{b=1}^B \theta^{(b)}\,.
$$
Entre **mayor sea la autocorrelación**, se necesitan **más muestras** para obtener la precisión deseada.

Para resolver problemas de correlación se recomienda **adelgazar la cadena sistemáticamente** (muestreo sistemático de la cadena).

### Tamaño efectivo de muestra {-}

Las cadenas suelen estar autocorrelacionadas positivamente, por lo tanto contienen menos información que una secuencia de muestras iid de la distribución objetivo.

Se quiere establecer la cantidad de muestras iid que contendrían la misma información de la cadena:
$$
n_{\text{eff}}(\theta) = \frac{B}{1+2\sum_{t=1}^\infty \text{acf}_t(\theta)}
$$
donde $\rho_t$ es la autocorrelación en el lag $t$. Comúnmente, la suma se hace hasta que $\text{acf}_{t-1}+\text{acf}_t < 0$.

### Error estándar de Monte Carlo {-}

El error estándar de Monte Carlo hace referencia a la variabilidad (desviación estándar) del promedio muestral respecto al tamaño efectivo de muestra:
$$
\textsf{EMC}(\hat\theta) = \frac{\textsf{DE}(\hat\theta)}{\sqrt{n_{\text{eff}}(\theta)}}
$$
donde $\textsf{DE}(\hat\theta) = \sqrt{\frac{1}{B-1}\sum_{b=1}^B (\theta^{(b)}-\bar\theta)^2}$ es la desviación estándar muestral de $\hat\theta$ y $n_{\text{eff}}$ es el tamaño efectivo de la muestra asociada con la cadena de $\theta$.


## Ejemplo: Diagnósticos de convergencia


### Series {-}


```{r, fig.width=8, fig.height=8, fig.align='center'}
# cadenas
par(mfrow = c(2,1), mar = c(3,3,1,1), mgp = c(1.75,.75,0))
plot(PHI[-c(1:10),1],   type = "p", pch = ".", xlab = "Iteración", ylab = expression(theta))
plot(1/PHI[-c(1:10),2], type = "p", pch = ".", xlab = "Iteración", ylab = expression(sigma^2))
```



```{r, fig.width=8, fig.height=4, fig.align='center'}
# log-verosimilitud
LL <- NULL
for (i in 1:B)
  LL[i] <- sum(dnorm(x = y, mean = PHI[i,1], sd = sqrt(1/PHI[i,2]), log = T))
# cadena
plot(LL[-c(1:10)], type = "p", pch = ".", xlab = "Iteración", ylab = "Log-verosimilitud")
```



### Autocorrelación {-}


```{r, fig.width=8, fig.height=4, fig.align='center'}
# autocorrelacion
par(mfrow = c(1,2), mar = c(3,3,3,1), mgp = c(1.75,0.75,0))
acf(PHI[,1], main = expression(theta))
acf(1/PHI[,2], main = expression(sigma^2))
```


### Tamaño efectivo de muestra {-}


```{r}
# tamaño efectivo de la muestra
coda::effectiveSize(PHI)
```


### Error estándar de Monte Carlo  {-}


```{r}
# error estandar de monte carlo
apply(X = PHI, MARGIN = 2, FUN = sd)/sqrt(coda::effectiveSize(PHI))
```

Ver https://cran.r-project.org/web/packages/bayesplot/vignettes/visual-mcmc-diagnostics.html y https://arviz-devs.github.io/arviz/api/diagnostics.html para más diagnósticos.

# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```