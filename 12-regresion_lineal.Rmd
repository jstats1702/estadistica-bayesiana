---
title: "Introducción a la regresión lineal"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    mathjax: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    toc: true
    toc_float: true
    theme: cerulean
    highlight: kate
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

El modelo de regresión se enfoca en describir cómo el proceso generativo asociado a una variable aleatoria \( y \) varía en función de otra variable o conjunto de variables \(\boldsymbol{x} = (x_1, \ldots, x_p)\). Específicamente, un modelo de regresión define una estructura para \( p(y \mid \boldsymbol{x}) \), es decir, la distribución condicional de \( y \) dado \(\boldsymbol{x}\).

La estimación de \( p(y \mid \boldsymbol{x}) \) se lleva a cabo utilizando el vector de observaciones \(\boldsymbol{y} = (y_1, \ldots, y_n)\), recopilado bajo diversas condiciones representadas por \(\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n\), donde cada \(\boldsymbol{x}_i = (x_{i,1}, \ldots, x_{i,p})\) para \(i = 1, \ldots, n\).

Una posible solución a este problema es asumir que \( p(y \mid \boldsymbol{x}) \) es una **función suave** de \(\boldsymbol{x}\), permitiendo que los valores de \(\boldsymbol{x}\) influyan en el proceso generativo de \(y\). Un **modelo de regresión lineal** constituye un caso particular de modelo para \( p(y \mid \boldsymbol{x}) \), en el que se especifica que \(\textsf{E}(y \mid \boldsymbol{x})\) sigue una relación lineal en términos de un conjunto de parámetros \(\boldsymbol{\beta} = (\beta_1, \ldots, \beta_p)\). Esta relación se expresa como:
$$
\textsf{E}(y \mid \boldsymbol{x}) = \int_{\mathcal{Y}} y\, p(y \mid \boldsymbol{x})\,\text{d}y = \sum_{k=1}^p \beta_k x_k = \boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}.
$$

El **modelo de regresión lineal Normal** asume que la variabilidad alrededor de \(\textsf{E}(y \mid \boldsymbol{x})\) sigue una distribución Normal, definida como:
\[
y_i \mid \boldsymbol{x}_i, \boldsymbol{\beta}, \sigma^2 \stackrel{\text{iid}}{\sim} \textsf{N}(\boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}_i, \sigma^2)
\qquad\Longleftrightarrow\qquad
y_i = \boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}_i + \epsilon_i, \quad \epsilon_i \mid \sigma^2 \stackrel{\text{iid}}{\sim} \textsf{N}(0, \sigma^2),
\]
para \(i = 1, \ldots, n\). Equivalentemente, en forma matricial:
\[
\boldsymbol{y} \mid \mathbf{X}, \boldsymbol{\beta}, \sigma^2 \sim \textsf{N}_n(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I})
\qquad\Longleftrightarrow\qquad
\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \mid \sigma^2 \sim \textsf{N}_n(\boldsymbol{0}, \sigma^2 \mathbf{I}),
\]
donde \(\mathbf{X} = [\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n]^{\textsf{T}}\) es la matriz de diseño, \(\boldsymbol{\epsilon} = (\epsilon_1, \ldots, \epsilon_n)\) es el vector de términos de error, y \(\mathbf{I}\) es la matriz identidad de tamaño \(n \times n\).

Esta formulación define completamente la **distribución muestral** de los datos, detallando la probabilidad conjunta de las observaciones \(\boldsymbol{y}\) dado el conjunto de predictores \(\mathbf{X}\), los parámetros \(\boldsymbol{\beta}\) y \(\sigma^2\):
\[
p(\boldsymbol{y} \mid \mathbf{X}, \boldsymbol{\beta}, \sigma^2) = \prod_{i=1}^n \textsf{N}(y_i \mid \boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}_i, \sigma^2),
\]
que puede expresarse como:
\[
p(\boldsymbol{y} \mid \mathbf{X}, \boldsymbol{\beta}, \sigma^2) \propto (\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}_i)^2 \right\}.
\]
Equivalente en notación matricial:
\[
p(\boldsymbol{y} \mid \mathbf{X}, \boldsymbol{\beta}, \sigma^2) = (\sigma^2)^{-n/2} \exp\left\{ -\frac{1}{2\sigma^2} (\boldsymbol{y} - \mathbf{X}\boldsymbol{\beta})^{\textsf{T}} (\boldsymbol{y} - \mathbf{X}\boldsymbol{\beta}) \right\}.
\]


# Mínimos cuadrados ordinarios

¿Qué valor de \(\boldsymbol{\beta}\) es más *adecuado* para los datos observados \(\boldsymbol{y}\) y \(\mathbf{X}\)?

Dado el conjunto de datos \(\boldsymbol{y}\) y \(\mathbf{X}\), el valor óptimo de \(\boldsymbol{\beta}\) maximiza el exponente de la distribución muestral. Esto equivale a minimizar la **suma de cuadrados residual** (SSR):
\[
\textsf{SSR}(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \boldsymbol{\beta}^{\textsf{T}} \boldsymbol{x}_i)^2 = \boldsymbol{\beta}^{\textsf{T}} \mathbf{X}^{\textsf{T}} \mathbf{X} \boldsymbol{\beta} - 2 \boldsymbol{\beta}^{\textsf{T}} \mathbf{X}^{\textsf{T}} \boldsymbol{y} + \boldsymbol{y}^{\textsf{T}} \boldsymbol{y}.
\]

La minimización de esta expresión con respecto a \(\boldsymbol{\beta}\) conduce al valor:
\[
\hat{\boldsymbol{\beta}}_{\text{ols}} = (\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\textsf{T}} \boldsymbol{y},
\]
conocido como el estimador de **mínimos cuadrados ordinarios (OLS)** para \(\boldsymbol{\beta}\).

Adicionalmente, un estimador insesgado de \(\sigma^2\) está dado por:
\[
\hat{\sigma}^2_{\text{ols}} = \frac{1}{n - p} \, \textsf{SSR}(\hat{\boldsymbol{\beta}}_{\text{ols}}),
\]
donde \(n\) es el número de observaciones y \(p\) es el número de parámetros estimados (incluyendo el intercepto, si corresponde).


# Inferencia Bayesiana 

**Motivación:**

- Construcción de **intervalos de credibilidad** para los parámetros \(\beta_j\), con \(j = 1, \ldots, p\).
- Cálculo de **probabilidades posteriores** de la forma \(\textsf{Pr}(\beta_j > 0 \mid \mathbf{X}, \boldsymbol{y})\), para \(j = 1, \ldots, p\).
- El estimador de mínimos cuadrados ordinarios (OLS) tiende a sobreajustar los datos cuando \(p\) es grande, mientras que el enfoque Bayesiano, al incorporar información previa, es más **conservador** similar a una **regresión regularizada**.
- Proporciona un marco natural para la **selección de modelos**, al permitir comparar y evaluar modelos mediante criterios basados en la probabilidad posterior.

## Modelo semi-conjugado

### Modelo {-}

**Distribución muestral:**
$$
\boldsymbol{y} \mid \mathbf{X},\boldsymbol{\beta},\sigma^2 \sim \textsf{N}_n(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I})
$$

**Distribución previa:**
$$
\begin{align*}
\boldsymbol{\beta} &\sim \textsf{N}_p(\boldsymbol{\beta}_0, \mathbf{\Sigma}_0) \\
\sigma^2 & \sim \textsf{GI}\left( \tfrac{\nu_0}{2}, \tfrac{\nu_0\sigma^2_0}{2} \right)
\end{align*}
$$

**Parámetros:** $(\boldsymbol{\beta},\sigma^2)$.

**Hiperparámetros:** $(\boldsymbol{\beta}_0, \mathbf{\Sigma}_0, \nu_0, \sigma^2_0)$.

### Muestreador de Gibbs {-}

**Distribuciones condicionales completas:**

$$
\begin{align*}
\boldsymbol{\beta}\mid\text{resto} &\sim \textsf{N}_p\left( (\mathbf{\Sigma}_0^{-1} + \tfrac{1}{\sigma^2}\mathbf{X}^{\textsf{T}}\mathbf{X} )^{-1} (\mathbf{\Sigma}_0^{-1}\boldsymbol{\beta}_0 + \tfrac{1}{\sigma^2}\mathbf{X}^{\textsf{T}}\boldsymbol{y}  )  , (\mathbf{\Sigma}_0^{-1} + \tfrac{1}{\sigma^2}\mathbf{X}^{\textsf{T}}\mathbf{X} )^{-1} \right) \\
\sigma^2 \mid \text{resto} &\sim \textsf{GI}\left( \frac{\nu_0 + n}{2}, \frac{\nu_0\sigma^2_0 + \textsf{SSR}(\boldsymbol{\beta})}{2} \right)
\end{align*}
$$

Observe que: 

- Si $\mathbf{\Sigma}_0 \approx \mathbf{0}\,$, entonces $\textsf{E}(\boldsymbol{\beta}\mid\mathbf{X}, \boldsymbol{y}) \approx \hat{\boldsymbol{\beta}}_{\text{ols}}$.
- Si $\sigma^2 \approx \infty$, entonces $\textsf{E}(\boldsymbol{\beta}\mid\mathbf{X}, \boldsymbol{y}) \approx \boldsymbol{\beta}_0$.

### Previa unitaria {-}

Determinar valores adecuados para los hiperparámetros que representen información previa sustantiva puede ser un desafío, y esta dificultad aumenta considerablemente a medida que crece \(p\), ya que el número de hiperparámetros se incrementa de forma drástica. 

En ausencia de información previa relevante, es recomendable emplear una distribución previa que sea lo **menos informativa posible**. 

Una opción común en estos casos es la **distribución previa de información unitaria** (*unit information prior*; Kass y Wasserman, 1995), que encapsula únicamente la información equivalente a la proporcionada por **una sola observación**.

Dado que \(\textsf{E}(\hat{\boldsymbol{\beta}}_{\text{ols}}) = \boldsymbol{\beta}\) y \((\,\textsf{Var}(\hat{\boldsymbol{\beta}}_{\text{ols}})\,)^{-1} = \frac{1}{\sigma^2} (\mathbf{X}^{\textsf{T}}\mathbf{X})\), se propone utilizar los siguientes valores:
\[
\boldsymbol{\beta}_0 = \hat{\boldsymbol{\beta}}_{\text{ols}}, \qquad  
\mathbf{\Sigma}_0 = n\,\hat{\sigma}^2_{\text{ols}} (\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1}, \qquad  
\nu_0 = 1, \qquad  
\sigma^2_0 = \hat{\sigma}^2_{\text{ols}}.
\]

Estrictamente hablando, **esta no es una distribución previa** en el sentido convencional, ya que se basa en los datos observados. Sin embargo, dado que solo utiliza una pequeña fracción de la información disponible, equivalente a \(\frac{1}{n}\), puede interpretarse como la distribución previa de un analista con **información previa débil**. Este enfoque permite incorporar suavemente la información de los datos mientras se mantiene una perspectiva no informativa.

### Previa g {-}

Otro principio para definir una distribución previa consiste en garantizar que la estimación de los parámetros sea **invariante a cambios en la escala** de los regresores. Esta propiedad se cumple si se elige \(\boldsymbol{\beta}_0 = \boldsymbol{0}\) y \(\mathbf{\Sigma}_0 = k\,(\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1}\), donde \(k > 0\) actúa como un parámetro de escala que controla la dispersión de la distribución previa.

La **distribución previa \(g\)** (*g-prior*; Zellner, 1986) propone que \(k = g\,\sigma^2\) con \(g > 0\). Este enfoque incorpora un factor \(g\) que controla la relación entre la varianza previa y la varianza del error. En particular, cuando \(g = n\), la distribución previa refleja la información equivalente a la proporcionada por **una sola observación**. 

Bajo este esquema, la distribución posterior de \(\boldsymbol{\beta}\) se expresa como:
\[
\boldsymbol{\beta} \mid \text{resto} \sim \textsf{N}_n\left( \frac{g}{g+1} (\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\textsf{T}} \boldsymbol{y}, \frac{g}{g+1} \sigma^2 (\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1} \right).
\]

Este enfoque garantiza una integración coherente entre la información previa y los datos observados, ajustando la influencia de la información previa mediante el parámetro \(g\).

Bajo la **previa \(g\)**, para **simular de manera eficiente la distribución posterior** de \((\boldsymbol{\beta}, \sigma^2)\), se puede aprovechar la factorización:
\[
p(\boldsymbol{\beta}, \sigma^2 \mid \mathbf{X}, \boldsymbol{y}) = p(\boldsymbol{\beta} \mid \sigma^2, \mathbf{X}, \boldsymbol{y}) \, p(\sigma^2 \mid \mathbf{X}, \boldsymbol{y}).
\]

La distribución marginal \(p(\sigma^2 \mid \mathbf{X}, \boldsymbol{y})\) se deriva observando que:
\[
p(\sigma^2 \mid \mathbf{X}, \boldsymbol{y}) \propto p(\boldsymbol{y} \mid \sigma^2, \mathbf{X}) \, p(\sigma^2),
\]
donde:
\[
p(\boldsymbol{y} \mid \sigma^2, \mathbf{X}) = \int_{\mathbb{R}^p} p(\boldsymbol{y} \mid \boldsymbol{\beta}, \sigma^2, \mathbf{X}) \, p(\boldsymbol{\beta} \mid \sigma^2, \mathbf{X}) \, \text{d}\boldsymbol{\beta}.
\]
Al realizar esta integración, se obtiene que:
\[
p(\sigma^2 \mid \mathbf{X}, \boldsymbol{y}) \propto \textsf{GI}\left(\sigma^2 \mid \frac{\nu_0 + n}{2}, \frac{\nu_0 \sigma^2_0 + \textsf{SSR}_g}{2}\right),
\]
donde:
\[
\textsf{SSR}_g = \boldsymbol{y}^{\textsf{T}} \left(\mathbf{I}_n - \frac{g}{g+1} \mathbf{X} (\mathbf{X}^{\textsf{T}} \mathbf{X})^{-1} \mathbf{X}^{\textsf{T}}\right) \boldsymbol{y}.
\]

Para simular de la distribución posterior de \((\boldsymbol{\beta}, \sigma^2)\), se sigue un sencillo **algoritmo de Monte Carlo** (¡no es MCMC!):

1. Simular \(\sigma^2 \sim p(\sigma^2 \mid \mathbf{X}, \boldsymbol{y})\) utilizando la distribución Inversa-Gamma obtenida.
2. Condicional en el valor simulado de \(\sigma^2\), simular \(\boldsymbol{\beta} \sim p(\boldsymbol{\beta} \mid \sigma^2, \mathbf{X}, \boldsymbol{y})\), que sigue una distribución Normal multivariada. 

Este método aprovecha la conjugación inherente a la previa \(g\) para facilitar simulaciones directas de la distribución posterior.


### Previa Difusa Independiente {-}

La **previa difusa independiente** es una opción estándar cuando no se dispone de información previa sustantiva o se desea minimizar su influencia en los resultados. 

Este enfoque asigna distribuciones independientes a los parámetros \(\boldsymbol{\beta}\) y \(\sigma^2\) con varianzas amplias, reflejando incertidumbre significativa. Sus parámetros son:
\[
\boldsymbol{\beta}_0 = \boldsymbol{0}, \quad \mathbf{\Sigma}_0 = 100\,\mathbf{I}_p, \quad \nu_0 = 1, \quad \sigma^2_0 = 100.
\]


# Ejemplo: Absorción de oxígeno

Estudio sobre el efecto de dos regímenes de ejercicio en la absorción de oxígeno.

El estudio evaluó el **efecto de dos programas de entrenamiento** sobre la **absorción máxima de oxígeno** en un grupo de 12 hombres. Los participantes se asignaron aleatoriamente en dos grupos: 

1. Programa de **carrera en terreno plano** durante 12 semanas (6 participantes).
2. Programa de **aeróbicos** durante 12 semanas (6 participantes).

Se midió el **consumo máximo de oxígeno** (en litros por minuto) de cada sujeto al correr en una cinta inclinada, tanto **antes** como **después** del programa. Este consumo se considera el mejor indicador de la capacidad de trabajo físico. 

El objetivo es analizar cómo el **cambio en la absorción máxima de oxígeno** \(y\) depende del tipo de programa de entrenamiento.

**Referencia**: 
Kuehl, R. O. (2000). *Designs of experiments: Statistical principles of research design and analysis* (2nd ed.). Pacific Grove, CA: Duxbury Press.

### Variables del Modelo {-}

- \(n\): tamaño de la muestra (12 participantes).
- \(y\): cambio en la absorción máxima de oxígeno.
- \(\text{trat}\): indicador del programa de entrenamiento (\(1\) si aeróbicos, \(0\) si carrera).
- \(\text{edad}\): edad del participante (en años).

El modelo lineal utilizado es:
\[
\textsf{E}(y \mid \mathbf{X}) = \beta_1x_1 + \beta_2x_2 + \beta_3x_3 + \beta_4x_4,
\]
donde:

- \(x_1 = 1\) (intercepto).
- \(x_2 = 1\) si el participante está en el programa de aeróbicos, \(0\) si está en el programa de carrera.
- \(x_3 = \text{edad}\).
- \(x_4 = x_2 \cdot x_3\) (interacción entre el programa y la edad).

#### Interpretación del Modelo {-}

El modelo permite estimar el cambio promedio en la absorción máxima de oxígeno según el tipo de programa y la edad del participante:

- **Programa de carrera**: 
  \[
  \textsf{E}(y \mid \mathbf{X}) = \beta_1 + \beta_3 \cdot \text{edad}.
  \]

- **Programa de aeróbicos**:
  \[
  \textsf{E}(y \mid \mathbf{X}) = (\beta_1 + \beta_2) + (\beta_3 + \beta_4) \cdot \text{edad}.
  \]


```{r}
# datos
trat <- c(0,0,0,0,0,0,1,1,1,1,1,1)
edad <- c(23,22,22,25,27,20,31,23,27,28,22,24)
y    <- c(-0.87,-10.74,-3.27,-1.97,7.50,-7.25,17.05,4.96,10.40,11.05,0.26,2.51)
# respuesta y matriz de diseño
y <- as.matrix(y)
X <- cbind(1, trat, edad, trat*edad)
colnames(X) <- paste0("x", 1:4)
# dimensiones
(n <- dim(X)[1])
(p <- dim(X)[2])
```

```{r, echo = F, fig.height=4, fig.width=8, fig.align='center'}
# dispersograma
par(mfrow=c(1,1), mar=c(3,3,1,1), mgp=c(1.75,.75,0))
plot(y ~ edad, pch=16, xlab = "Edad", ylab = "Cambio en la absorción", col = c("black","gray")[trat+1])
legend("topleft", legend=c("Aeróbicos","Correr"), pch=c(16,16), col=c("gray","black"))
```

```{r}
# OLS
beta_ols <- solve(t(X)%*%X)%*%t(X)%*%y
round(beta_ols, 3)
sig2_ols <- sum((y - X%*%beta_ols)^2)/(n-p)
round(sig2_ols, 3)
# usando lm
# fit.ols <- lm(y ~ -1 + X)
# summary(fit.ols)$coef
# summary(fit.ols)$sigm
```


```{r}
# hiperparámetros (previa g)
nu0 <- 1
s20 <- sig2_ols
g   <- n
```


```{r}
# ajuste del modelo
# algunas cantidades
Hg    <- (g/(g+1))*(X%*%solve(t(X)%*%X)%*%t(X))
SSRg  <- as.numeric(t(y)%*%(diag(1,n) - Hg)%*%y)
Vbeta <- (g/(g+1))*solve(t(X)%*%X)
Ebeta <- Vbeta%*%t(X)%*%y
# Monte Carlo
S <- 10000
sig2_mc <- matrix(data = NA, nrow = S, ncol = 1) 
beta_mc <- matrix(data = NA, nrow = S, ncol = p) 
set.seed(123)
for(s in 1:S) {
  sig2_mc[s] <- 1/rgamma(1, (nu0 + n)/2, (nu0*s20 + SSRg)/2)
  beta_mc[s,] <- c(mvtnorm::rmvnorm(1, Ebeta, sig2_mc[s]*Vbeta))
}
colnames(beta_mc) <- paste0("beta", 1:p)
```


```{r}
# inferencia sobre sigma^2
round(apply(X = sig2_mc, MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)), 3)
```


```{r}
# inferencia sobre beta
round(apply(X = beta_mc, MARGIN = 2, FUN = quantile, probs = c(0.025,0.5,0.975)), 3)
round(colMeans(beta_mc > 0), 3)
```

Las distribuciones posteriores de \(\beta_2\) y \(\beta_4\) muestran evidencia limitada sobre una diferencia entre los dos grupos, dado que los intervalos de credibilidad al 95% incluyen el valor cero. 

Sin embargo, analizar estos parámetros de manera aislada no es suficiente para obtener conclusiones sólidas, ya que es necesario considerar si existen diferencias en función de la edad.

Efecto del tratamiento aeróbico $\delta$:
$$
\begin{align*}
\delta(\text{edad}) &= \textsf{E}(y \mid \text{edad}, \text{aerobico}) - \textsf{E}(y \mid \text{edad}, \text{aerobico}) \\
       &= ( (\beta_1 + \beta_2) + (\beta_3 + \beta_4)*\text{edad} ) - ( \beta_1 + \beta_3*\text{edad} ) \\
       &= \beta_2 + \beta_4*\text{edad}
\end{align*}
$$


```{r}
# muestras de MC de delta por edad
r_edad <- min(edad):max(edad)
n_edad <- length(r_edad)
TE <- matrix(data = NA, nrow = S, ncol = n_edad)
for (j in 1:n_edad)
  TE[,j] <- beta_mc[,2] + beta_mc[,4]*r_edad[j]
```


```{r, echo = F, fig.height=4,fig.width=8, fig.align='center'}
that <- colMeans(TE)
ic1  <- apply(X = TE, MARGIN = 2, FUN = function(x) quantile(x, c(0.050,0.950)))
ic2  <- apply(X = TE, MARGIN = 2, FUN = function(x) quantile(x, c(0.025,0.975)))
colo <- c("blue","black")[as.numeric((ic2[1,] < 0) & (0 < ic2[2,]))+1]
# visualización
par(mfrow=c(1,1), mar=c(3,3,1.5,1), mgp=c(1.75,0.75,0))
plot(NA, NA, xlab = "Edad", ylab = "Efecto tratamiento", main = "", 
     xlim = range(edad), ylim = range(TE), cex.axis = 0.75, xaxt = "n")
axis(side = 1, at = r_edad, labels = r_edad, cex.axis = 0.75)
abline(h = 0, col = "gray", lwd = 2)
abline(v = r_edad, col = "gray95", lwd = 1, lty = 2) 
for (j in 1:n_edad) {
      segments(x0 = r_edad[j], y0 = ic1[1,j], x1 = r_edad[j], y1 = ic1[2,j], lwd = 3, col = colo[j])
      segments(x0 = r_edad[j], y0 = ic2[1,j], x1 = r_edad[j], y1 = ic2[2,j], lwd = 1, col = colo[j])
      lines(x = r_edad[j], y = that[j], type = "p", pch = 16, cex = 0.8, col = colo[j])
}
```

Hay evidencia razonablemente fuerte de una diferencia en las edades jóvenes, y menos evidencia en las más viejas.

---

# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```
