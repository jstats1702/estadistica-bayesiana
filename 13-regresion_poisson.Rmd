---
title: "Regresión Poisson"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\yv}{\boldsymbol{y}}

# Modelo

A continuación se presenta la formulación básica de un modelo de Regresión Poisson como un **modelo lineal generalizado** (GLM, *generalized linear model*):

**Distribución muestral**:
$$
y_i\mid\theta_i\stackrel{\text {iid}}{\sim}\textsf{Poisson}(\theta_i)\,,\qquad i = 1,\ldots,n
$$
donde
$$
\log(\theta_i) = \boldsymbol{\beta}^{\textsf{T}}\boldsymbol{x}_i = \sum_{j=1}^k\beta_j\, x_{i,j}
$$
con $\boldsymbol{\beta}=(\beta_1,\ldots,\beta_k)$ y $\boldsymbol{x} = (x_{i,1},\ldots,x_{i,k})$. 

Observe que la **función de enlace** (*link function*) en este modelo lineal generalizado es la función $\log$.

**Distribuciones previas conjugadas** para modelos lineales generalizados **no existen** (a excepción del modelo de regresión Normal).

**Distribución previa**:
$$
\boldsymbol{\beta}\sim\textsf{N}(\boldsymbol{\beta}_0,\Sigma_0)
$$

Los **parámetros** del modelo son $(\beta_1,\ldots,\beta_k)$.

Los **hiperparámetros** del modelo son $(\boldsymbol{\beta}_0,\Sigma_0)$.

# Distibución conjunta posterior

$$
\begin{align*}
p(\boldsymbol{\beta}\mid\boldsymbol{y}) &\propto p(\boldsymbol{y}\mid\boldsymbol{\beta})\,p(\boldsymbol{\beta}) \\
&= \prod_{i=1}^n \textsf{Poisson}(y_i\mid\theta_i) \times\textsf{N}(\boldsymbol{\beta}\mid\boldsymbol{\beta}_0,\Sigma_0) \\
&= \prod_{i=1}^n \frac{e^{-\theta_i}\,\theta_i^{y_i}}{y_i!} \times (2\pi)^{-k/2} \, |\Sigma_0|^{-1/2} \, \exp\left\{ -\tfrac{1}{2}(\boldsymbol{\beta}-\boldsymbol{\beta}_0)^{\textsf{T}}\Sigma_0^{-1}(\boldsymbol{\beta}-\boldsymbol{\beta}_0) \right\} \\
&\propto \prod_{i=1}^n e^{-\theta_i}\,\theta_i^{y_i} \times \exp\left\{ -\tfrac{1}{2} \left[ \boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta}_0 \right]   \right\}
\end{align*}
$$
con $\log(\theta_i) = \boldsymbol{\beta}^{\textsf{T}}\boldsymbol{x}_i$. 

Observe que $p(\boldsymbol{\beta}\mid\boldsymbol{y})$ no corresponde a ninguna familia paramétrica de distribuciones conocida.

Por lo tanto, en **escala logarítmica**, se tiene que
$$
\log p(\boldsymbol{\beta}\mid\boldsymbol{y}) = \sum_{i=1}^n(y_i\log(\theta_i) - \theta_i) -\tfrac{1}{2} \left[ \boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta} - 2\boldsymbol{\beta}^{\textsf{T}}\Sigma_0^{-1}\boldsymbol{\beta}_0 \right] + \text{C}
$$
donde $\text{C}$ es una constante (términos que no dependen de $\boldsymbol{\beta}$).


Además, el gradiente correspondiente está dado por
$$
\frac{\partial}{\partial \boldsymbol{\beta}}\log p(\boldsymbol{\beta} \mid \yv)
     = \sum_{i=1}^n \left( y_i - \exp{\left(\boldsymbol{\beta}^{\textsf{T}}\boldsymbol{x}_i\right)} \right)\boldsymbol{x}_i\,.
$$


# Algorithmo de Metropolis-Hastings

El algoritmo Metropolis-Hastings es un método genérico de aproximación de **cualquier distribución posterior**.

El problema radica cuando $p(\boldsymbol{\theta}\mid \boldsymbol{y})$ no tiene una distribución estándar conocida de la cual sea posible simular fácilmente.

## Metropolis

**Algoritmo:**

Dado un estado actual $\theta^{(b)}$ del parámetro $\theta$, se genera el siguiente estado $\theta^{(b+1)}$ como sigue: 

1. Simular $\theta^*\sim J(\theta\mid\theta^{(b)})$ donde $J$ es una distribución **simétrica**: $J(\theta_1\mid\theta_2) = J(\theta_2\mid\theta_1)$.
Esta distribución se denomina **distribución de propuestas** (*proposal distribution*). Típicamente,
$$
J(\theta\mid\theta^{(b)}) = \textsf{N}(\theta\mid\theta^{(b)},\delta^2)\qquad\text{o}\qquad J(\theta\mid\theta^{(b)}) = \textsf{U}(\theta\mid\theta^{(b)} - \delta, \theta^{(b)} + \delta)
$$
donde $\delta$, conocido como **parámetro de ajuste**, se escoge de forma que el algoritmo funcione eficientemente.
2. Calcular la tasa de aceptación:
$$
r = \frac{p(\theta^*\mid y)}{p(\theta^{(b)}\mid y)}\,.
$$
Para brindar estabilidad numérica en el procesamiento, esta tasa usualmente se calcula primero en escala logarítmica:
$$
r = \exp\left( \log p(\theta^*\mid y) - \log p(\theta^{(b)}\mid y) \right)\,.
$$
3. Establecer:
$$
\theta^{(b+1)} =
\left\{
  \begin{array}{ll}
    \theta^*\,     , & \hbox{con probabilidad $\min(1,r)$;} \\
    \theta^{(b)}\, , & \hbox{con probabilidad $1-\min(1,r)$.}
  \end{array}
\right.
$$

## Observaciones

- Este algoritmo produce una secuencia de valores $\theta^{(1)},\ldots,\theta^{(B)}$, donde $\theta^{(b+1)}$ depende de los valores previos únicamente a través de $\theta^{(b)}$, y por lo tanto la secuencia es una **Cadena de Markov**.
- La **correlación de la cadena** depende del valor de $\delta$ en la distribución de propuestas. 
- Es una práctica común **elegir un valor** de $\delta$ tal que la **tasa de aceptación** esté entre 30\% y 50\%. Existen **algoritmos adaptativos** para elegir automáticamente el valor de $\delta$.

## Metropolis-Hastings

El algoritmo Metrópolis-Hastings **generaliza los algoritmos de Gibbs** y **Metropolis** al permitir una distribución de propuesta arbitraria:

**Algoritmo:**

Dado un estado actual $\theta^{(b)}$ del parámetro $\theta$, se genera el siguiente estado $\theta^{(b+1)}$ como sigue: 

1. Simular $\theta^*\sim J(\theta\mid\theta^{(b)})$ donde $J$ es una **distribución de propuestas arbitraria**.

2. Calcular la tasa de aceptación:
$$
r = \frac{p(\theta^*\mid y)}{p(\theta^{(b)}\mid y)}\,\frac{J(\theta^{(b)}\mid\theta^*)}{J(\theta^*\mid \theta^{(b)})}\,.
$$
3. Establecer:
$$
\theta^{(b+1)} =
\left\{
  \begin{array}{ll}
    \theta^*\,     , & \hbox{con probabilidad $\min(1,r)$;} \\
    \theta^{(b)}\, , & \hbox{con probabilidad $1-\min(1,r)$.}
  \end{array}
\right.
$$

## Observaciones

- Cada paso del **algoritmo de Gibbs** se puede ver como la generación de una propuesta de una distribución condicional completa y luego aceptarla con probabilidad 1.
- El algoritmo de Gibbs y el algoritmo de Metropolis son **casos particulares** del algoritmo de Metropolis-Hastings.


# Ejemplo: Modelo Normal

Algoritmo de Metropolis en un modelo Normal con varianza conocida.

**Distribución muestral:**
$$
y_i\mid\theta \stackrel{\text {iid}}{\sim}\textsf{Normal}(\theta, 1)\,,\qquad i = 1,\ldots,n
$$

**Distribución previa:**
$$
\theta \sim \textsf{Normal}(\mu, \tau^2)
$$


```{r}
# simular datos
n  <- 5
s2 <- 1 
set.seed(1)
y <- round(rnorm(n = n, mean = 10, sd = 1), 2)
# previa
t2 <- 10 
mu <- 5
# Metropolis
theta <- 0      # valor inicial
delta <- 1.75   # parámetro de ajuste
S     <- 10000  # numero de muestras
THETA <- NULL   # almacenamiento
# cadena
set.seed(1)
for (s in 1:S) {
  # 1. propuesta
  theta.star <- rnorm(1,theta,sqrt(delta))
  # 2. tasa de aceptación
  log.r <- (sum(dnorm(y, theta.star, sqrt(s2), log = T)) + dnorm(theta.star, mu, sqrt(t2), log = T)) - (sum(dnorm(y, theta, sqrt(s2), log = T)) + dnorm(theta, mu, sqrt(t2), log = T)) 
  # 3. actualizar
  if (runif(1) < exp(log.r)) 
    theta <- theta.star 
  # 4. almacenar
  THETA <- c(THETA, theta)
}
```


```{r, fig.height=5, fig.width=10, echo = F, fig.align='center'}
# grafico
par(mfrow = c(1,2), mar = c(3,3,1,1), mgp = c(1.75,0.75,0))
# cadena
plot(x = 1:S, y = THETA, type = "l", xlab = "Iteración", ylab = expression(theta))
# histograma metropolis
# se omiten las primeras 100 observaciones (periodo de calentamiento)
hist(x = THETA[-(1:100)], prob = T, main = "", xlab = expression(theta), ylab = "Densidad", col = "gray", border = "gray")
# posterior analítica
th <- seq(min(THETA), max(THETA), length = 1000)
mu.n <- (mu/t2 + mean(y)*n/s2)/(1/t2 + n/s2) 
t2.n <- 1/(1/t2 + n/s2)
lines(x = th, y = dnorm(th, mu.n, sqrt(t2.n)), col = 2, lty = 1, lwd = 1)
```


```{r}
ACR    <- NULL  # tasa de aceptaciones
ACF    <- NULL  # autocorrelaciones
THETAA <- NULL  # muestras
for(delta2 in 2^c(-5,-1,1,5,7) ) {
  # parametros iniciales
  THETA <- NULL
  S     <- 10000
  theta <- 0
  acs   <- 0  # tasa de aceptacion
  # cadena
  set.seed(1)
  for(s in 1:S) {
    # 1. propuesta
    theta.star<-rnorm(1,theta,sqrt(delta2))
    # 2. tasa de aceptacion
    log.r <- (sum(dnorm(y, theta.star, sqrt(s2), log = T)) + dnorm(theta.star, mu, sqrt(t2), log = T)) - (sum(dnorm(y, theta, sqrt(s2), log = T)) + dnorm(theta, mu, sqrt(t2), log = T)) 
    # 3. actualizar
    if(runif(1) < exp(log.r)) { 
      theta <- theta.star 
      acs   <- acs + 1 
    }
    # 4. almacenar
    THETA <- c(THETA, theta) 
  }
  # fin MCMC
  # almacenar valores de todos los casos (delta2)
  ACR    <- c(ACR, acs/s) 
  ACF    <- c(ACF, acf(THETA, plot = F)$acf[2])
  THETAA <- cbind(THETAA, THETA)
}
```


```{r, fig.height=4, fig.width=12, fig.align='center', echo = F}
# gráficos
par(mfrow = c(1,3), mar = c(2.75,2.75,.5,.5), mgp = c(1.7,0.75,0))
laby <- c(expression(theta),"","","","")
for (k in c(1,3,5)) {
  plot(x = THETAA[1:500,k], type = "l", xlab = "Iteración", ylab = laby[k], ylim = range(THETAA))
  abline(h = mu.n, lty = 2)
}
```


```{r}
# tasas de aceptacion
round(ACR, 3)
# autocorrelaciones
round(ACF ,3)
# tamaños efectivos de muestra
round(coda::effectiveSize(THETAA), 3)
```


# Ejemplo: Regresión Poisson

Actividades de reproducción de gorriones en función de la edad.

Caracterizar el número de crías en función de la edad.

***Arcese, P., Smith, J. N., Hochachka, W. M., Rogers, C. M., & Ludwig, D. (1992). Stability, regulation, and the determination of abundance in an insular song sparrow population. Ecology, 73(3), 805-822.***

```{r, eval = TRUE, echo=FALSE, out.width="50%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("sparrow.jpg")
```

- $y_i\,\,\,\,$: número de crías, para $i=1,\ldots,n$.
- $x_{i,j}\,$: predictor $j$ observado en el individuo $i$, para $i=1,\ldots,n$ y $j=1,\ldots,k$.

```{r}
#-------------------------------------------------------------------------------
# Descripcion: 
# Actividades de reproduccion de gorriones en funcion de la edad (Arcese et al, 1992).
# n = 52 gorriones hembras.
# "age"     : edad.
# "fledged" : numero de crias.
#-------------------------------------------------------------------------------

############
### data ###
############
spfage <- structure(c(3, 1, 1, 2, 0, 0, 6, 3, 4, 2, 1, 6, 2, 3, 3, 4, 7, 2, 2, 1, 
                      1, 3, 5, 5, 0, 2, 1, 2, 6, 6, 2, 2, 0, 2, 4, 1, 2, 5, 1, 2, 
                      1, 0, 0, 2, 4, 2, 2, 2, 2, 0, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
                      1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 
                      2, 2, 2, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 
                      4, 4, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 6, 1, 1, 9, 9, 1, 1, 
                      1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 25, 25, 16, 16, 
                      16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 25, 16, 16, 16, 16, 
                      25, 25, 25, 25, 9, 9, 9, 9, 9, 9, 9, 36, 1, 1), 
                    .Dim = c(52L, 4L), 
                    .Dimnames = list(NULL, c("fledged", "intercept", "age", "age2")))
spfage <- as.data.frame(spfage)
# diseño
spf  <- spfage$fledged  # y  = variable dependiente (respuesta)
age  <- spfage$age      # x1 = variable independiente 1
age2 <- age^2           # x2 = variable independiente 2
```


```{r, fig.width=10, fig.height=5}
############################
### analisis descriptivo ###
############################
par(mar=c(3,3,1,1),mgp=c(1.75,.75,0))
plot(spf~as.factor(age), range=0, xlab="Edad (años)", ylab="No. Crias", col="gray", border="lightgray")
```


```{r}
# GLM (frecuentista)
summary(glm(spf~age+age2,family="poisson"))
```

En muchos problemas, la varianza posterior es una elección eficiente para la 
distribución de propuestas. Aunque no se conoce la varianza posterior antes de 
ejecutar el algoritmo de Metrópolis, basta con utilizar una aproximación.
Si esto da como resultado una tasa de aceptación demasiado alta o demasiado baja, 
siempre es posible ajustar la variabilidad de la propuesta en consecuencia.


```{r}
# datos 
y <- spf                                # variable respuesta
X <- cbind(rep(1,length(y)),age,age^2)  # matriz de diseño
n <- length(y)                          # tamaño de la muestra
p <- dim(X)[2]                          # numero de predictores
# previa
pmn.beta <- rep(0,  p)  # beta0 = 0
psd.beta <- rep(sqrt(10), p)  # Sigma0 = 10*I
# log: funcion de enlace 
# y + 1/2 evitar problemas en la frontera con 0
var.prop <- var(log(y + 1)) * solve(t(X)%*%X) # matriz de varianza propuesta
beta <- rep(0,p) # valor inicial beta
######## algoritmo de metropolis
S    <- 50000
BETA <- matrix(data = NA, nrow = S, ncol = p)
ac   <- 0
ncat <- floor(S/10)
######## cadena
set.seed(1)
for(s in 1:S) {
  # 1. propuesta
  beta.p <- c(mvtnorm::rmvnorm(n = 1, mean = beta, sigma = var.prop))
  # 2. tasa de aceptacion
  lhr <- sum(dpois(x = y, lambda = exp(X%*%beta.p), log = T)) - sum(dpois(x = y, lambda = exp(X%*%beta), log = T)) + sum(dnorm(x = beta.p, mean = pmn.beta, sd = psd.beta, log = T)) - sum(dnorm(x = beta, mean = pmn.beta, sd = psd.beta, log = T))
  # 3. actualizar
  if (runif(1) < exp(lhr)) { 
    beta <- beta.p 
    ac   <- ac + 1 
  }
  # 4. almacenar
  BETA[s,] <- beta
  # 5. Progreso
  if (s%%ncat == 0) cat(100*round(s/S, 1), "% completado ... \n", sep = "" )
}
######### fin mcmc
```


```{r}
# tasa de aceptacion
round(100*ac/S, 1)
# diagnosticos
round(apply(X = BETA, MARGIN = 2, FUN = coda::effectiveSize), 1)
```


```{r, fig.height=4, fig.width=12, echo = F, fig.align='center'}
# grafico diagnostico
blabs <- c(expression(beta[1]),expression(beta[2]),expression(beta[3]))  # etiquetas
thin  <- seq(from = 10, to = S, by = 10)  # muestreo sistematico 
par(mfrow = c(1,3), mar = c(2.75,2.75,.5,.5), mgp = c(1.7,.7,0))
j <- 3
plot(x = thin, y = BETA[thin,j], type = "l", xlab = "Iteración", ylab = blabs[j])
acf (x = BETA[thin,j], xlab = "lag/10")
acf (x = BETA[,j], xlab = "lag")
```

```{r, fig.height=4, fig.width=12, echo = F, fig.align='center'}
par(mfrow=c(1,3), mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
plot(density(BETA[,2]), type="l", main="",
     xlab=expression(beta[2]),
     ylab=expression(paste(italic("p("),beta[2],"|",italic("y)"),sep="") ) ,
     lwd=1,lty=1,col="black")
plot(density(BETA[,3]), type="l", main="",
     xlab=expression(beta[3]),
     ylab=expression(paste(italic("p("),beta[3],"|",italic("y)"),sep="") ),
     lwd=1,col="black",lty=1)
Xs       <- cbind(rep(1,6),1:6,(1:6)^2) 
eXB.post <- exp(t(Xs%*%t(BETA )) )
qE       <- apply( eXB.post,2,quantile,probs=c(.025,.5,.975))
plot(c(1,6),range(c(0,qE)),type="n",xlab="Edad (años)", ylab="No. Crias")
lines(x = qE[1,], col = 4, lwd = 1 )
lines(x = qE[2,], col = 1, lwd = 2 )
lines(x = qE[3,], col = 4, lwd = 1 )
```

```{r}
round(quantile(BETA[,2], c(.025, .975)), 3)
round(quantile(BETA[,3], c(.025, .975)), 3)
round(mean(BETA[,2] > 0), 3)
round(mean(BETA[,3] > 0), 3)
```


# Algoritmo de Monte Carlo Hamiltoniano


Un algoritmo alternativo para mejorar la eficiencia del Algoritmo de Metropolis-Hastings es el algoritmo de **Monte Carlo Hamiltoniano** (HMC, *Hamiltonian Monte Carlo*).

En el algoritmo Hamiltoniano se incorpora una variable de "impulso" $\varphi$, con el fin de explorar más rápidamente la distribución objetivo mediante movimientos en diferentes trayectorias, suprimiendo el movimiento de caminata aleatoria local. 

Las simulaciones se hacen de la distribución conjunta $p(\theta, \varphi \mid \yv) = p(\theta \mid \yv)\,p(\varphi)$. Solo son de interés las simulaciones de $\theta$, dado que $\varphi$ opera como una variable auxiliar introducida para que el algoritmo se mueva de manera eficiente a través del espacio de parámetros. 

**Algoritmo:**

1. Simular $\varphi \sim \text{N}(0, \mathbf{M})$, donde $\mathbf{M}$ es una matriz diagonal que representa la matriz de covarianza de la función de impulso $p(\varphi)$.
2. Actualizar $(\theta, \varphi)$ mediante $L$ saltos escaldados por un factor $\epsilon$:
    
    a. Actualizar $\varphi$:
    $$
    \varphi \leftarrow \varphi + \frac{\epsilon }{2}\,\frac{\partial}{\partial \theta}\log p(\theta \mid \yv)
    $$
    b. Actualizar $\theta$:
    $$
    \theta \leftarrow \theta + \epsilon\,\mathbf{M}\varphi
    $$
    c. Repetir los pasos anteriores $L-1$ veces.
    d. Actualizar $\theta$ para completar el último salto:
    $$
    \varphi \leftarrow \varphi + \frac{\epsilon }{2}\,\frac{\partial}{\partial \theta}\log p(\theta \mid \yv)
    $$
3. Sean $\theta^{(b-1)}$ y $\varphi^{(b-1)}$ los valores iniciales de $\theta$ y $\varphi$ respectivamente, y $\theta^{*}$, $\varphi^{*}$ los valores correspondientes después de los $L$ pasos. Calcular la tasa de aceptación:
$$
r= \frac{p(\theta^{*} \mid \yv)\,p(\varphi^{*})}{p(\theta^{(b-1)} \mid \yv)\,p(\varphi^{(b-1)})}
$$
4. Establecer $\theta^{(b)} = \theta^*$ con probabilidad $\min\{r,1\}$ o $\theta^{(b)} = \theta^{(b-1)}$ con probabilidad $1-\min\{r,1\}$.

## Observaciones

- Es una práctica común elegir un valor de $\mathbf{M}$, $\epsilon$ y $L$ tal que la tasa de aceptación esté entre 60% y 70%. Existen algoritmos adaptativos para elegir automáticamente estas cantidades.
- Típicamente se hace $\mathbf{M} = \mathbf{I}$, $\epsilon = 1/L$ con $L$ un entero positivo (e.g. $L=10$).
- No es necesario almacenar los valores de $\varphi$.

# Ejemplo: Regresión Poisson (continuación)

```{r}
# datos 
y <- spf                                # variable respuesta
X <- cbind(rep(1,length(y)),age,age^2)  # matriz de diseño
n <- length(y)                          # tamaño de la muestra
p <- dim(X)[2]                          # numero de predictores
# previa
pmn.beta <- rep(0,  p)        # beta0 = 0
psd.beta <- rep(sqrt(10), p)  # Sigma0 = 10*I
# log: funcion de enlace 
# y + 1/2 evitar problemas en la frontera con 0
var.prop <- var(log(y + 1)) * solve(t(X)%*%X) # matriz de varianza propuesta
beta <- rep(0,p) # valor inicial beta
######## algoritmo de metropolis
S    <- 2100
BETA <- matrix(data = NA, nrow = S, ncol = p)
ac   <- 0
ncat <- floor(S/10)
######## cadena
set.seed(1)
for(s in 1:S) {
  # 1. propuesta
  beta.p <- c(mvtnorm::rmvnorm(n = 1, mean = beta, sigma = var.prop))
  # 2. tasa de aceptacion
  lhr <- sum(dpois(x = y, lambda = exp(X%*%beta.p), log = T)) - sum(dpois(x = y, lambda = exp(X%*%beta), log = T)) + sum(dnorm(x = beta.p, mean = pmn.beta, sd = psd.beta, log = T)) - sum(dnorm(x = beta, mean = pmn.beta, sd = psd.beta, log = T))
  # 3. actualizar
  if (runif(1) < exp(lhr)) { 
    beta <- beta.p 
    ac   <- ac + 1 
  }
  # 4. almacenar
  BETA[s,] <- beta
  # 5. Progreso
  if (s%%ncat == 0) cat(100*round(s/S, 1), "% completado ... \n", sep = "" )
}
######### fin mcmc
# tasa de aceptación
round(ac/S, 2)
```


```{r}
# gradiente
grad <- function(bet, y, X) {
  out <- 0
  for (i in 1:length(y))
    out <- out + (y[i] - exp(sum(bet*X[i,])))*X[i,]
  out
}
```


```{r}
##########
# previa #
##########
mu0    <- rep(0,  p)   # beta0 = 0
Sigma0 <- diag(10, p)  # Sigma0 = 10*I
##########
# cadena #
##########
L   <- 100
eps <- 1/L
bet <- rep(0, p) # valor inicial beta
ac  <- 0         # tasa de aceptación
S   <- 2100
BETA2 <- matrix(data = NA, nrow = S, ncol = p)
set.seed(123)
for(s in 1:S) {
  # 1. propuesta
  bet.p <- bet
  z <- c(mvtnorm::rmvnorm(n = 1, mean = rep(0,p), sigma = diag(p)))
  z.p <- z
  z.p <- z.p + 0.5*eps*grad(bet.p, y, X)
  for (l in 1:L) {
    bet.p <- bet.p + eps*z.p
    if (l < L)
      z.p <- z.p + eps*grad(bet.p, y, X)
  }
  z.p <- z.p + 0.5*eps*grad(bet.p, y, X)
  # 2. tasa de aceptación
  logr <- sum(dpois(x = y, lambda = exp(X%*%bet.p), log = T)) - sum(dpois(x = y, lambda = exp(X%*%bet),log = T)) + 
    mvtnorm::dmvnorm(x = bet.p, mean = mu0, sigma = Sigma0, log = T) - mvtnorm::dmvnorm(x = bet, mean = mu0, sigma = Sigma0, log = T) +
    mvtnorm::dmvnorm(x = z.p, log = T) - mvtnorm::dmvnorm(x = z, log = T)
  # 3. actualizar
  if (runif(n = 1) < exp(logr)) { 
    bet <- bet.p 
    ac  <- ac + 1 
  }
  # 4. almacenar
  BETA2[s,] <- bet
  # 5. Progreso
  if (s%%floor(S/10) == 0) cat(100*round(s/S, 1), "% completado ...\n", sep = "" )
}
# fin mcmc
# tasa de aceptación
round(ac/S, 2)
```


```{r, fig.align='center', fig.width=10, fig.height=9}
BETA <- BETA[-c(1:100),]
BETA2 <- BETA2[-c(1:100),]
par(mfrow = c(3,2), mar = c(2.75,2.75,1.5,.5), mgp = c(1.7,.7,0))
for (j in 1:p) {
  rango <- range(BETA[,j], BETA2[,j])
  # Metropolis
  plot(x = 1:nrow(BETA), y = BETA[,j], col = 2, type = "p", pch = 16, ylim = rango, cex.axis = 0.9, main = "Metropolis", xlab = "Iteración", ylab = "")
  # HMC
  plot(x = 1:nrow(BETA2), y = BETA2[,j], col = 4, type = "p", pch = 16, ylim = rango, cex.axis = 0.9, main = "Hamiltoniano", xlab = "Iteración", ylab = "")
}
```


```{r}
# tamaños efectivos & CV de MC
tab <- cbind(
  round(apply(X = BETA,  MARGIN = 2, FUN = coda::effectiveSize), 1),
  round(apply(X = BETA2, MARGIN = 2, FUN = coda::effectiveSize), 1),
  round(apply(X = BETA,  MARGIN = 2, FUN = sd)/sqrt(apply(X = BETA,  MARGIN = 2, FUN = coda::effectiveSize)), 3),
  round(apply(X = BETA2, MARGIN = 2, FUN = sd)/sqrt(apply(X = BETA2, MARGIN = 2, FUN = coda::effectiveSize)), 3)
)
rownames(tab) <- paste("beta", 1:3)
colnames(tab) <- c("neff Metro", "neff HMC", "CVMC Met", "CVMC HMC")
round(tab, 3)
```


# Ejemplo: Regresión Poisson en JAGS

```{r, eval=F}
########
# JAGS #
########
library(R2jags)
model <- function() {
    # verosimilitud
    for (i in 1:n) {
        y[i] ~ dpois(theta[i])
        log(theta[i]) <- inprod(X[i,], beta) # X[i,1]*beta[1]+X[i,2]*beta[2]+X[i,3]*beta[3]
    }
    # previa
    for (j in 1:p) {
        beta[j] ~ dnorm(beta0, phi0)    
    }
}
# previa
beta0 <- 0
phi0  <- 1/10
# input
model_data <- list(y = y, X = X, n = n, p = p, beta0 = beta0, phi0 = phi0)
# parameters
model_parameters <- c("beta")
# initial values
initial_values <- list(list("beta" = rep(beta0, p)), 
                       list("beta" = rep(beta0, p)),
                       list("beta" = rep(beta0, p)))
# mcmc settings
niter  <- 26000
nburn  <- 1000
nthin  <- 25
nchain <- length(initial_values)
# mcmc
set.seed(123)
fit <- jags(data = model_data, inits = initial_values, 
            parameters.to.save = model_parameters, model.file = model, 
            n.chains = nchain, n.iter = niter, n.thin = nthin, n.burnin = nburn)
print(fit)
# transformar a objecto MCMC               
fit_mcmc <- coda::as.mcmc(fit)
# cadena
dim(fit$BUGSoutput$sims.list$beta)
# plots
mcmcplots::traplot(fit_mcmc)
mcmcplots::denplot(fit_mcmc)
```


# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```


