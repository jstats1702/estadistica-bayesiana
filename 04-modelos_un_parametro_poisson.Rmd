---
title: "Modelo Poisson"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Ejemplo de motivación

**Censo Nacional de Población y Vivienda - CNPV - 2018**  disponible en este [enlace](https://microdatos.dane.gov.co/index.php/catalog/643/study-description). 

Diccionario de datos (ddi-documentation-spanish-643.pdf) disponible en este [enlace](https://microdatos.dane.gov.co/index.php/catalog/643/datafile/F11).

La base de datos contiene la información de una **muestra aleatoria simple de personas** que residen en hogares particulares o personas que residen en lugares especiales de alojamiento con las características correspondientes al censo.

¿Existen diferencias significativas entre las tasas promedio del **número de hijos** de mujeres de 40 a 44 años con educación superior y sin educación superior?

# Modelo

El modelo para **variables de conteo** $y_i\in \mathbb{N}_0$, para $i = 1,\ldots,n$, está dado por
$$
\begin{align}
	y_i\mid\theta &\stackrel{\text{iid}}{\sim}\textsf{Poisson}(\theta) \\
	\theta &\sim p(\theta)
\end{align}
$$
donde $\theta\in \mathbb{R}^+$.

Este modelo es potencialmente **restrictivo** por la **relación media-varianza**: $\textsf{E}(y_i\mid\theta) = \textsf{Var}(y_i\mid\theta) = \theta$. 

Alternativas: 

- Distribución Binomial Negativa (sobre-dispersión, varianza superior a la esperada).
- Distribución Comway-Maxwell-Poisson (sub-dispersión, varianza menor a la esperada).

La **distribución muestral** (distribución condicional conjunta) de $\boldsymbol{y} = (y_1,\ldots,y_n)$ dado $\theta$ es
$$
  p(\boldsymbol{y}\mid\theta) = \prod_{i=1}^n \frac{\theta^{y_i}\,e^{-\theta}}{y_i!} = \frac{1}{\prod_{i=1}^n y_i!}\,\theta^{s}e^{-n\theta}\,,\quad  s= \textstyle\sum_{i=1}^n y_i\,,
$$
lo cual sugiere que $s$ es un **estadístico suficiente** para $\theta$.

Dado que las $y_i$'s son condicionalmente i.i.d. dado $\theta$ y $s$ es un estadístico suficiente para $\theta$, entonces se acostumbra utilizar el modelo
$$
\begin{align*}
	s\mid\theta &\sim \textsf{Poisson}(n\theta) \\
	\theta &\sim p(\theta) 
\end{align*}
$$
$s\in\mathbb{N}_0$.

# Modelo Gamma-Poisson

La familia de distribuciones **Gamma** es **conjugada** para la distribución muestral **Poisson**.

Así, el **modelo Gamma-Poisson** es
$$
\begin{align*}
	s\mid\theta&\stackrel{\text{iid}}{\sim}\textsf{Poisson}(n\theta) \\
	\theta &\sim \textsf{Gamma}(a,b)
\end{align*}
$$
donde $a$ y $b$ son los **hiperparámetros** (cantidades fijas conocidas) del modelo. Los hiperparámetros se eligen de tal forma que la distribución previa refleje el **estado de información externo** al conjunto de datos.

### Distribución posterior {-}

Bajo el **modelo Gamma-Poisson** se tiene que la **distribución posterior** de $\theta$ es
$$
\theta \mid s \sim \textsf{Gamma}(\theta\mid a + s, b+n)\,.
$$

### Distribución marginal {-}

La **distribución marginal** de $s$ es 
$$
p(s) = \frac{\Gamma(a+n)}{\Gamma(a)\,\Gamma(s+1)}\left( \frac{b}{b+1}\right)^a\left(\frac{1}{b+1}\right)^s\,,\quad s\in\mathbb{N}_0\,.
$$

Esta distribución se conoce como **distribución Beta Binomial** con parámetros $n\in\mathbb{N}$, $a>0$ y $b>0$, lo que se denota con $y\sim\textsf{Beta-Binomial}(n,a,b)$. Esta distribución es un promedio ponderado (mezcla) de distribuciones Binomiales, ponderadas por la distribución Beta. 

### Media posterior {-}

La **media posterior** es
$$
        \textsf{E}(\theta\mid s) = \frac{a+s}{b+n} = \frac{b}{b+n}\cdot \frac{a}{b}+\frac{n}{b+n}\cdot \frac{s}{n}\,,
$$
la cual es un **promedio ponderado** de la media previa $\textsf{E}(\theta) = \frac{a}{a+b}$ y la media muestral $\bar{y} = \frac{s}{n}$ con pesos proporcionales a $b$ y $n$, respectivamente. 

- Tal observación conlleva a la siguiente interpretación de los hiperparámetros: 
    - $b$ = número previo de observaciones.
    - $a$ = suma de los conteos asociados con las $b$ observaciones previas.
- Si $n>>b$, entonces la mayoría de la información proviene de los datos en lugar de la información previa.

### Predicción {-}

La **distribución predictiva posterior** de una observación futura $y^*\in\mathbb{N}_0$ es $y^*\mid s \sim \textsf{BN}(a+s,b+n)$, i.e.,
$$
p(y^*\mid s) = \frac{\Gamma(y^* +a+s)}{\Gamma(a+s)\Gamma(y^*+1)}\left[\frac{b+n}{b+n+1}\right]^{a+s} \left[\frac{1}{b+n+1}\right]^{y^*}\,.
$$

**(Definición.)** La variable aleatoria $X$ tiene distribución **Binomial Negativa** con parámetros $\alpha,\beta > 0$, i.e., $X\sim\textsf{BN}(\alpha,\beta)$, si su función de masa de probabilidad es
$$
p(x) = \frac{\Gamma(x+\alpha)}{\Gamma(\alpha)\,\Gamma(x+1)}\,\left[\frac{\beta}{\beta+1}\right]^{\alpha}\,\left[\frac{1}{\beta+1}\right]^x\,,\quad x\in\mathbb{N}_0\,.
$$

Por medio de la distribución predictiva posterior se caracterizan diversos aspectos acerca de una observación futura. Por ejemplo, la varianza predictiva $\textsf{Var}(y^*\mid \boldsymbol{y})$ se puede interpretar como una medida de la **incertidumbre posterior acerca de una observación futura** $y^*$. 

Esto motiva un contraste interesante entre **inferencia** y **predicción**: $\theta$ (el objetivo inferencial) y $y^*$ (el objetivo predictivo) tienen la misma media posterior, pero la varianza posterior de $y^*$ es mayor:
$$
\textsf{E}(\theta\mid\boldsymbol{y}) = \textsf{E}(y^*\mid\boldsymbol{y}) = \frac{a+s}{b+n}\,,
$$
mientras que 
$$
\textsf{Var}(\theta\mid\boldsymbol{y}) = \frac{a+s}{b+n}\left(0 + \frac{1}{b+n}\right)
\qquad\text{y}\qquad
\textsf{Var}(y^*\mid\boldsymbol{y}) = \frac{a+s}{b+n}\left(1 + \frac{1}{b+n}\right)\,.
$$


# Ejemplo: Número de hijos y educación

**Censo Nacional de Población y Vivienda - CNPV - 2018**  disponible en este [enlace](https://microdatos.dane.gov.co/index.php/catalog/643/study-description). 

Diccionario de datos (ddi-documentation-spanish-643.pdf) disponible en este [enlace](https://microdatos.dane.gov.co/index.php/catalog/643/datafile/F11).

La base de datos contiene la información de una **muestra aleatoria simple de personas** que residen en hogares particulares o personas que residen en lugares especiales de alojamiento con las características correspondientes al censo.

¿Existen diferencias significativas entre las tasas promedio del **número de hijos** de mujeres de 40 a 44 años con educación superior y sin educación superior?

## Tratamiento de datos {-}

Se consideran personas identificadas como: mujer, jefe de hogar, 40 a 44 años, alfabeta, lugar de nacimiento en Colombia, lugar de residencia hace 5 años en Colombia, ningún grupo étnico, informa si tiene hijos o no.

```{r}
# datos 
df <- read.csv("CNPV2018.txt")
dim(df)
```
`P_NIVEL_ANOSR`: Nivel educativo más alto alcanzado y último año o grado aprobado en ese nivel.
   
- 1: Preescolar.
- 2: Básica primaria.
- 3: Básica secundaria.
- 4: Media académica o clásica.
- 5: Media técnica.
- 6: Normalista.
- 7: Técnica profesional o tecnológica.
- 8: Universitario.
- 9: Especialización, maestría, doctorado.
- 10: Ninguno.
- 99: No Informa.

```{r}
# codificación: nivel educativo
df[df$P_NIVEL_ANOSR %in% c(1,2,3,4,5,6,7,10), c("P_NIVEL_ANOSR")] <- 0 # sin educación superior
df[df$P_NIVEL_ANOSR %in% c(8,9),              c("P_NIVEL_ANOSR")] <- 1 # con educación superior
df$P_NIVEL_ANOSR <- as.numeric(df$P_NIVEL_ANOSR)
```


```{r}
# frecuencias: indicadora de educación superior
table(df$P_NIVEL_ANOSR)
```


`PA1_THNV`: Hijos(as) nacidos vivos.


```{r}
# codificación: sin hijos
df[is.na(df$PA1_THNV), c("PA1_THNV")] <- 0
df$PA1_THNV <- as.numeric(df$PA1_THNV)
```


```{r}
# frecuencias: número de hijos
table(df$PA1_THNV)
```


```{r}
# remover datos faltantes
df <- df[(df$P_NIVEL_ANOSR != 99) & (df$PA1_THNV != 99),]
```


```{r}
# filtro
indices <- (df$P_PARENTESCOR == 1) & (df$P_SEXO == 2) & (df$P_EDADR == 9) & (df$PA1_GRP_ETNIC == 6) & (df$PA_LUG_NAC %in% c(2,3)) & (df$PA_VIVIA_5ANOS %in% c(2,3)) & (df$PA_HNV %in% c(1,2)) & (df$P_ALFABETA == 1)
# y1: número de hijos, mujeres de 40 años, sin educación superior
# y2: número de hijos, mujeres de 40 años, con educación superior
y1 <- as.numeric(df[indices & (df$P_NIVEL_ANOSR == 0), c("PA1_THNV")])
y2 <- as.numeric(df[indices & (df$P_NIVEL_ANOSR == 1), c("PA1_THNV")])
```


```{r}
# tamaños de muestra
n1 <- length(y1)
print(n1)
n2 <- length(y2)
print(n2)
# estadísticos suficientes
s1 <- sum(y1)
print(s1)
s2 <- sum(y2)
print(s2)
# relación media-varianza
r1 <- mean(y1)/var(y1)
print(r1)
r2 <- mean(y2)/var(y2)
print(r2)
```

```{r, echo = F, fig.height = 5, fig.width = 5, fig.align='center'}
# distribución de frecuencias
par(mfrow = c(1,1), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
y <- 0:6
plot(y - .07, table(factor(x = y1, levels = y))/n1, col = 2, type = "h", ylim = c(0,.4), lwd = 3, ylab = "F. Relativa", xlab = "No. de hijos", main = "", yaxt = "n")
points(y + .07, table(factor(x = y2, levels = y))/n2, col = 4, type = "h", lwd = 3)
axis(side = 2)
legend("topright", legend = c("Sin superior", "Con superior"), bty = "n", lwd = 2, col = c(2,4))
```


## Distribuciones posterior y predictiva posterior


```{r}
# previa Gamma(2,1)
a <- 2
b <- 1
# media previa de theta
a/b
# CV previo de theta
sqrt(a/b^2)/(a/b)
# parámetros de la distribución posterior de theta
ap1 <- a + s1
print(ap1)
bp1 <- b + n1
print(bp1)
ap2 <- a + s2
print(ap2)
bp2 <- b + n2
print(bp2)
```


```{r, echo=F, fig.height = 5, fig.width = 10, fig.align='center'}
# gráfico
par(mfrow = c(1,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
# posterior
theta <- seq(0, 5, length = 1000)
plot(NA, NA, xlim = c(0,4), ylim = c(0,5.5), xlab = expression(theta), ylab = expression(paste("p","(",theta," | ",y,")",sep="")), main = "Posterior")
lines(theta, dgamma(theta, shape = ap1, rate = bp1), col = 2, lwd = 2)
lines(theta, dgamma(theta, shape = ap2, rate = bp2), col = 4, lwd = 2)
lines(theta, dgamma(theta, shape = a  , rate = b  ), col = 1, lwd = 1)
abline(h = 0, col = 1)
legend("topright", legend = c("Sin superior", "Con superior", "Previa"), bty = "n", lwd = 2, col = c(2, 4, 1))
# predictiva
y <- 0:12
plot(y - .07, dnbinom(y, size = ap1, mu = ap1/bp1), col = 2, type = "h", ylim = c(0,.4), lwd = 3, ylab = "p(y* | y )", xlab = "y*", main = "Predictiva posterior")
points(y + .07, dnbinom(y, size = ap2, mu = ap2/bp2), col = 4, type = "h", lwd = 3)
```


```{r, echo = FALSE}
# media posterior de theta
theta_hat_1 <- ap1/bp1
theta_hat_2 <- ap2/bp2 
# CV posterior de eta
cv_1 <- sqrt(ap1/bp1^2)/(ap1/bp1)
cv_2 <- sqrt(ap2/bp2^2)/(ap2/bp2)
# intervalo de credibilidad al 95% para theta
ic95_1 <- qgamma(p = c(.025,.975), shape = ap1, rate = bp1)
ic95_2 <- qgamma(p = c(.025,.975), shape = ap2, rate = bp2)
# probabilidad posterior de theta > 2
pr_theta_1 <- pgamma(q = 2, shape = ap1, rate = bp1, lower.tail = F)
pr_theta_2 <- pgamma(q = 2, shape = ap2, rate = bp2, lower.tail = F)
# probabilidad posterior de y* > 2
pr_y_1 <- pnbinom(q = 2, size = ap1, mu = ap1/bp1, lower.tail = F)
pr_y_2 <- pnbinom(q = 2, size = ap2, mu = ap2/bp2, lower.tail = F)
# tabla
tab <- cbind(c(theta_hat_1, cv_1, ic95_1, pr_theta_1, pr_y_1),
             c(theta_hat_2, cv_2, ic95_2, pr_theta_2, pr_y_2))
colnames(tab) <- c("Sin superior", "Con superior")
rownames(tab) <- c("Media tasa", "CV tasa", "Q2.5% tasa", "Q97.5% tasa", "Pr. tasa > 2", "Pr. pred. > 2")
knitr::kable(x = t(tab), digits = 3, align = "c", caption = "Inferencia sobre la tasa del número de hijos.")
```


## Comparación de grupos


```{r}
set.seed(123)
# muestras de la distribución posterior de theta
th1_mc <- rgamma(n = 10000, shape = ap1, rate = bp1)
th2_mc <- rgamma(n = 10000, shape = ap2, rate = bp2)
# muestras de la distribución predictiva posterior
y1_mc <- rpois(n = 10000, lambda = th1_mc)
y2_mc <- rpois(n = 10000, lambda = th2_mc)
```


```{r}
# media posterior de theta_1
mean(th1_mc)
```

```{r}
# CV posterior de theta_1
sd(th1_mc)/mean(th1_mc)
```


```{r}
# intervalos de credibilidad al 95% para theta_1
quantile(x = th1_mc, probs = c(.025,.975))
```


```{r}
# probabilidad posterior de theta_1 > 2
mean(th1_mc > 2)
```

```{r}
# probabilidad posterior de y_1* > 2
mean(y1_mc > 2)
```

```{r}
# muestras de la distribución posterior de eta = theta_1 - theta_2
eta <- th1_mc - th2_mc
```


```{r, echo = FALSE, fig.height = 5, fig.width = 10, fig.align='center'}
# gráfico
par(mfrow = c(1,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
# posterior
theta <- seq(0, 5, length = 1000)
hist(x = eta, freq = F, col = "gold2", border = "gold2", xlab = expression(eta), ylab = expression(paste("p","(",eta," | ",y,")",sep="")), main = "Posterior")
plot(x = density(eta), col = "gold3", lwd = 2, xlab = expression(eta), ylab = expression(paste("p","(",eta," | ",y,")",sep="")), main = "Posterior")
```


```{r}
# media posterior de eta = theta_1 - theta_2
mean(eta)
# CV posterior de eta = theta_1 - theta_2
sd(eta)/mean(eta)
# intervalos de credibilidad al 95% para eta = theta_1 - theta_2
quantile(x = eta, probs = c(.025,.975))
```

```{r, echo=F}
# ingerencia Bayesiana
est_B <- mean(eta)
cv_B  <- sd(eta)/mean(eta)
ic_B  <- quantile(x = eta, probs = c(.025,.975))
# inferencia frecuentista: asintótica
yb1 <- mean(y1)
yb2 <- mean(y2)
sd1 <- sd(y1)
sd2 <- sd(y2)
est_F1 <- yb1 - yb2
cv_F1  <- sqrt(sd1^2/n1 + sd2^2/n2)/(yb1 - yb2)
ic_F1  <- yb1 - yb2 + c(-1,1)*qnorm(p = 0.975)*sqrt(sd1^2/n1 + sd2^2/n2)
# inferencia frecuentista: bootstrap paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy1 <- rpois(n = n1, lambda = yb1)
  yy2 <- rpois(n = n2, lambda = yb2)
  out[i] <- mean(yy1) - mean(yy2)
}
est_F2 <- mean(out)
cv_F2  <- sd(out)/mean(out)
ic_F2  <- quantile(x = out, probs = c(.025,.975))
# inferencia frecuentista: bootstrap no paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy1 <- sample(x = y1, size = n1, replace = T)
  yy2 <- sample(x = y2, size = n2, replace = T)
  out[i] <- mean(yy1) - mean(yy2)
}
est_F3 <- mean(out)
cv_F3  <- sd(out)/mean(out)
ic_F3  <- quantile(x = out, probs = c(.025,.975))
```


```{r, echo=F}
# resultados
tab <- rbind(c(est_B , cv_B , ic_B ), 
             c(est_F1, cv_F1, ic_F1), 
             c(est_F2, cv_F2, ic_F2),
             c(est_F3, cv_F3, ic_F3))
colnames(tab) <- c("Estimación", "CV", "L. Inf.", "L. Sup.")
rownames(tab) <- c("Bayesiana", "Frec. Asintótico", "Frec. Bootstrap Par.", "Frec. Bootstrap No Par.")
knitr::kable(x = tab, digits = 3, align = "c", caption = "Inferencia sobre la diferencia entre las tasas promedio del número de hijos de mujeres de 40 a 44 años con educación superior y sin educación superior.")
```


# Ejemplo: Número de hijos y educación (cont.)


```{r, echo = F}
# filtro
indices <- (df$P_PARENTESCOR == 1) & (df$P_SEXO == 2) & (df$PA1_GRP_ETNIC == 6) & (df$PA_LUG_NAC %in% c(2,3)) & (df$PA_VIVIA_5ANOS %in% c(2,3)) & (df$PA_HNV %in% c(1,2)) & (df$P_ALFABETA == 1)
# previa Gamma(2,1)
a <- 2
b <- 1
# P_EDADR: Edad en Grupos Quinquenales
#   1 de 00 A 04 Años
#   2 de 05 A 09 Años
#   3 de 10 A 14 Años
#   4 de 15 A 19 Años
#   5 de 20 A 24 Años
#   6 de 25 A 29 Años
#   7 de 30 A 34 Años
#   8 de 35 A 39 Años
#   9 de 40 A 44 Años
#   10 de 45 A 49 Años
#   11 de 50 A 54 Años
#   12 de 55 A 59 Años
#   13 de 60 A 64 Años
#   14 de 65 A 69 Años
#   15 de 70 A 74 Años
#   16 de 75 A 79 Años
#   17 de 80 A 84 Años
#   18 de 85 A 89 Años
#   19 de 90 A 94 Años
#   20 de 95 A 99 Años
#   21 de 100 y más Años
out <- NULL
set.seed(1234)
for (k in 5:14) {
  # datos
  y1 <- as.numeric(df[indices & (df$P_EDADR == k) & (df$P_NIVEL_ANOSR == 0), c("PA1_THNV")])
  y2 <- as.numeric(df[indices & (df$P_EDADR == k) & (df$P_NIVEL_ANOSR == 1), c("PA1_THNV")])
  # tamaños de muestra
  n1 <- length(y1)
  n2 <- length(y2)
  # estadísticos suficientes
  s1 <- sum(y1)
  s2 <- sum(y2)
  # parámetros de la posterior
  ap1 <- a + s1
  bp1 <- b + n1
  ap2 <- a + s2
  bp2 <- b + n2
  # muestras distribución posterior
  th1_mc <- rgamma(n = 10000, shape = ap1, rate = bp1)
  th2_mc <- rgamma(n = 10000, shape = ap2, rate = bp2)
  # inferencia Bayesiana
  est  <- mean(th1_mc - th2_mc)
  cv   <- sd(th1_mc - th2_mc)/mean(th1_mc - th2_mc)
  ic95 <- quantile(x = th1_mc - th2_mc, probs = c(.025,.975))
  ic99 <- quantile(x = th1_mc - th2_mc, probs = c(.005,.995))
  # output
  out <- rbind(out, c(est, cv, ic95, ic99))
}
colnames(out) <- c("Estimación","CV","Q2.5%","Q97.5%","Q0.5%","Q99.5%")
rownames(out) <- paste0(seq(from = 20, to = 65, by = 5), "-", seq(from = 24, to = 69, by = 5))
knitr::kable(x = out[,1:4], digits = 3, align = "c", caption = "Inferencia sobre la diferencia entre las tasas promedio del número de hijos con educación superior y sin educación superior, por grupos quinqueniales de edad.")
```


```{r, echo=F, fig.align='center'}
plot(x = 1:nrow(out), y = out[,1], ylim = c(0,2), pch = 16, cex = 1.1, xaxt = "n", xlab = "Edad en Grupos Quinquenales", ylab = expression(theta[1]-theta[2]))
lines(x = 1:nrow(out), y = out[,1], type = "l", col = 4)
abline(h = 0, col = 1, lty = 2)
abline(v = 1:nrow(out), col = "gray95")
segments(x0 = 1:nrow(out), y0 = out[,3], x1 = 1:nrow(out), y1 = out[,4], lwd = 3)
segments(x0 = 1:nrow(out), y0 = out[,5], x1 = 1:nrow(out), y1 = out[,6], lwd = 1)
axis(side = 1, at = 1:nrow(out), labels = rownames(out), las = 1)
```


```{r, echo=F, fig.align='center'}
plot(x = 1:nrow(out), y = out[,2], ylim = c(0,0.25), pch = 20, cex = 1.1, xaxt = "n", xlab = "Edad en Grupos Quinquenales", ylab = "Coef. Variación")
axis(side = 1, at = 1:nrow(out), labels = rownames(out), las = 1)
abline(v = 1:nrow(out), col = "gray95")
abline(h = 0.05, lty = 2, col = 3)
abline(h = 0.10, lty = 2, col = "#FFA500")
abline(h = 0.15, lty = 2, col = 2)
lines(x = 1:nrow(out), y = out[,2], type = "l", col = 4)
lines(x = 1:nrow(out), y = out[,2], type = "p", pch = 20, cex = 1.1)
```


# Ejercicios {-}

1.  Un laboratorio está estimando la tasa de *tumorigenesis* en dos cepas de ratones, A y B. Los ratones tipo A han sido bien estudiados, tanto así que información de otros laboratorios sugiere que los ratones tipo A tienen conteos de tumores que siguen una distribución de Poisson con media $\theta_\text{A} = 12$. Se desconoce la tasa promedio de los tumores para los ratones tipo B, $\theta_\text{B}$, pero existe suficiente evidencia empírica para asegurar que los ratones tipo B están relacionados con los ratones tipo A. Los conteos de tumores observados para las dos cepas de ratones son
$$
\boldsymbol{y}_\text{A} = (12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
\qquad\text{y}\qquad
\boldsymbol{y}_\text{B} = (11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)\,.
$$

    a. Encuentre las distribuciones posteriores, las medias posteriores, las varianzas posteriores y los intervalos de credibilidad al 95\% para $\theta_\text{A}$ y $\theta_\text{B}$, asumiendo modelos Gamma-Poisson independientes para cada grupo, con distribuciones previas $\theta_\text{A}\sim\textsf{Gamma}(120,10)$ y $\theta_\text{B}\sim\textsf{Gamma}(12,1)$. ¿Por qué estas distribuciones previas son razonables?
    
    b. Grafique la media posterior de $\theta_\text{B}$ bajo la distribución previa $\theta_\text{B}\sim \textsf{Gamma}(12m,m)$, para cada valor de $m\in\{1, 2,\ldots,50\}$. Describa qué tipo de información previa sobre $\theta_\text{B}$ sería necesaria para que la media posterior de $\theta_\text{B}$ sea cercana a la de $\theta_\text{A}$.



2. Un investigador del Departamento de Ingeniería Electrónica y Eléctrica de la Universidad de Bath (Inglaterra) necesitaba analizar unos datos sobre los tiempos de falla de un determinado tipo de alambre de metal (en este problema el tiempo de falla se define como el número de veces que una máquina podría tensionar el alambre antes de romperse). Los siguientes datos corresponden a $n = 14$ tiempos de falla de una parte del experimento:
$$
\boldsymbol{y} = 
(495,541,1461,1555,1603,2201,2750,3468,3516,4319,6622,7728,13159,21194)\,.
$$ 
El modelo más simple para los datos del tiempo de falla involucra la distribución Exponencial,
$y_i \mid \lambda \stackrel{\text{iid}}{\sim} \textsf{Exponencial} ( \lambda )$, con $\textsf{E}(y_i \mid \lambda) = \lambda$, para $i=1,\ldots,n$.

    a. Sea $X \sim \textsf{Gamma} (\alpha, \beta)$, con $\alpha > 0$ y $\beta > 0$. Encuentre la función de densidad de probabilidad de $Y=\frac{ 1 }{ X }$. La distribución de $Y$ se denomina distribución Gamma Inversa con parámetros $\alpha$ y $\beta$, lo que se denota con $Y\sim\textsf{GI} (\alpha,\beta)$.
    b. Encuentre la distribución posterior de $\lambda$, asumiendo que $\lambda\sim\textsf{GI}(a,b)$, con $a > 0$ y $b > 0$.
    c. Muestre que la media posterior de $\lambda$ es un promedio ponderado de la media previa $\textsf{E}(\lambda)$ y la media muestral $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$.
    d. Se tiene información externa de otro experimento de acuerdo con el cual la distribución previa de $\lambda$ debería tener una media $\mu_0 = 4500$ y una desviación estándar $\sigma_0 = 1800$. Grafique las distribuciones previa y posterior de $\lambda$ en el mismo gráfico con $\lambda$ en el eje horizontal tomando valores de 1000 a 12000, identificando qué curva corresponde a qué densidad.
    e. Muestre que el estimador de máxima verosimilitud (MLE, por sus siglas en inglés) de $\lambda$ y la información observada de Fisher son respectivamente 
    $$
    \hat\lambda_{\text{MLE}} = \bar{y}
    \qquad\text{y}\qquad
    \hat{I}(\hat\lambda_{\text{MLE}}) = \frac{n}{\bar{y}^2}\,.
    $$

        Recuerde que la información observada de Fisher se define como
    $$
    \hat{I}(\hat\lambda_{\text{MLE}}) = \left[ -\frac{\partial^2}{\partial\lambda^2}\log p(\boldsymbol{y}\mid\lambda) \right]_{\lambda = \hat\lambda_{\text{MLE}}}\,,
    $$
    donde $p(\boldsymbol{y}\mid\lambda) = \prod_{i=1}^n p(y_i\mid\lambda)$ es la distribución muestral conjunta de $\boldsymbol{y}$.	
	
    f. Encuentre la estimación, el coeficiente de variación y un intervalo de de credibilidad/confianza al 95\% para $\lambda$, bajo el paradigma Bayesiano como Frecuentista (usando la Normalidad asintóica del MLE, Bootstrap paramétrico y Bootstrap no paramétrico).
    
        Recuerde que asintóticamente se tiene que $\hat\lambda_{\text{MLE}}\sim\textsf{N}(\lambda,\hat{I}^{-1}(\hat\lambda_{\text{MLE}}))$.
    



3. El estimador óptimo del parámetro $\theta\in \Theta\subseteq\mathbb{R}$ de acuerdo con la regla de Bayes corresponde al estimador $\hat\theta=\hat\theta(\boldsymbol{y})$ que minimiza la pérdida esperada posterior dada por
$$
\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)) = \int_{\Theta} L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta\,,
$$
donde $L(\theta,\hat\theta)$ es una función de pérdida (costo que conlleva estimar $\theta$ por medio de $\hat\theta$) y $\boldsymbol{y}=(y_1,\ldots, y_n)$ es el conjunto de datos observado.

    a. Muestre que si $L(\theta,\hat\theta) = (\theta - \hat\theta)^2$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la media posterior $\hat\theta=\textsf{E}(\theta\mid\boldsymbol{y})$.
    b. Muestre que si $L(\theta,\hat\theta) = |\theta - \hat\theta|$, entonces el estimador óptimo de acuerdo con la regla de Bayes es la mediana posterior $\hat\theta=(\theta\mid\boldsymbol{y})_{0.5}$.
    c. El riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ se define como el valor medio de la función de pérdida $L(\theta,\hat\theta)$ a través de todos los valores de $\boldsymbol{y} \in \mathcal{Y}$, esto es,
    $$
    R_{\textsf{F}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}\mid\theta}(L(\theta,\hat\theta)) = \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\,\textsf{d}\boldsymbol{y}\,.
    $$

        De otra parte, el riesgo Bayesiano $R_{\textsf{B}}(\theta,\hat\theta)$ se define como el valor medio del riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ previo a través de todos los valores de $\theta\in\Theta$, esto es,
    $$
    R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta)) = \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta\,
    $$

        Muestre que
    $$
    R_{\textsf{B}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}}(\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)))\,,
    $$
    donde $\textsf{E}_{\boldsymbol{y}}(\cdot)$ denota el valor esperado respecto a la distribución marginal $p(\boldsymbol{y})$.



4. Considere el modelo Beta-Binomial $y\mid\theta\sim \textsf{Bin}(n,\theta)$ con $\theta\sim \textsf{Beta}(\sqrt{n}/2,\sqrt{n}/2)$, donde $n$ es conocido.

    a. Sea $\hat{\theta}$ el estimador que minimiza la pérdida esperada posterior de la función de pérdida cuadrática $L(\theta,\hat\theta)=(\theta-\hat\theta)^2$. Muestre que el riesgo frecuentista correspondiente es constante.
    b. Encuentre el riesgo frecuentista del estimador de máxima verosimilitud de $\theta$. 
    c. Compare los riesgos frecuentistas de estos estimadores, para $n=10,50,100$.



5. Considere el modelo Gamma-Poisson $y_i\mid\theta\stackrel{\text{iid}}{\sim}\textsf{Poisson}(\theta)$, para $i = 1,\ldots,n$, con $\theta \sim \textsf{Gamma}(a,b)$, y la función de pérdida cuadrática $L(\theta,\hat\theta)=(\theta-\hat\theta)^2$.

    a. Muestre que el estimador que minimiza la pérdida esperada posterior es $\hat\theta = c_1 + c_2\,\bar{y}$, donde $c_1 > 0$, $c_2 \in (0, 1)$ y $\bar{y}=\frac{1}{n}\sum_{i=1}^n y_i$.
    b. Calcule el riesgo frecuentista de $\hat\theta$ y del estimador de máxima verosimilitud de $\theta$.
    c. Calcule el riesgo Bayesiano de $\hat\theta$.
    d. Suponga que un investigador quiere recolectar una muestra lo suficientemente grande para que el riesgo Bayesiano después del experimento sea la mitad del riesgo Bayesiano antes del experimento. Encuentre ese tamaño de muestra.



6. Sea $L(\theta,\hat\theta) = \omega(\theta)(\theta - \hat\theta)^2$ la función de pérdida cuadrática ponderada, donde $\omega(\theta)$ es una función no negativa. Muestre que el estimador que minimiza la pérdida esperada posterior tiene la forma
$$
\hat\theta=\frac{\textsf{E}(\omega(\theta)\,\theta\mid \boldsymbol{y})}{\textsf{E}(\omega(\theta)\mid \boldsymbol{y})}\,.
$$


#### Ejercicio 1 {-}

a. Estas distribuciones previas son razonables porque están centradas al rededor de 12 unidades dado que
$\textsf{E}(\theta_A) = 120/10 = 12$ y $\textsf{E}(\theta_B) = 12/1 = 12$ (la evidencia empírica indica que los ratones tipo A han sido bien estudiados y tienen media 12 y los ratones tipo B están relacionados con los ratones tipo A). La diferencia radica en el grado de concentración de las densidades a priori al rededor de 12, pues $\textsf{CV}(\theta_A)=1/\sqrt{120}\approx 9\%$ mientras que $\textsf{CV}(\theta_B)=1/\sqrt{12}\approx 29\%$. Esto último concuerda con el estado de información externa al conjunto de datos de acuerdo con el contexto del problema, porque estamos más seguros acerca de la evidencia previa acerca de los ratones tipo A en comparación con los ratones tipo B.

```{r, fig.align='center'}
# gráfico
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,0.75,0))
curve(expr = dgamma(x, shape = 120, rate = 10), from = 0, to = 25, n = 1000, col = 2, lwd = 1, ylim = c(0,0.5), xlab = expression(theta), ylab = "Densidad", main = "Distribución Previa")
curve(expr = dgamma(x, shape = 12, rate = 1), n = 1000, col = 4, lwd = 1, add = TRUE)
abline(v = 12, lty = 2)
legend("topright", legend = c("Tipo A", "Tipo B"), col = c(2, 4), lty = 1, lwd = 2, bty = "n")
```

Así, la distribución posterior en ambos casos es tipo Gamma. En particular, se tiene que las distribuciones posteriores correspondientes son $\theta_A\mid\boldsymbol{y}_A\sim\textsf{Gamma}(237,20)$ y $\theta_B\mid\boldsymbol{y}_B\sim\textsf{Gamma}(125,14)$.

```{r}
# datos
yA <- c(12, 9, 12, 14, 13, 13, 15, 8, 15, 6)
yB <- c(11, 11, 10, 9, 9, 8, 7, 10, 6, 8, 8, 9, 7)
# tamaños
nA <- length(yA)
nB <- length(yB)
# estadistico suficiente
sA <- sum(yA)
sB <- sum(yB)
# previa
aA <- 120
bA <- 10
aB <- 12
bB <- 1
# posterior
apA <- aA + sA
bpA <- bA + nA
print(c(apA, bpA))
apB <- aB + sB
bpB <- bB + nB
print(c(apB, bpB))
```

Por tal motivo las medias posteriores, las varianzas posteriores, y los intervalos de credibilidad correspondientes al 95\% son los que se presentan en la tabla a continuación.

```{r}
# inferencia
tab <- rbind(c(apA/bpA, apA/bpA^2, qgamma(p = c(0.025, 0.975), shape = apA, rate = bpA)), 
             c(apB/bpB, apB/bpB^2, qgamma(p = c(0.025, 0.975), shape = apB, rate = bpB)))
rownames(tab) <- c("Tipo A","Tipo B")
colnames(tab) <- c("Media","Varianza","Q2.5%","Q97.5%")
knitr::kable(x = tab, digits = 3)
```

```{r, fig.align='center'}
# gráfico
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
# tipo A
curve(expr = dgamma(x, shape = apA, rate = bpA), from = 0, to = 25, n = 1000, col = 2, lwd = 2, ylim = c(0,0.5), xlab = expression(theta), ylab = "Densidad", main = "")
curve(expr = dgamma(x, shape = aA,  rate = bA),  n = 1000, col = 2, lwd = 1, lty = 2, add = TRUE)
curve(expr = dgamma(x, shape = apB, rate = bpB), n = 1000, col = 4, lwd = 2, lty = 1, add = TRUE)
curve(expr = dgamma(x, shape = aB,  rate = bB),  n = 1000, col = 4, lwd = 1, lty = 2, add = TRUE)
legend("topright", legend = c("Posterior A","Posterior B","Previa A","Previa B"), col = c(2,4,2,4), lty = c(1,1,2,2), lwd = c(2,3,1,1), bty = "n")
```


b. Como se evidencia en las Figuras, la información previa sobre $\theta_B$ para que $\textsf{E}(\theta_B\mid\boldsymbol{y}_B)$ sea cercana a $\textsf{E}(\theta_A\mid\boldsymbol{y}_A)=11.850$ (línea en color negro) debe ser muy informativa al rededor de $\textsf{E}(\theta_B)=12m/m=12$, lo cual se logra incrementando el valor de $m$, de forma que $\textsf{Var}(\theta_B)=12m/m^2=12/m$ decrezca haciendo que la información previa sea mas fuerte. 


```{r, fig.align='center'}
# gráfico
mgrid <- c(1,8,15,25,50)
col <- RColorBrewer::brewer.pal(n = 9, name = "Set1")
par(mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
plot(NA, NA, xlim = c(6,14), ylim = c(0,1), xlab = expression(theta), ylab = "Densidad posterior", main = "Tipo B")
for (j in 1:length(mgrid)) {
  curve(expr = dgamma(x, shape = 12*mgrid[j] + sB, rate = 1*mgrid[j] + nB),  n = 1000, col = col[j], lwd = 2, lty = 1, add = TRUE)
  abline(v = (12*mgrid[j] + sB)/(1*mgrid[j] + nB), lty = 2,  col = col[j])
}
abline(v = (12 + sA)/(1 + nA), lty = 1)
legend("topright", legend = c(paste0("m = ", mgrid), "Media post. A"), col = c(col[1:length(mgrid)], "black"), lty = 1, lwd = 2, bty = "n")
```


```{r, fig.width=10, fig.height=5, fig.align='center'}
mgrid <- 1:50
mmean <- (12*mgrid + sB)/(1*mgrid + nB)
mvar  <- (12*mgrid + sB)/(1*mgrid + nB)^2
par(mfrow = c(1,2), mar = c(3,3,1.4,1.4), mgp = c(1.75,.75,0))
# media 
plot(x = mgrid, y = mmean, type = "p", pch = 18, cex = 0.8, ylim = c(9, 12), xlab = "m", ylab = "Media posterior", main = "Tipo B")
abline(h = (12 + sA)/(1 + nA), lty = 1)
# CV
plot(x = mgrid, y = sqrt(mvar)/mmean, type = "p", pch = 18, cex = 0.8, xlab = "m", ylab = "CV posterior", main = "Tipo B")
```


#### Ejercicio 3 {-}

a. Diferenciado $\textsf{E}_{\theta\mid\boldsymbol{y}}((\theta-\hat\theta)^2)$ respecto a $\hat\theta$, se tiene que
$$
\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left((\theta-\hat\theta)^2\right)=\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(\theta^2-2\theta\hat\theta+\hat{\theta}^2\right) = -2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta) + 2\hat\theta\,
$$
y por lo tanto el punto crítico correspondiente es $\hat\theta = \textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)$. Así, por el criterio de la segunda derivada, como
$$
\frac{\textsf{d}^2}{\textsf{d}\hat\theta^2}\textsf{E}_{\theta\mid\boldsymbol{y}}\left((\theta-\hat\theta)^2\right)= 2 > 0
$$
para todo $\hat\theta$, entonces en efecto la media posterior $\hat\theta = \textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)$ minimiza $\textsf{E}_{\theta\mid\boldsymbol{y}}((\theta-\hat\theta)^2)$.

b. Diferenciado $\textsf{E}_{\theta\mid\boldsymbol{y}}(|\theta-\hat\theta|)$ respecto a $\hat\theta$ aplicando la regla de Leibniz (e.g., https://mathworld.wolfram.com/LeibnizIntegralRule.html), se tiene que
$$
\frac{\textsf{d}}{\textsf{d}\hat\theta}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(|\theta-\hat\theta|\right)=\frac{\textsf{d}}{\textsf{d}\hat\theta}\int_\Theta |\theta-\hat\theta|\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \frac{\textsf{d}}{\textsf{d}\hat\theta}\left( \int_{-\infty}^{\hat\theta} (\hat\theta-\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta + \int_{\hat\theta}^\infty (\theta-\hat\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta \right)
= \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta - \int_{\hat\theta}^\infty \,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta\,,
$$
y por lo tanto, el punto crítico satisface que
$$
 \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \int_{\hat\theta}^\infty \hat\theta\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
 \quad\text{de donde}\quad
 2\int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = \int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta+\int_{\hat\theta}^\infty p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta = 1\,,
$$
esto es, $\int_{-\infty}^{\hat\theta}p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta =1/2$. Luego, el punto crítico correspondiente es $\hat\theta = (\theta\mid\boldsymbol{y})_{0.5}$.  Así, por el criterio de la segunda derivada, como
$$
\frac{\textsf{d}^2}{\textsf{d}\hat\theta^2}\textsf{E}_{\theta\mid\boldsymbol{y}}\left(|\theta-\hat\theta|\right)=  2 > 0    
$$
para todo $\hat\theta$, entonces en efecto la mediana posterior $\hat\theta = (\theta\mid\boldsymbol{y})_{0.5}$ minimiza $\textsf{E}_{\theta\mid\boldsymbol{y}}(|\theta-\hat\theta|)$.

c. Se tiene que
$$
\begin{align*}
R_{\textsf{B}}(\theta,\hat\theta) &= \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\,\textsf{d}\boldsymbol{y} \right)\, p(\theta)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\boldsymbol{y}\mid\theta)\, p(\theta)\,\textsf{d}\boldsymbol{y} \right)\,\textsf{d}\theta \\
&= \int_{\Theta} \left( \int_{\mathcal{Y}} L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \right)\,\textsf{d}\theta \\
&= \int_{\mathcal{Y}} \left( \int_{\Theta}  L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, p(\boldsymbol{y})\, \textsf{d}\theta \right)\textsf{d}\boldsymbol{y} \\
&= \int_{\mathcal{Y}} \left( \int_{\Theta}  L(\theta,\hat\theta)\, p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta \right)\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \\
&= \int_{\mathcal{Y}} \textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta))\, p(\boldsymbol{y})\,\textsf{d}\boldsymbol{y} \\
&= \textsf{E}_{\boldsymbol{y}}(\textsf{E}_{\theta\mid\boldsymbol{y}}(L(\theta,\hat\theta)))\,.
\end{align*}
$$

#### Ejercicio 4 {-}


a. La distribución posterior en este caso es
$$
\begin{align*}
    p(\theta\mid x) &= \frac{p(x\mid\theta)\,p(\theta)}{\int_\Theta p(x\mid\theta)\,p(\theta)\, \textsf{d}\theta}\\
    &= \frac{\binom{n}{x} \theta^x (1-\theta)^{n-x}\frac{1}{B(\sqrt{n}/2,\sqrt{n}/2)} \theta^{\sqrt{n}/2-1} (1-\theta)^{\sqrt{n}/2-1}}{\int_\Theta \binom{n}{x} \theta^x (1-\theta)^{n-x}\frac{1}{B(\sqrt{n}/2,\sqrt{n}/2)} \theta^{\sqrt{n}/2-1} (1-\theta)^{\sqrt{n}/2-1} \, \textsf{d}\theta}\\
    &= \frac{\theta^{\sqrt{n}/2 + x-1} (1-\theta)^{\sqrt{n}/2 + n-x-1}}{\int_\Theta \theta^{\sqrt{n}/2 + x-1} (1-\theta)^{\sqrt{n}/2 + n-x-1} \, \textsf{d}\theta}\\
    &= \frac{\theta^{\sqrt{n}/2 + x-1} (1-\theta)^{n-x+\sqrt{n}/2-1}}{\textsf{B}(\sqrt{n}/2 + x,\sqrt{n}/2 + n-x)}\\
\end{align*}
$$
y por lo tanto, $\theta\mid x \sim \textsf{Beta}(\sqrt{n}/2 + x,\sqrt{n}/2 + n-x)$.

    Si la función de pérdida es $L(\theta,\hat\theta)=(\theta-\hat\theta)^2$, i.e., la función de pérdida cuadrática, entonces el estimador que minimiza la función de pérdida esperada a posteriori es la media posterior. Así, se tiene que
$$
\hat{\theta}=\textsf{E}(\theta\mid x)=\frac{x+\sqrt{n}/2}{(x+\sqrt{n}/2)+(n-x+\sqrt{n}/2)} =
\frac{x+\sqrt{n}/2}{\sqrt{n}+ n} = \frac{\frac{x}{\sqrt{n}}+\frac12}{1 + \sqrt{n}}\,.
$$

    Ahora, si la función de pérdida es la función de pérdida cuadrática, entonces el riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ es
$$
R_{\textsf{F}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}\mid\theta}(L(\theta,\hat\theta)) = \textsf{E}_{\boldsymbol{y}\mid\theta}((\theta-\hat\theta)^2)
$$
el cual corresponde al error cuadrático medio (MSE, por sus siglas en inglés) evaluado en $\hat\theta$, i.e,
$$
\begin{align*}
R_{\textsf{F}}(\theta,\hat\theta) &= \textsf{MSE}(\hat\theta)\\
&= \textsf{Var}_{x\mid\theta}(\hat\theta) + \left(\textsf{E}_{x\mid\theta}(\hat\theta) - \theta\right)^2\\
&= \textsf{Var}_{x\mid\theta}\left(\frac{\frac{x}{\sqrt{n}}+\frac12}{1 + \sqrt{n}}\right) + \left(\textsf{E}_{x\mid\theta}\left(\frac{\frac{x}{\sqrt{n}}+\frac12}{1 + \sqrt{n}}\right) - \theta\right)^2\\
&= \left(\frac{\frac{1}{\sqrt{n}}}{1 + \sqrt{n}}\right)^2\textsf{Var}_{x\mid\theta}(x)+ \left(\frac{\frac{\textsf{E}_{x\mid\theta}(x)}{\sqrt{n}}+\frac12}{1 + \sqrt{n}} - \theta\right)^2\\
&= \frac{1}{n(1 + \sqrt{n})^2}\,n\theta(1-\theta) + \left(\frac{\frac{n\theta}{\sqrt{n}}+\frac12}{1 + \sqrt{n}}-\theta\right)^2\\ 
&=\frac{\theta(1-\theta)}{(1 + \sqrt{n})^2} + \left(\frac{\sqrt{n}\theta+\frac12 -\theta - \sqrt{n}\theta}{1 + \sqrt{n}}\right)^2\\
&=\frac{\theta(1-\theta)}{(1 + \sqrt{n})^2} + \frac{(\frac12 -\theta)^2}{(1 + \sqrt{n})^2}\\
&=\frac{\theta -\theta^2+\frac14 -\theta + \theta^2}{(1 + \sqrt{n})^2}\\
&=\frac{1}{4(1 + \sqrt{n})^2}\,,
\end{align*}
$$
lo cual resulta ser una cantidad constante.

b. En este caso, se puede demostrar que el estimador de máxima verosimilitud de $\theta$ es la media muestral $\theta_0=x/n$ (¡ejercicio!). Así, el riesgo de $\theta_0$ es
$$
\begin{align*}
        R_{\textsf{F}}(\theta,\theta_0)&= \textsf{MSE}(\theta_0)\\
        &= \textsf{Var}_{x\mid\theta}(\theta_0) + \left(\textsf{E}_{x\mid\theta}(\theta_0) - \theta\right)^2\\
        &= \textsf{Var}_{x\mid\theta}(x/n) + \left(\textsf{E}_{x\mid\theta}(x/n) - \theta\right)^2\\
        &= \frac{1}{n^2}\textsf{Var}_{x\mid\theta}(x)+ \left(\frac{\textsf{E}_{x\mid\theta}(x)}{n}-\theta\right)^2\\
        &= \frac{n\theta(1-\theta)}{n^2}+ \left[\frac{n\theta}{n}-\theta\right]^2\\
        &= \frac{\theta(1-\theta)}{n}\,.
\end{align*}
$$

c. Los gráficos de los riesgos de $\hat\theta$ y $\theta_0$ para $n=10,50,100$ muestran que ninguno de los estimadores domina al otro para todos los valores de $\theta\in\Theta$.  

    Se observa que $\hat\theta$ domina $\theta_0$ cuando:

    - $0.174<\theta<0.825$ para $n=10$.
    - $0.258<\theta<0.741$ para $n=50$.
    - $0.291<\theta<0.708$ para $n=100$.

    Un patrón claro consiste en que a medida que le tamaño de muestra se incrementa:

    - El riesgo de ambos estimadores decrece.
    - El intervalo donde $\hat\theta$ domina a $\theta_0$ se reduce.
    - El riesgo de $\theta_0$ donde $\theta_0$ es dominado por $\hat\theta$ se aproxima al riesgo de $\hat\theta$.

```{r, fig.align='center'}
# r.1  = riesgo de theta^
# r.12 = riesgo de theta0
r.1  <- function(theta, n) theta^0/(4*(1+sqrt(n))^2)
r.2  <- function(theta, n) theta*(1-theta)/n
grid <- seq(from = 0.0, to = 1.0, length = 1000)
# n = 10
fun <- function(x) r.1(x, n=10) - r.2(x, n=10)
uni1 <- uniroot(fun, c(0.0, 0.5))$root
uni2 <- uniroot(fun, c(0.5, 1.0))$root
plot(grid, r.2(theta=grid, n=10), type = 'l', cex.axis=0.7, xlab = expression(theta), ylab = 'Riesgo', lwd = 1, las=1, col="red", main="n=10", ylim=c(0,0.025))
lines(grid, r.1(theta=grid, n=10), lwd = 1, col="blue")
legend("bottom", c(expression(hat(theta)),expression(theta[0])), col = c("blue", "red"), lty=1, lwd=2, bty="n")	
abline(v = uni1, lty = 3)
abline(v = uni2, lty = 3)
points(uni1, r.1(uni1, n=10), pch = 16, cex = 0.8)
points(uni2, r.1(uni2, n=10), pch = 16, cex = 0.8)
text( 0.3, 0.013, '(0.174;0.014)', cex = 1.0 )
text( 0.7, 0.013, '(0.825;0.014)', cex = 1.0 )
segments(uni1, y0=0.01, x1 = uni2, lwd=3)
text( 0.5, 0.008, '0.650', cex = 1.0)
```

```{r, fig.align='center'}
# n = 50
fun <- function(x) r.1(x, n=50) - r.2(x, n=50)
uni1 <- uniroot(fun, c(0.0, 0.5))$root
uni2 <- uniroot(fun, c(0.5, 1.0))$root
plot(grid, r.2(theta=grid, n=50), type = 'l', cex.axis=0.7, xlab = expression(theta), ylab = 'Riesgo', lwd = 1, las=1, col="red", main="n=50", ylim=c(0,0.025))
lines(grid, r.1(theta=grid, n=50), lwd = 1, col="blue")
legend("top", c(expression(hat(theta)),expression(theta[0])), col = c("blue", "red"), lty=1, lwd=2, bty="n")	
abline(v = uni1, lty = 3)
abline(v = uni2, lty = 3)
points(uni1, r.1(uni1, n=50), pch = 16, cex = 0.8)
points(uni2, r.1(uni2, n=50), pch = 16, cex = 0.8)
text( 0.1, 0.006, '(0.258;0.003)', cex = 1.0 )
text( 0.9, 0.006, '(0.741;0.003)', cex = 1.0 )
segments(uni1, y0=0.01, x1 = uni2, lwd=3)
text( 0.5, 0.008, '0.482', cex = 1.0)
```

```{r, fig.align='center'}
# n = 100
fun <- function(x) r.1(x, n=100) - r.2(x, n=100)
uni1 <- uniroot(fun, c(0.0, 0.5))$root
uni2 <- uniroot(fun, c(0.5, 1.0))$root
plot(grid, r.2(theta=grid, n=100), type = 'l', cex.axis=0.7, xlab = expression(theta), ylab = 'Riesgo', lwd = 1, las=1, col="red", main="n=100", ylim=c(0,0.025))
lines(grid, r.1(theta=grid, n=100), lwd = 1, col="blue")
legend("top",  c(expression(hat(theta)),expression(theta[0])), col = c("blue", "red"), lty=1, lwd=2, bty="n")	
abline(v = uni1, lty = 3)
abline(v = uni2, lty = 3)
points(uni1, r.1(uni1, n=100), pch = 16, cex = 0.8)
points(uni2, r.1(uni2, n=100), pch = 16, cex = 0.8)
text( 0.1, 0.004, '(0.291;0.002)', cex = 1.0 )
text( 0.9, 0.004, '(0.708;0.002)', cex = 1.0 )
segments(uni1, y0=0.01, x1 = uni2, lwd=3)
text( 0.5, 0.008, '0.416', cex = 1.0)
```


#### Ejercicio 5 {-}

a. En este caso, la distribución posterior es
$$
\begin{align*}
    p(\theta\mid\boldsymbol{y}) &\propto p(\boldsymbol{y}\mid\theta)\,p(\theta)\\
    &\propto \prod_{i=1}^n \frac{e^{-\theta}\theta^{y_i}}{y_i!} \times \frac{\beta^\alpha}{\Gamma(\alpha)}\,\theta^{\alpha-1}\, e^{-\beta\theta}\\
    &\propto \theta^{\alpha+s -1}\,e^{-(\beta+n)\theta}\,,
\end{align*}
$$
donde $s=\sum_{i=1}^n y_i$. Entonces, $\theta\mid \boldsymbol{y}\sim \textsf{Gamma}(\alpha+s,\beta+n)$. Si la función de pérdida es la función de pérdida cuadrática, entonces el estimador que minimiza la pérdida posterior esperada es la media posterior. Así, se tiene que
$$
\hat{\theta}=\hat{\theta}(\boldsymbol{y})=\textsf{E}(\theta\mid\boldsymbol{y})=\frac{\alpha+s}{\beta+n} =\frac{\alpha}{\beta+n}+\frac{n}{\beta+n}\frac{s}{n}\,,
$$
y por lo tanto, $\hat{\theta}=a+b\bar{y}$, donde $a=\alpha/(\beta+n)$, $b=n/(\beta+n)$, y $\bar{y}=\frac{s}{n}$. Como $\alpha,\beta,n>0$, entonces $a>0$ y $b\in (0,1)$.

b. El riesgo frecuentista $R_{\textsf{F}}(\theta,\hat\theta)$ es
$$
R_{\textsf{F}}(\theta,\hat\theta) = \textsf{E}_{\boldsymbol{y}\mid\theta}(L(\theta,\hat\theta)) = \textsf{E}_{\boldsymbol{y}\mid\theta}((\theta-\hat\theta)^2)\,,
$$
el cual corresponde al error cuadrático medio (MSE, por sus siglas en inglés) evaluado en $\hat\theta$, i.e,
$$
\begin{align*}
R_{\textsf{F}}(\theta,\hat\theta) &= \textsf{MSE}(\hat\theta)\\
&= \textsf{Var}_{\boldsymbol{y}\mid\theta}(\hat\theta) + \left(\textsf{E}_{\boldsymbol{y}\mid\theta}(\hat\theta) - \theta\right)^2\\
&= \textsf{Var}_{\boldsymbol{y}\mid\theta}(a+b\bar{y}) + \left(\textsf{E}_{\boldsymbol{y}\mid\theta}(a+b\bar{y}) - \theta\right)^2\\
&= b^2\textsf{Var}_{\boldsymbol{y}\mid\theta}(\bar{y})+ \left(a+b\textsf{E}_{\boldsymbol{y}\mid\theta}(\bar{y}) - \theta\right)^2\\
&= b^2\frac{n\theta}{n^2}+ \left(a+b\frac{n\theta}{n} - \theta\right)^2\\
&= b^2\frac{\theta}{n}+ \left(a+(b-1)\theta \right)^2\\
&= (b-1)^2\theta^2 + \left(\frac{b^2}{n}+2a(b-1)\right)\theta + a^2\\
&=c_1\theta^2 + c_2\theta + c_3
\end{align*}
$$
donde $c_1=(b-1)^2$, $c_2= \frac{b^2}{n}+2a(b-1)$, y $c_3=a^2$. Después de algo de álgebra, se obtiene que
$$
\begin{align*}
&c_1=(b-1)^2=\left(\frac{n}{\beta + n}-1\right)^2\quad\Rightarrow\quad c_1=\frac{\beta^2}{(\beta+n)^2}\,,\\ 
&c_2=\frac{b^2}{n}+2a(b-1)=\frac{\left(\frac{n}{\beta+n}\right)^2}{n}+2\frac{\alpha}{\beta+n}\left(\frac{n}{\beta+n}-1\right)
\quad\Rightarrow\quad c_2=\frac{n-2\alpha\beta}{(\beta+n)^2}\,,\\
&c_3=\left(\frac{\alpha}{\beta+n}\right)^2\quad\Rightarrow\quad c_3=\frac{\alpha^2}{(\beta+n)^2}\,.
\end{align*}
$$
De otra parte, se demuestra que el estimador de máxima verosimilitud (MLE, por sus siglas en inglés) es  $\hat{ \theta }_{ \text{MLE} } = \bar{y}$ (¡ejercicio!). Así, se tiene que le riesgo frecuentista de $\hat{ \theta }_{ \text{MLE} }$ es
$$
\begin{align*}
R_{\textsf{F}}(\theta,\hat{ \theta }_{ \text{MLE} }) &= \textsf{MSE}(\hat{ \theta }_{ \text{MLE} })\\
&= \textsf{Var}_{\boldsymbol{y}\mid\theta}(\hat{ \theta }_{ \text{MLE} }) + \left(\textsf{E}_{\boldsymbol{y}\mid\theta}(\hat{ \theta }_{ \text{MLE} }) - \theta\right)^2\\
&= \textsf{Var}_{\boldsymbol{y}\mid\theta}(\bar{y}) + \left(\textsf{E}_{\boldsymbol{y}\mid\theta}(\bar{y}) - \theta\right)^2\\
&= \frac{n\theta}{n^2}+ \left(\frac{n\theta}{n} - \theta\right)^2\\
&= \frac{\theta}{n}\,.
\end{align*}
$$

c. El riesgo Bayesiano de $\hat\theta$ es
$$
\begin{align*}
    R_{\textsf{B}}(\theta,\hat\theta) &= \textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta))\\
    &= \int_{\Theta} R_{\textsf{F}}(\theta,\hat\theta)\, p(\theta)\,\textsf{d}\theta\,,\\
    &= \int_{\Theta} (c_1\theta^2 + c_2\theta + c_3)\,p(\theta) \, \textsf{d}\theta\\
    &= c_1 \int_{\Theta} \theta^2\, p(\theta) \, \textsf{d}\theta
    +  c_2 \int_{\Theta} \theta \, p(\theta) \, \textsf{d}\theta
    +  c_3 \int_{\Theta} p(\theta) \, \textsf{d}\theta\\
    &=c_1\textsf{E}_\theta(\theta^2) + c_2\textsf{E}_\theta(\theta)+c_3\\
    &=c_1\left(\textsf{Var}_\theta(\theta)+(\textsf{E}_\theta(\theta))^2\right) + c_2\textsf{E}_\theta(\theta)+c_3\\
    &=c_1\left(\frac{\alpha}{\beta^2}+\frac{\alpha^2}{\beta^2}\right) + c_2\frac{\alpha}{\beta}+c_3\\
    &=c_1\frac{\alpha(1-\alpha)}{\beta^2}+c_2\frac{\alpha}{\beta}+c_3\,,
\end{align*}
$$
donde
$$
\begin{align*}
      c_1=\frac{\beta^2}{(\beta+n)^2},\quad
      c_2=\frac{n-2\alpha\beta}{(\beta+n)^2},\quad\text{y}\quad
      c_3=\frac{\alpha^2}{(\beta+n)^2}\,.
\end{align*}
$$

d. Se necesita un tamaño de muestra tal que
$$
\textsf{E}_{\theta\mid\boldsymbol{y}}(R_{\textsf{F}}(\theta,\hat\theta))=\frac12\,\textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta)).
$$
Ya se sabe que el riesgo Bayesiano antes del experimento es de la forma $\textsf{E}_{\theta}(R_{\textsf{F}}(\theta,\hat\theta))=c_1\frac{\alpha(1-\alpha)}{\beta^2}+c_2\frac{\alpha}{\beta}+c_3$. 

    Siguiendo la misma idea, se tiene que el riesgo Bayesiano de $\hat\theta$ después del experimento es
    $$
    \begin{align*}
       \textsf{E}_{\theta\mid\boldsymbol{y}}(R_{\textsf{F}}(\theta,\hat\theta))= &=\int_\Theta R_{\textsf{F}}(\theta,\hat\theta)\,p(\theta\mid\boldsymbol{y})\, \textsf{d}\theta\\
       &= \int_\Theta (c_1\theta^2 + c_2\theta + c_3) p(\theta\mid\boldsymbol{y}) \, \textsf{d}\theta\\
       &= c_1 \int_\Theta \theta^2\, p(\theta\mid\boldsymbol{y}) \, \textsf{d}\theta
        +  c_2 \int_\Theta \theta \, p(\theta\mid\boldsymbol{y}) \, \textsf{d}\theta
        +  c_3 \int_\Theta \pi(\theta\mid\boldsymbol{y}) \, d\theta\\
        &=c_1\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta^2) + c_2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)+c_3\\
        &=c_1\left(\textsf{Var}_{\theta\mid\boldsymbol{y}}(\theta) + (\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta))^2\right) + c_2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)+c_3\\
        &=c_1\left(\frac{\alpha+s }{(\beta + n)^2}+\frac{\left(\alpha+s\right)^2}{(\beta+n)^2}\right) + c_2\frac{\alpha+s}{\beta+n}+c_3\\
        &=c_1\frac{\left(\alpha+s\right)\left(1-\left(\alpha+s\right)\right)}{(\beta + n)^2}+c_2\frac{\alpha}{\beta + n}+c_3\,,
    \end{align*}
    $$
    donde
    $$
    \begin{align*}
          c_1=\frac{\beta^2}{(\beta+n)^2},\quad
          c_2=\frac{n-2\alpha\beta}{(\beta+n)^2},\quad\text{y}\quad
          c_3=\frac{\alpha^2}{(\beta+n)^2}\,.
    \end{align*}
    $$
    Entonces, se requiere el tamaño de la muestra $n$ que satisfaga la ecuación
    $$
      2c_1\frac{\left(\alpha+s\right)\left(1-\left(\alpha+s\right)\right)}{(\beta + n)^2}+2c_2\frac{\alpha}{\beta + n}+2c_3
      =
      c^*_1\frac{\alpha(1-\alpha)}{\beta^2}+c^*_2\frac{\alpha}{\beta}+c^*_3\,,
    $$
    donde
    $$
    \begin{align*}
          c^*_1=\frac{\beta^2}{(\beta+0)^2}=1,\quad
          c^*_2=\frac{0-2\alpha\beta}{(\beta+0)^2}=-2\frac{\alpha}{\beta},\quad\text{y}\quad
          c^*_3=\frac{\alpha^2}{(\beta+0)^2}=\frac{\alpha^2}{\beta^2}\,.
    \end{align*}
    $$

    Se observa que los hiperparámetros $\alpha$ y $\beta$ son cantidades conocidas, y por lo tanto, las constantes $c_k$ y $c_k^*$, para $k=1,2,3$, también lo son. Así mismo, el estadístico suficiente $s=\sum_{i=1}^n y_i$ también es una cantidad conocida después de que el experimento tenga lugar. En consecuencia, en la ecuación anterior la única cantidad desconocida es el tamaño de muestra $n$.


#### Ejercicio 6 {-}

Se tiene que
$$
\begin{align*}
    \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)(\theta - \hat\theta)^2) &= \int_\Theta \omega(\theta)(\theta-\hat\theta)^2\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta\\
    &=\int_\Theta\omega(\theta)(\theta-\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)+\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)^2\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta\\
    &=\int_\Theta\omega(\theta)(\theta-\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta))^2\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta
    +2\int_\Theta\omega(\theta)(\theta-\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta))(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
    +\int_\Theta\omega(\theta)(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)^2\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta\,.
\end{align*}
$$

La primera integral es constante respecto a $\hat\theta$ y no afecta el proceso de minimización de respecto a $\hat\theta$. De otra parte, la segunda integral es
$$
\begin{align*}
    2(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)\int_\Theta\omega(\theta)(\theta-\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta))\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
    &=2(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)\left(
    \int_\Theta\omega(\theta)\,\theta\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
    -\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\int_\Theta\omega(\theta)\,p(\theta\mid\boldsymbol{y})\,\textsf{d}\theta
    \right)\\
    &=2(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)\left(
    \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta)
    -\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\,\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))
    \right)\,.
\end{align*}
$$
De esta forma, tomando la primera derivada de $\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)(\theta - \hat\theta)^2)$ respecto a $\hat\theta$, se tiene que
$$
\begin{align*}
    \frac{\textsf{d}}{\textsf{d}\hat\theta}\,\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)(\theta - \hat\theta)^2)
    &=-2\left( \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) - \textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)) \right)
    +\frac{\textsf{d}}{\textsf{d}\hat\theta}\int_\Theta\omega(\theta)(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)^2\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta\\
    &=-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))
    +\frac{\textsf{d}}{\textsf{d}\hat\theta}(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)^2\int_\Theta\omega(\theta)\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta\\
    &=-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))
    -2(\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)-\hat\theta)\int_\Theta\omega(\theta)\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta\\
    &=-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))
    -2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\int_\Theta\omega(\theta)\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta+2\hat\theta\int_\Theta\omega(\theta)\,p(\theta\mid \boldsymbol{y})\,\textsf{d}\theta\\
    &=-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))
    -2\textsf{E}_{\theta\mid\boldsymbol{y}}(\theta)\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))+2\hat\theta\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))\\
    &=-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\hat\theta\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))\,.
  \end{align*}
$$

Igualando esta derivada a cero y despejando para $\hat\theta$ con el fin de hallar el punto crítico correspondiente, se tiene que
$$
-2 \textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta) +2\hat\theta\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)) = 0\,,
$$
y en consecuencia, de $\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)(\theta - \hat\theta)^2)$ respecto a $\hat\theta$ es
$$
\hat\theta = \frac{\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)\,\theta)}{\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta))}\,.
$$

Además, la segunda derivada 
$$
 \frac{\textsf{d}^2}{\textsf{d}\hat\theta^2}\,\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)(\theta - \hat\theta)^2)=2\textsf{E}_{\theta\mid\boldsymbol{y}}(\omega(\theta)) >0\,,
$$
para todo $\hat\theta$ (pues $\omega(\theta)$ es una función no negativa), lo que significa que el punto crítico correspondiente en efecto minimiza la función objetivo.




# Referencias {-}


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```