---
title: "Modelo Normal"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Ejemplo de motivación

La base de datos contiene la información de una **muestra aleatoria simple** de los **estudiantes que presentaron la Prueba Saber 11 en 2023-2**. 

La **prueba de matemáticas** está diseñada con una **escala de 0 a 100** (sin decimales), un **puntaje promedio de 50** y una **desviación estándar de 10 puntos**.

¿Existe suficiente evidencia para concluir que el puntaje promedio de Matemáticas de Bogotá es significativamente superior al puntaje promedio preestablecido por la prueba?

¿Qué ocurre en el caso de Vichada?

Los datos son públicos y están disponibles en este [enlace](https://www.icfes.gov.co/data-icfes/). 


# Modelo

El modelo Normal para **variables continuas** $y_i\in \mathbb{R}$, para $i = 1,\ldots,n$, está dado por
$$
\begin{align*}
	y_i\mid\theta,\sigma^2 &\stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2)\\
	(\theta,\sigma^2) &\sim p(\theta,\sigma^2) 
\end{align*}
$$

El modelo tiene $k=2$ parámetros desconocidos, a saber, la **media** $\theta$ (parámetro de localización) y la **varianza** $\sigma^2$ (cuadrado del parámetro de escala). Por lo tanto, $\boldsymbol{\theta}=(\theta,\sigma^2)\in\Theta = \mathbb{R}\times\mathbb{R}^+$.


# Estadístico suficiente

Si $y_i\mid\theta,\sigma^2\stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2)$, para $i=1,\ldots,n$, entonces la **distribución muestral conjunta** de las observaciones es:
$$
p\left(\boldsymbol{y} \mid \theta, \sigma^{2}\right)=\prod_{i=1}^n \left(2 \pi \sigma^{2}\right)^{-1 / 2} \exp{ \left\{-\frac{1}{2} \left(\frac{y_{i}-\theta}{\sigma}\right)^{2}\right\} } = \left(2 \pi \sigma^{2}\right)^{-n / 2} \exp { \left\{-\frac{1}{2} \sum_{i=1}^{n}\left(\frac{y_{i}-\theta}{\sigma}\right)^{2}\right\} }\,,
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$.

El **núcleo** de esta distribución se puede escribir como:
$$
\sum_{i=1}^n\left(\frac{y_{i}-\theta}{\sigma}\right)^{2}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n} y_{i}^{2}-2 \frac{\theta}{\sigma^{2}} \sum_{i=1}^{n} y_{i}+n \frac{\theta^{2}}{\sigma^{2}}\,,
$$
lo cual sugiere que
$$
\left(\sum_{i=1}^{n} y_{i}, \sum_{i=1}^{n} y_{i}^{2}\right)
$$
es un **estadístico suficiente** para $(\theta,\sigma^2)$. 

La media muestral y la varianza muestral $(\bar{y},s^2)$,
$$
\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i\qquad\text{y}\qquad s^2 =\frac{1}{n-1}\left(\sum_{i=1}^n y_i^2 - n\bar{y}^2\right)\,,
$$ 
también constituye un **estadístico suficiente** para $(\theta,\sigma^2)$, dado que
$$
\sum_{i=1}^n(y_i - \theta)^2 = (n-1)s^2 + n(\bar{y} - \theta)^2\,.
$$


# Modelo Normal-Gamma Inversa-Normal

El **modelo Normal-Gamma Inversa-Normal** es
$$
\begin{align*}
	y_i\mid\theta,\sigma^2 &\stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2) \\
	\theta \mid \sigma^{2} & \sim \textsf{N}\left(\mu_{0}, \tfrac{\sigma^{2}}{\kappa_{0}}\right) \\
	\sigma^{2} & \sim \textsf{GI}\left(\tfrac{\nu_{0}}{2}, \tfrac{\nu_{0}\,\sigma_{0}^{2}}{2}\right)
\end{align*}
$$
donde $\mu_0$, $\kappa_0$, $\nu_0$ y $\sigma^2_0$ son los hiperparámetros del modelo.

Esta parametrización de la previa es muy conveniente para establecer una **interpretación práctica** de los hiperparámetros.

**(Definición.)** La variable aleatoria $X$ tiene distribución **Gamma-Inversa** con parámetros $\alpha,\beta > 0$, i.e., $X\mid\alpha,\beta\sim\textsf{GI}(\alpha,\beta)$, si su función de densidad de probabilidad es
$$
p(x\mid\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\,x^{-(\alpha+1)}\,e^{-\beta/x}\,,\quad x>0\,.
$$

Si $X\sim\textsf{Gamma}(\alpha,\beta)$, entonces $\tfrac{1}{X}\sim\textsf{GI}(\alpha,\beta)$.

     
## Distribución posterior {-}

Bajo el **modelo Normal-Gamma Inversa-Normal** se tiene que la **distribución posterior** de $\boldsymbol{\theta}$ es
$$
p(\theta,\sigma^2\mid \boldsymbol{y}) = p(\theta\mid \sigma^2, \boldsymbol{y})\,p(\sigma^2\mid \boldsymbol{y})
$$
donde:

- $p(\theta\mid \sigma^2, \boldsymbol{y}) = \textsf{N}\left(\theta\mid\mu_{n}, \frac{\sigma^{2}}{\kappa_{n}}\right)$, donde
	$$
	\mu_n = \frac{\kappa_0}{\kappa_n}\mu_0 + \frac{n}{\kappa_n}\bar{y} \qquad\text{con}\qquad \kappa_n = \kappa_0+n\,,
	$$
	dado que 
	$$
	p(\theta\mid\sigma^2,\boldsymbol{y})\propto p(\boldsymbol{y}\mid\theta,\sigma^2)\,p(\theta\mid\sigma^2)\,.
	$$
- $p(\sigma^2\mid \boldsymbol{y}) =  \textsf{GI}\left(\sigma^2\mid\frac{\nu_n}{2},\frac{\nu_n\,\sigma^2_n}{2}\right)$, donde
	$$
	\nu_n = \nu_0+n\qquad\text{y}\qquad \nu_{n}\sigma_{n}^{2} = \nu_{0} \sigma_{0}^{2}+(n-1) s^{2}+n \frac{\kappa_{0}}{\kappa_{n}}\left(\bar{y}-\mu_{0}\right)^2\,,
	$$
	dado que 
	$$
	p(\sigma^2\mid\boldsymbol{y}) \propto p(\sigma^2)\,p(\boldsymbol{y}\mid \sigma^2) = p(\sigma^2)\int_{-\infty}^\infty p(\boldsymbol{y}\mid\theta,\sigma^2)\,p(\theta\mid\sigma^2)\,\text{d}\theta\,.
	$$

Bajo esta parametrización de la distribución previa, se puede interpretar que:

- $\mu_0$ corresponde a la media previa de $\kappa_0$ observaciones previas.
- $\sigma^2_0$ corresponde a la varianza previa de $\nu_0$ observaciones previas.
- $(n-1) s^{2}$, $\nu_{0} \sigma_{0}^{2}$ y $\nu_{n} \sigma_{n}^{2}$ son las sumas de cuadrados muestral, previa, y posterior, respectivamente.
- ¿Qué ocurre si $\kappa_0\rightarrow 0$ y $\nu_0\rightarrow 0$?


## Distribuciones marginales {-}

Las **distribuciones marginales posteriores** de $\theta$ y $\sigma^2$ son respectivamente:
$$
\theta\mid \boldsymbol{y}   \sim  \textsf{t}_{\nu_n}\left( \mu_n,\tfrac{\sigma^2_n}{\kappa_n} \right)
\qquad\text{y}\qquad
\sigma^2\mid \boldsymbol{y} \sim  \textsf{GI}\left(\tfrac{\nu_n}{2},\tfrac{\nu_n\,\sigma^2_n}{2}\right)\,,
$$
donde $\textsf{t}_\nu(\mu,\sigma^2)$ es la distribución $\textsf{t}$ con $\nu$ grados de libertad, media $\mu$, y varianza $\tfrac{\nu}{\nu-2}\,\sigma^2$.

**(Definición.)** La variable aleatoria $X$ tiene distribución **t** con $\nu\in\mathbb{N}_0$ (grados de libertad), $\mu\in\mathbb{R}$ (localización), y $\sigma\in\mathbb{R}^+$ (escala), i.e., $X\mid\nu,\mu,\sigma^2\sim\textsf{t}_\nu(\mu,\sigma^2)$, si su función de densidad de probabilidad es
$$
p(x\mid\nu,\mu,\sigma^2) = \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)\sqrt{\nu\pi\sigma^2}}\,\left( 1 + \tfrac{1}{\nu\sigma^2}\,( x-\mu )^2 \right)^{-(\nu + 1 )/2} \,,\quad x>0\,.
$$

Si $X\sim\textsf{t}_\nu(\mu,\sigma^2)$, entonces $\tfrac{X-\mu}{\sigma}\sim\textsf{t}_\nu$.


## Predicción {-}

La **distribución predictiva posterior** de una observación futura $y^*\in\mathbb{R}$ es
$$
y^*\mid\boldsymbol{y} \sim \textsf{t}_{\nu_n}\left( \mu_n,\tfrac{\kappa_n+1}{\kappa_n}\,\sigma^2_n \right)
$$

# Previa impropia

La función $p(\theta,\log\sigma^2)\propto 1$ se denomina **distribución previa impropia**.

Esta "distribución" es equivalente a $p(\theta,\sigma^2)\propto 1/\sigma^2$ y se llama **impropia** porque no corresponde a una función de densidad de probabilidad.

Lo esencial es que la distribución posterior obtenida al utilizar una distribución previa impropia sí sea una distribución de probabilidad propia.

Una previa impropia permite realizar análisis sin imponer una información previa específica, lo que refleja una **postura máxima incertidumbre** (estado de información neutral que **minimiza la subjetividad**).

En este caso se tiene que la **distribución posterior** satisface que
$$
\theta\mid\sigma^2,\boldsymbol{y} \sim\textsf{N}\left(\bar{y},\tfrac{\sigma^2}{n}\right)
\qquad\text{y}\qquad
\sigma^2\mid\boldsymbol{y} \sim \textsf{GI}\left(\tfrac{n-1}{2},\tfrac{n-1}{2}s^2\right)\,,
$$
mientras que la **distribución marginal posterior** de $\theta$ es $\theta\mid\boldsymbol{y}\sim\textsf{t}_{n-1}\left(\bar{y},\tfrac{s^2}{n}\right)$, de donde
$$
\frac{\theta-\bar{y}}{s/\sqrt{n}}\,\Big|\,\boldsymbol{y} \sim\textsf{t}_{n-1}\,.
$$


# Error cuadrático medio

Un **estimador puntual** $\hat{\theta}$ de un parámetro desconocido \(\theta\) es una función que aplica $y_1,\ldots,y_n$ a un único elemento del espacio de parámetros \(\Theta\).

Las **propiedades muestrales** de \(\hat{\theta}\) se refiere al comportamiento de \(\hat{\theta}\) bajo una sucesión infinita de **procesos de observación hipotéticos**.

Para el modelo Normal, sea $\hat{\theta}_{\text{e}} = \bar{y}$ el estimador de **máxima verosimilitud** de $\theta$, y sea $\hat{\theta}_{\text{b}} = (1-\omega)\mu_0 + \omega \bar{y}$ el **estimador Bayesiano** de $\theta$ basado en la media posterior bajo la previa Normal-Gamma Inversa.

Si el valor verdadero de $\theta$ es $\theta_0$, entonces
$$
\textsf{E}(\hat{\theta}_{\text{e}}) = \theta_0
\qquad\text{y}\qquad
\textsf{E}(\hat{\theta}_{\text{b}}) =  (1-\omega)\mu_0 + \omega \theta_0\,.
$$
Por lo tanto, $\hat{\theta}_{\text{e}}$ es un **estimador insesgado** de $\theta$, y $\hat{\theta}_{\text{b}}$ es un **estimador sesgado** de $\theta$ a menos que $\mu_0 = \theta_0$.

Para evaluar qué tan "cercano" se encuentra \(\hat{\theta}\) de \(\theta_0\), se acostumbra usar el **error cuadrático medio** (MSE; *mean square error*) de  \(\hat{\theta}\), dado por
$$
\textsf{MSE}(\hat{\theta}) = \textsf{E}((\hat{\theta} -\theta)^2) = \textsf{Var}(\hat{\theta}) + \textsf{Bias}^2(\hat{\theta})\,,
$$
donde $\textsf{Bias}(\hat{\theta}) = \textsf{E}(\hat{\theta}) - \theta_0$ es el **sesgo** (*bias*) de $\hat{\theta}$.

En este caso, se tiene que
$$
\textsf{MSE}(\hat{\theta}_{\text{e}}) = \frac{\sigma^2}{n}
\qquad\text{y}\qquad
\textsf{MSE}(\hat{\theta}_{\text{b}}) = \omega^2\,\frac{\sigma^2}{n} + (1-\omega)^2(\mu_0-\theta_0)^2\,,
$$
y además, $\textsf{MSE}(\hat{\theta}_{\text{b}}) < \textsf{MSE}(\hat{\theta}_{\text{e}})$ siempre que
$$
(\mu_0-\theta_0)^2 < \left(\tfrac{1}{n} + \tfrac{2}{\kappa_0}\right)\sigma^2\,.
$$
Si se sabe un poco sobre la población, se pueden encontrar valores de \(\mu_0\) y \(\theta_0\) tales que se cumpla esta desigualdad, en cuyo caso es posible construir un estimador Bayesiano con menor distancia cuadrática promedio a la media de la población en comparación con la media muestral.


# Simulación de muestras de la distribución posterior

Para $b=1,\ldots,B$: 

- Simular $(\sigma^2)^{(b)}\sim \textsf{GI}\left(\frac{\nu_n}{2},\frac{\nu_n\,\sigma^2_n}{2}\right)$.
- Simular $\theta^{(b)} \sim \textsf{N} \left(\mu_{n}, \frac{(\sigma^{2})^{(b)}}{\kappa_{n}}\right)$.

Este procedimiento genera un conjunto de **muestras independientes de la distribución posterior** de la forma
$$
(\theta^{(1)},(\sigma^2)^{(1)}),\ldots,(\theta^{(B)},(\sigma^2)^{(B)})\,,
$$
que se pueden utilizar para caracterizar cualquier aspecto de $p(\theta,\sigma^2\mid \boldsymbol{y})$, $p(\theta\mid \boldsymbol{y})$, y $p(\sigma^2\mid \boldsymbol{y})$.


# Ejemplo: Puntajes de Matemáticas

La base de datos contiene la información de una **muestra aleatoria simple** de los **estudiantes que presentaron la Prueba Saber 11 en 2023-2**. 

La **prueba de matemáticas** está diseñada con una **escala de 0 a 100** (sin decimales), un **puntaje promedio de 50** y una **desviación estándar de 10 puntos**.

¿Existe suficiente evidencia para concluir que el puntaje promedio de Matemáticas de Bogotá es significativamente superior al puntaje promedio preestablecido por la prueba?

¿Qué ocurre en el caso de Vichada?

Los datos son públicos y están disponibles en este [enlace](https://www.icfes.gov.co/data-icfes/). 


## Tratamiento de datos

```{r}
# datos
dat <- read.csv("C:/Users/User/Dropbox/UN/estadistica_bayesiana/SB11_20232_muestra.txt", sep=";")
# dimensión de la base
dim(dat)
# distribución de frecuencias
table(dat$ESTU_DEPTO_RESIDE)
```


```{r}
# datos Bogotá
y <- dat[dat$ESTU_COD_RESIDE_DEPTO == 11, "PUNT_MATEMATICAS"]
# tamaño de muestra
(n <- length(y))
# estadísticos suficientes
(yb <- mean(y))
(s2 <- var(y))
```


## Distribución posterior


```{r}
# hiperparámetros
mu0 <- 50  
k0  <- 1
s20 <- 10^2 
nu0 <- 1
```


```{r}
# distribución posterior
(kn  <- k0 + n)
(nun <- nu0 + n)
(mun <- (k0/kn)*mu0 + (n/kn)*yb)
(s2n <- (nu0*s20 +(n-1)*s2 + k0*n*(yb-mu0)^2/kn)/nun)
```


```{r, fig.width=10, fig.height=5, fig.align='center', echo=F}
# calcula la función de densidad de una Gamma-Inversa
dinvgamma <- function(x, a, b, log = FALSE) {
  out <- a*log(b) - lgamma(a) - (a+1)*log(x) - b/x 
  if (log == FALSE) out <- exp(out)
  return(out)
}
# rango de valores
g <- 50
theta  <- seq(from = yb-3*sqrt(s2/n), to = yb+3*sqrt(s2/n), length = g)
sigma2 <- seq(from = 125, to = 165, length = g)
# evaluar y almacenar la distribución posterior en escala log
lp <- matrix(data = NA, nrow = g, ncol = g)
for(i in 1:g)
  for(j in 1:g)
    lp [i,j] <- dnorm(x = theta[i], mean = mun, sd = sqrt(sigma2[j]/kn), log = T) + 
      dinvgamma(x = sigma2[j], a = nun/2, nun*s2n/2, log = T)
# visualización de la distribución posterior
par(mfrow = c(1,2), mar = c(3,3,1,1), mgp = c(1.75,0.75,0))
# 2-D
image(x = theta, y = sigma2, z = exp(lp), col = heat.colors(100), xlab = expression(theta), ylab = expression(sigma^2))
# 3-D
persp(x = theta, y = sigma2, z = exp(lp), theta = 30, phi = 30, expand = 1, cex.axis = 0.75, xlab = "Media", ylab = "Varianza", zlab = "Densidad", col = "gray95", ticktype = "detailed")
```


## Muestras de la distribución posterior


```{r}
B <- 10000
set.seed(1234)
sigma2 <- 1/rgamma(n = B, shape = nun/2, rate = nun*s2n/2)
theta  <- rnorm(n = B, mean = mun, sd = sqrt(sigma2/kn))
```


```{r, echo=FALSE, fig.width=10, fig.height=5, fig.align='center'}
par(mfrow = c(1,2), mar = c(2.75,3,0.5,0.5), mgp = c(1.7,0.7,0))
# distribución conjunta
plot(theta, sigma2, pch = ".", xlim = yb+c(-1,1)*3*sqrt(s2/n), ylim = c(125,165), xlab = expression(theta), ylab = expression(sigma^2))
# distribución marginal de theta
plot(density(theta), xlim = yb+c(-1,1)*3*sqrt(s2/n), xlab = expression(theta), main = "", ylab = expression(paste(italic("p("), theta," | y)",sep="")))
```


## Inferencia: Bogotá

```{r, echo=F}
# inferencia Bayesiana
est_B <- mean(theta)
cv_B  <- sd(theta)/mean(theta)
ic_B  <- quantile(x = theta, probs = c(.025,.975))
# inferencia frecuentista: asintótica
est_F1 <- yb
cv_F1  <- sqrt(s2/n)/yb
ic_F1  <- yb + c(-1,1)*qnorm(p = 0.975)*sqrt(s2/n)
# inferencia frecuentista: bootstrap paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy <- rnorm(n = n, mean = yb, sd = sqrt(s2))
  out[i] <- mean(yy)
}
est_F2 <- mean(out)
cv_F2  <- sd(out)/mean(out)
ic_F2  <- quantile(x = out, probs = c(.025,.975))
# inferencia frecuentista: bootstrap no paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy <- sample(x = y, size = n, replace = T)
  out[i] <- mean(yy)
}
est_F3 <- mean(out)
cv_F3  <- sd(out)/mean(out)
ic_F3  <- quantile(x = out, probs = c(.025,.975))
```


```{r, echo=F}
# resultados
tab <- rbind(c(est_B , cv_B , ic_B ), 
             c(est_F1, cv_F1, ic_F1), 
             c(est_F2, cv_F2, ic_F2),
             c(est_F3, cv_F3, ic_F3))
colnames(tab) <- c("Estimación", "CV", "L. Inf.", "L. Sup.")
rownames(tab) <- c("Bayesiana", "Frec. Asintótico", "Frec. Bootstrap Par.", "Frec. Bootstrap No Par.")
knitr::kable(x = tab, digits = 3, align = "c", caption = "Inferencia sobre el puntaje promedio de Matemáticas de Bogotá.")
```


## Inferencia: Vichada

```{r, echo=FALSE}
#datos
y <- dat[dat$ESTU_DEPTO_RESIDE == "VICHADA", "PUNT_MATEMATICAS"]
# tamaño de muestra
n <- length(y)
# estadísticos suficientes
yb <- mean(y)
s2 <- var(y)
# hiperparámetros
mu0 <- 50  
k0  <- 1
s20 <- 10^2 
nu0 <- 1
# distribución posterior
kn  <- k0 + n
nun <- nu0 + n
mun <- (k0*mu0 + n*yb)/kn
s2n <- (nu0*s20 +(n-1)*s2 + k0*n*(yb-mu0)^2/kn)/nun
# muestras distribución posterior
B <- 10000
set.seed(1234)
sigma2 <- 1/rgamma(n = B, shape = nun/2, rate = nun*s2n/2)
theta  <- rnorm(n = B, mean = mun, sd = sqrt(sigma2/kn))
# inferencia Bayesiana
est_B <- mean(theta)
cv_B  <- sd(theta)/mean(theta)
ic_B  <- quantile(x = theta, probs = c(.025,.975))
# inferencia frecuentista: asintótica
est_F1 <- yb
cv_F1  <- sqrt(s2/n)/yb
ic_F1  <- yb + c(-1,1)*qnorm(p = 0.975)*sqrt(s2/n)
# inferencia frecuentista: bootstrap paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy <- rnorm(n = n, mean = yb, sd = sqrt(s2))
  out[i] <- mean(yy)
}
est_F2 <- mean(out)
cv_F2  <- sd(out)/mean(out)
ic_F2  <- quantile(x = out, probs = c(.025,.975))
# inferencia frecuentista: bootstrap no paramétrico
out <- NULL
set.seed(123)
for (i in 1:10000) {
  yy <- sample(x = y, size = n, replace = T)
  out[i] <- mean(yy)
}
est_F3 <- mean(out)
cv_F3  <- sd(out)/mean(out)
ic_F3  <- quantile(x = out, probs = c(.025,.975))
# resultados
tab <- rbind(c(est_B , cv_B , ic_B ), 
             c(est_F1, cv_F1, ic_F1), 
             c(est_F2, cv_F2, ic_F2),
             c(est_F3, cv_F3, ic_F3))
colnames(tab) <- c("Estimación", "CV", "L. Inf.", "L. Sup.")
rownames(tab) <- c("Bayesiana", "Frec. Asintótico", "Frec. Bootstrap Par.", "Frec. Bootstrap No Par.")
knitr::kable(x = tab, digits = 3, align = "c", caption = "Inferencia sobre el puntaje promedio de Matemáticas de Vichada.")
```

# Referencias {-}

```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```