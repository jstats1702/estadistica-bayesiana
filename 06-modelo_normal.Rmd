---
title: "Modelo Normal"
author: 
- Juan Sosa PhD
- Email   jcsosam@unal.edu.co
- GitHub  https://github.com/jstats1702 
date: ""
output:
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Modelo

El modelo Normal para **variables continuas** $y_i\in \mathbb{R}$, para $i = 1,\ldots,n$, está dado por
$$
\begin{align*}
	y_i\mid\theta,\sigma^2 &\stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2)\\
	(\theta,\sigma^2) &\sim p(\theta,\sigma^2) 
\end{align*}
$$

El modelo tiene $k=2$ parámetros desconocidos, a saber, la **media** $\theta$ (parámetro de localización) y la **varianza** $\sigma^2$ (cuadrado del parámetro de escala). Por lo tanto, $\boldsymbol{\theta}=(\theta,\sigma^2)$.


# Estadístico suficiente


Si $y_i\mid\theta,\sigma^2\stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2)$, para $i=1,\ldots,n$, entonces la **distribución muestral conjunta** de las observaciones es:
$$
p\left(\boldsymbol{y} \mid \theta, \sigma^{2}\right)=\prod_{i=1}^n \left(2 \pi \sigma^{2}\right)^{-1 / 2} \exp{ \left\{-\frac{1}{2} \left(\frac{y_{i}-\theta}{\sigma}\right)^{2}\right\} } = \left(2 \pi \sigma^{2}\right)^{-n / 2} \exp { \left\{-\frac{1}{2} \sum_{i=1}^{n}\left(\frac{y_{i}-\theta}{\sigma}\right)^{2}\right\} }\,,
$$
donde $\boldsymbol{y}=(y_1,\ldots,y_n)$.


Note que el **núcleo** de esta distribución se puede escribir como:
$$
\sum_{i=1}^n\left(\frac{y_{i}-\theta}{\sigma}\right)^{2}=\frac{1}{\sigma^{2}} \sum_{i=1}^{n} y_{i}^{2}-2 \frac{\theta}{\sigma^{2}} \sum_{i=1}^{n} y_{i}+n \frac{\theta^{2}}{\sigma^{2}}\,,
$$
lo cual sugiere que
$$
\left(\sum_{i=1}^{n} y_{i}, \sum_{i=1}^{n} y_{i}^{2}\right)
$$
es un **estadístico suficiente** para $(\theta,\sigma^2)$. 


La media muestral y la varianza muestral $(\bar{y},s^2)$,
$$
\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i\qquad\text{y}\qquad s^2 =\frac{1}{n-1}\left(\sum_{i=1}^n y_i^2 - n\bar{y}^2\right)
$$ 
también constituye un **estadístico suficiente** para $(\theta,\sigma^2)$, dado que
$$
\sum_{i=1}^n(y_i - \theta)^2 = (n-1)s^2 + n(\bar{y} - \theta)^2\,.
$$

# Modelo  Gamma Inversa-Normal-Normal


- **Distribución muestral**:
$$
y_i\mid\theta,\sigma^2 \stackrel{\text{iid}}{\sim} \textsf{N}(\theta,\sigma^2),\qquad i=1,\ldots,n\,.
$$
- **Distribución previa**:
$$
\begin{aligned}
	\theta \mid \sigma^{2} & \sim \textsf{N}\left(\mu_{0}, \frac{\sigma^{2}}{\kappa_{0}}\right) \\
	\sigma^{2} & \sim \textsf{GI}\left(\frac{\nu_{0}}{2}, \frac{\nu_{0}\,\sigma_{0}^{2}}{2}\right)
\end{aligned}
$$

- **Hiperparámetros**: $\mu_0$, $\kappa_0$, $\nu_0$, y $\sigma^2_0$.

Esta parametrización de la previa es muy conveniente para establecer una **interpretación práctica** de los hiperparámetros.

La variable aleatoria $X$ tiene distribución Gamma-Inversa con parámetros $\alpha,\beta > 0$, i.e., $X\mid\alpha,\beta\sim\textsf{GI}(\alpha,\beta)$, si su función de densidad de probabilidad es
$$
p(x\mid\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}\,x^{-(\alpha+1)}\,e^{-\beta/x}\,,\quad x>0\,.
$$
Esta distribución se puede obtener a partir de la distribución Gamma: Si $X\sim\textsf{Gamma}(\alpha,\beta)$, entonces $\tfrac{1}{X}\sim\textsf{GI}(\alpha,\beta)$.

Bajo el modelo Gamma Inversa-Normal-Normal se tiene que la **distribución posterior** de $\boldsymbol{\theta}$ es
$$
p(\theta,\sigma^2\mid \boldsymbol{y}) = p(\theta\mid \sigma^2, \boldsymbol{y})\,p(\sigma^2\mid \boldsymbol{y})
$$
donde:

- $p(\theta\mid \sigma^2, \boldsymbol{y}) = \textsf{N}\left(\theta\mid\mu_{n}, \frac{\sigma^{2}}{\kappa_{n}}\right)$, con
	$$
	\kappa_n = \kappa_0+n \qquad\text{y}\qquad \mu_n = \frac{\kappa_0}{\kappa_n}\mu_0 + \frac{n}{\kappa_n}\bar{y}\,,
	$$
	dado que $p(\theta\mid\sigma^2,\boldsymbol{y})\propto p(\boldsymbol{y}\mid\theta,\sigma^2)\,p(\theta\mid\sigma^2)$.
	Esta distribución se denomina **distribución condicional completa** de $\theta$.
- $p(\sigma^2\mid \boldsymbol{y}) =  \textsf{GI}\left(\sigma^2\mid\frac{\nu_n}{2},\frac{\nu_n\,\sigma^2_n}{2}\right)$, con
	$$
	\nu_n = \nu_0+n\qquad\text{y}\qquad \nu_{n}\sigma_{n}^{2} = \nu_{0} \sigma_{0}^{2}+(n-1) s^{2}+n \frac{\kappa_{0}}{\kappa_{n}}\left(\bar{y}-\mu_{0}\right)^2\,,
	$$
	dado que 
	$$
	p(\sigma^2\mid\boldsymbol{y}) \propto p(\sigma^2)\,p(\boldsymbol{y}\mid \sigma^2) = p(\sigma^2)\int_{-\infty}^\infty p(\boldsymbol{y}\mid\theta,\sigma^2)\,p(\theta\mid\sigma^2)\,\text{d}\theta\,.
	$$

Bajo esta parametrización de la distribución previa, se puede interpretar que:

- $\mu_0$ corresponde a la media previa de $\kappa_0$ observaciones previas.
- $\sigma^2_0$ corresponde a la varianza previa de $\nu_0$ observaciones previas.
- $(n-1) s^{2}$, $\nu_{0} \sigma_{0}^{2}$ y $\nu_{n} \sigma_{n}^{2}$ son las sumas de cuadrados muestral, previa, y posterior, respectivamente.


# Distribuciones marginales


Bajo el modelo Gamma Inversa-Normal-Normal se tiene que las distribuciones posteriores marginales para $\sigma^2$, $\theta$, y $y^*$ son respectivamente:
$$
\begin{align*}
\sigma^2\mid \boldsymbol{y} &\sim  \textsf{GI}\left(\frac{\nu_n}{2},\frac{\nu_n\,\sigma^2_n}{2}\right) \\
\theta\mid \boldsymbol{y}   &\sim  \textsf{t}_{\nu_n}\left( \mu_n,\frac{\sigma^2_n}{\kappa_n} \right) \\
y^*\mid\boldsymbol{y}       &\sim  \textsf{t}_{\nu_n}\left( \mu_n,\frac{\kappa_n+1}{\kappa_n}\,\sigma^2_n \right)
\end{align*}	
$$


# Simulación de muestras de la distribución posterior


Para $b=1,\ldots,B$: 

- Simular $(\sigma^2)^{(b)}\sim \textsf{GI}\left(\frac{\nu_n}{2},\frac{\nu_n\,\sigma^2_n}{2}\right)$.
- Simular $\theta^{(b)} \sim \textsf{N} \left(\mu_{n}, \frac{(\sigma^{2})^{(b)}}{\kappa_{n}}\right)$.

Este procedimiento genera un conjunto de **muestras independientes de la distribución posterior** $p(\theta,\sigma^2\mid \boldsymbol{y})$ de la forma
$$
(\theta^{(1)},(\sigma^2)^{(1)}),\ldots,(\theta^{(B)},(\sigma^2)^{(B)})\,,
$$
que se pueden utilizar para caracterizar cualquier aspecto de la distribución posterior, como de las distribuciones posteriores marginales $p(\theta\mid \boldsymbol{y})$ y $p(\sigma^2\mid \boldsymbol{y})$.


# Ejemplo: Anatomia de mosquitos Af

W. L. Grogan y W. W. Wirth descubrieron en las selvas de Brasil dos nuevas variedades de un mosquito picador (*midge*). 
A un tipo de mosquito lo llamaron mosquito Apf y al otro mosquito Af. 
Los biólogos descubrieron que el mosquito Apf es portador de una enfermedad debilitante que puede causar la inflamación del cerebro. 
Aunque la enfermedad rara vez es fatal, la discapacidad causada por la hinchazón puede ser permanente. 
La otra forma de mosquito, el Af, es inofensiva y un valioso polinizador. 
En un esfuerzo por distinguir las dos variedades, los biólogos tomaron medidas 
taxonómicas de los mosquitos que capturaron.

***Grogan Jr, W. L., & Wirth, W. W. (1981). A new American genus of predaceous midges related to Palpomyia and Bezzia (Diptera: Ceratopogonidae). Proceedings of the Biological Society of Washington., 94(4), 1279-1305.***

```{r, eval = TRUE, echo=FALSE, out.width="75%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("modelo_normal_midge.png")
```

Considere los datos de la **longitud del ala en milímetros** de $n=9$ miembros de 
la **especie Af de mosquitos**. 
A partir de estas nueve mediciones, se quiere hacer inferencia sobre la **media poblacional** $\theta$. 

Otros estudios sugieren que la longitud de las alas suele ser de alrededor de 1.9 mm con una una desviación estándar de 0.1 mm. Claramente, se tiene que las longitudes deben ser positivas, lo que implica que $\theta > 0$.

Los datos de Grogan y Wirth se encuentran disponibles en la librería `Flury` de R, pero esta librería no se encuentra disponible para versiones recientes de R.

```{r}
# datos y estadisticos suficientes
y <- c(1.64, 1.70, 1.72, 1.74, 1.82, 1.82, 1.82, 1.90, 2.08)
(n <- length(y))
(ybar <- mean(y))
(s2 <- var(y))
```

## Elicitación de los hiperparámetros

Se considera un modelo Normal conjugado para caracterizar la población asociada con este conjunto de datos.

Información previa:

- Otros estudios sugieren que la longitud promedio suele ser de 1.9 mm con una una desviación estándar de 0.1 mm.
- Se usa $\kappa_0 = \nu_0 = 1$ para que las previas sean no informativas dado que esta población en particular puede diferir de aquellas estudiadas en otras investigaciones.

```{r}
# hiperparámetros
mu0 <- 1.9  
k0  <- 1
s20 <- 0.1^2 
nu0 <- 1
```


## Distribución posterior


La distribución posterior de $(\theta,\sigma^2)$ es:
$$
p( \theta, \sigma^2 \mid \boldsymbol{y} ) = p( \theta \mid \sigma^2, \boldsymbol{y} ) \, p(\sigma^2 \mid \boldsymbol{y})
$$
donde
$$
p( \theta \mid \sigma^2, \boldsymbol{y} ) = \textsf{N}( \theta \mid 1.814, \sigma^2/10 )
\qquad\text{y}\qquad
p(\sigma^2 \mid \boldsymbol{y}) = \textsf{GI}( \sigma^2 \mid 10/2, 10*0.015324/2 )
$$


```{r}
# distribución posterior
(kn  <- k0 + n)
(nun <- nu0 + n)
(mun <- (k0*mu0 + n*ybar)/kn)
(s2n <- (nu0*s20 +(n-1)*s2 + k0*n*(ybar-mu0)^2/kn)/nun)
```


```{r}
# calcula la función de densidad de una Gamma-Inversa
dinvgamma <- function(x, a, b, log = FALSE) {
  out <- a*log(b) - lgamma(a) - (a+1)*log(x) - b/x 
  if (log == FALSE) out <- exp(out)
  return(out)
}
# rango de valores
gs    <- 250  # n. de puntos a evaluar
theta <- seq(from = 1.6,  to = 2.0,  length = gs)  # theta     : media
is2   <- seq(from = 15,   to = 160,  length = gs)  # 1/sigma^2 : precisión
s2g   <- seq(from = .001, to = .045, length = gs)  # sigma^2   : varianza  
# evaluar y almacenar la distribución posterior en escala log
ld.th.is2 <- matrix(data = NA, nrow = gs, ncol = gs)  # para ( theta, 1/sigma^2 ) 
ld.th.s2  <- matrix(data = NA, nrow = gs, ncol = gs)  # para ( theta, sigma^2 )
for(i in 1:gs) { 
  for(j in 1:gs) {
    ld.th.is2[i,j] <- dnorm(x = theta[i], mean = mun, sd = 1/sqrt(is2[j]*kn), log = T) + 
      dgamma(x = is2[j], shape = nun/2, rate = nun*s2n/2, log = T)
    ld.th.s2 [i,j] <- dnorm(x = theta[i], mean = mun, sd = sqrt(s2g[j]/kn), log = T) + 
      dinvgamma(x = s2g[j], a = nun/2, nun*s2n/2, log = T)
  }
} 
```


```{r, fig.width=12, fig.height=6, fig.align='center'}
# curvas de nivel de la distribución posterior
par(mfrow = c(1,2), mar = c(3,3,1,1), mgp = c(1.75,0.75,0))
# posterior (theta, 1/sigma^2)
image(x = theta, y = is2, z = exp(ld.th.is2), col = terrain.colors(100), xlab = expression(theta), ylab = expression(tilde(sigma)^2)) 
# posterior (theta, sigma^2)
image(x = theta, y = s2g, z = exp(ld.th.s2), col = terrain.colors(100), xlab = expression(theta), ylab = expression(sigma^2))
```


```{r, r, fig.width=6, fig.height=6, fig.align='center'}
# rango de valores
gs    <- 30
theta <- seq(from = 1.72, to = 1.9,  length = gs)
s2g   <- seq(from = .005, to = .030, length = gs)
# evaluar y almacenar la distribución posterior en escala log
ld.th.s2 <- matrix(data = NA, nrow = gs, ncol = gs)
for(i in 1:gs) { 
  for(j in 1:gs) {
    ld.th.s2[i,j] <- dnorm(x = theta[i], mean = mun, sd = sqrt(s2g[j]/kn), log = T) + 
      dinvgamma(x = s2g[j], a = nun/2, nun*s2n/2, log = T)
  }
}
# gráfico 3-D la distribución posterior
par(mfrow = c(1,1), mar = c(3,3,1,1), mgp = c(1.75,0.75,0))
persp(x = theta, y = s2g, z = exp(ld.th.s2), theta = 30, phi = 30, expand = 1, cex.axis = 0.75, xlab = "Media", ylab = "Varianza", zlab = "Densidad", col = "gray95", ticktype = "detailed")
```


## Generación de muestras de la distribución posterior

- Simulación de Monte Carlo.
- Para caracterizar cualquier cantidad posterior de interés: tendencia, variabilidad, probabilidades, gráficos, etc.

```{r}
B <- 100000
set.seed(1234)
s2_mc <- 1/rgamma(n = B, shape = nun/2, rate = nun*s2n/2)
theta_mc <- rnorm(n = B, mean = mun, sd = sqrt(s2_mc/kn))
```


```{r, echo=FALSE, fig.width=10, fig.height=10, fig.align='center'}
par(mfrow = c(2,2), mar = c(2.75,3,0.5,0.5), mgp = c(1.7,0.7,0))
# distribución conjunta
plot(theta_mc, 1/s2_mc, pch = ".", xlim = range(theta_mc), ylim = range(1/s2_mc), xlab = expression(theta), ylab = expression(tilde(sigma)^2))
# distribución conjunta
plot(theta_mc, s2_mc, pch = ".", xlim = range(theta_mc), ylim = range(s2_mc), xlab = expression(theta), ylab = expression(sigma^2))
# theta
plot(density(theta_mc), xlab = expression(theta), main = "", ylab = expression(paste(italic("p("), theta," | y)",sep="")))
# precisión
plot(density(1/s2_mc), xlab = expression(tilde(sigma)^2), main = "", ylab = expression(paste(italic("p("),tilde(sigma)^2," | y)",sep=""))) 
```


## Inferencia


```{r}
# media
round(quantile(theta_mc, c(.025, 0.5, 0.975)), 3)
# coeficiente de variación
round(quantile(100*abs(sqrt(s2_mc)/theta_mc), c(.025, 0.5, 0.975)), 3)
```



# Referencias {-}


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Hoffcoverbook.jpg")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Gelmancoverbook.png")
```


```{r, eval = TRUE, echo=FALSE, out.width="25%", fig.pos = 'H', fig.align = 'center'}
knitr::include_graphics("Reichcoverbook.jpg")
```